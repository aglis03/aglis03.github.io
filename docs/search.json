[
  {
    "objectID": "posts/Problem Set 2/index.html",
    "href": "posts/Problem Set 2/index.html",
    "title": "Data Mining: Problem Set 2",
    "section": "",
    "text": "Rows: 731\nColumns: 10\n$ date        &lt;date&gt; 2011-01-01, 2011-01-02, 2011-01-03, 2011-01-04, 2011-01-0…\n$ season      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ holiday     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n$ weekday     &lt;dbl&gt; 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4…\n$ weather     &lt;dbl&gt; 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2…\n$ temperature &lt;dbl&gt; 46.71653, 48.35024, 34.21239, 34.52000, 36.80056, 34.88784…\n$ realfeel    &lt;dbl&gt; 46.39865, 45.22419, 25.70131, 28.40009, 30.43728, NA, 28.0…\n$ humidity    &lt;dbl&gt; 0.805833, 0.696087, 0.437273, 0.590435, 0.436957, 0.518261…\n$ windspeed   &lt;dbl&gt; 6.679665, 10.347140, 10.337565, 6.673420, 7.780994, 3.7287…\n$ rentals     &lt;dbl&gt; 985, 801, 1349, 1562, 1600, 1606, 1510, 959, 822, 1321, 12…"
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-1",
    "href": "posts/Problem Set 2/index.html#step-1",
    "title": "Data Mining: Problem Set 2",
    "section": "",
    "text": "Rows: 731\nColumns: 10\n$ date        &lt;date&gt; 2011-01-01, 2011-01-02, 2011-01-03, 2011-01-04, 2011-01-0…\n$ season      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ holiday     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n$ weekday     &lt;dbl&gt; 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4…\n$ weather     &lt;dbl&gt; 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2…\n$ temperature &lt;dbl&gt; 46.71653, 48.35024, 34.21239, 34.52000, 36.80056, 34.88784…\n$ realfeel    &lt;dbl&gt; 46.39865, 45.22419, 25.70131, 28.40009, 30.43728, NA, 28.0…\n$ humidity    &lt;dbl&gt; 0.805833, 0.696087, 0.437273, 0.590435, 0.436957, 0.518261…\n$ windspeed   &lt;dbl&gt; 6.679665, 10.347140, 10.337565, 6.673420, 7.780994, 3.7287…\n$ rentals     &lt;dbl&gt; 985, 801, 1349, 1562, 1600, 1606, 1510, 959, 822, 1321, 12…"
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-2",
    "href": "posts/Problem Set 2/index.html#step-2",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 2",
    "text": "Step 2\nWhen looking at our data that has been marked as numeric data types, there are a few variables that are not truly represented as numeric. These variables are season, holiday, weekday and weather. For example, the table below represents the numbers of observations for the variable season, for the total of observations within each month and season is shown. The numbers are better off added up to see the total observations within each category, not to compare the results and summmaries of each.\n\n\n      month\nseason  1  2  3  4  5  6  7  8  9 10 11 12\n     1 62 57 40  0  0  0  0  0  0  0  0 22\n     2  0  0 22 60 62 40  0  0  0  0  0  0\n     3  0  0  0  0  0 20 62 62 44  0  0  0\n     4  0  0  0  0  0  0  0  0 16 62 60 40\n\n\nFrom the table above, we can turn these variables into factors and get even more specific with season by specifying which season such as Winter, Spring, Summer and Fall is assigned to which number. I have assigned the seasons as, Winter = 1, Spring - 2, Summer = 3 and Fall = 4, for the table helps create the assumptions on which numbers would best fit with specific months and seasons. The variables of holiday, weekday and weather are currently numeric but better off represented as categorical data. These values represents a specific answer and are known as nominal data types. When it comes to holiday we can see that the numbers of 0 and 1 indicates that the data is either considered a holiday or not, so creating two groups for those numbers were easy to replace and specify. Now for weekday, changing the the numeric values into the days of the week such as: Sunday = 0, Monday = 1, Tuesday - 2, Wednesday = 3, Thursday = 4, Friday = 5 and Saturday = 6, will help indicate specific days that rentals took place throughout the week."
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-3",
    "href": "posts/Problem Set 2/index.html#step-3",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 3",
    "text": "Step 3\nNow that we’ve got everything properly recognized as numeric or factor, we can use summary() to look at some basic statistics and also scout out missing values. To make things easier to read, we’ll divide summaries by numeric and factor data types.\nThis data is showcasing our numeric values.\n\n\n  temperature       realfeel         humidity        windspeed      \n Min.   :22.60   Min.   : 12.59   Min.   :0.0000   Min.   : 0.9322  \n 1st Qu.:46.12   1st Qu.: 43.38   1st Qu.:0.5200   1st Qu.: 5.6182  \n Median :59.76   Median : 61.25   Median :0.6267   Median : 7.5343  \n Mean   :59.51   Mean   : 59.60   Mean   :0.6279   Mean   : 7.9303  \n 3rd Qu.:73.05   3rd Qu.: 75.43   3rd Qu.:0.7302   3rd Qu.: 9.7092  \n Max.   :90.50   Max.   :103.10   Max.   :0.9725   Max.   :21.1266  \n                 NA's   :27                                         \n    rentals    \n Min.   :  22  \n 1st Qu.:3152  \n Median :4548  \n Mean   :4504  \n 3rd Qu.:5956  \n Max.   :8714  \n               \n\n\nThis data is showcasing our factor values that we had just developed.\n\n\n    season    holiday        weekday    weather\n Winter:181   Yes:710   Sunday   :105   1:463  \n Spring:184   No : 21   Monday   :105   2:247  \n Summer:188             Tuesday  :104   3: 21  \n Fall  :178             Wednesday:104          \n                        Thursday :104          \n                        Friday   :104          \n                        Saturday :105          \n\n\nFor the realfeel variable in the set of numeric variables, we are missing 27 values. These missing values arr originally shown in our dataset as NA. We will impute those missing values, meaning we will fill in numbers in the blank spots.\nNow, lets impute the missing values and compare to our original data.\nNow we can compare the resulting distributions.\n\n\n    realfeel      realfeel_orig   \n Min.   : 12.59   Min.   : 12.59  \n 1st Qu.: 43.80   1st Qu.: 43.38  \n Median : 61.25   Median : 61.25  \n Mean   : 59.66   Mean   : 59.60  \n 3rd Qu.: 74.98   3rd Qu.: 75.43  \n Max.   :103.10   Max.   :103.10  \n                  NA's   :27      \n\n\nLooking at the above distributions, we see that realfeel doesn’t have any missing values and has the same median and a very similar mean. Nothing else has changed expect for the 1st and 3rd quartile values have shifted a bit.\nStep 4\nRentals appears to encode the total numbers of bike rentals that occurred on a given date. This is count data. We can use both descriptive statistics as well as a histogram to get a visual of this data. Additionally, we can look at a picture of rentals over time to see if there is some trends or outliers present within our dataset.\n\n\n    rentals    \n Min.   :  22  \n 1st Qu.:3152  \n Median :4548  \n Mean   :4504  \n 3rd Qu.:5956  \n Max.   :8714  \n\n\nThe lowest recorded number is 22 rentals, and the highest recorded number is 8,714 rentals. Across the data the mean is about 4500 rentals and the median is only a little higher, meaning the model shouldn’t have a big skew and is fairly symmetric.\n\n\n\n\n\nWe can see that we don’t have a huge number of outliers and the distribution is not highly skewed in either direction. However, one thing to note is that it is a tri-model looking distribution. There are peaks in the data which suggest that there might be three different normal distributions over-lapping with one another."
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-5",
    "href": "posts/Problem Set 2/index.html#step-5",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 5",
    "text": "Step 5\nMany of the supervised learning algorithms can be helped or hurt by the relationships between features that will be used as predictors. We need to understand the distributions of each variable, looking for skew, outliers, and any other weirdness. This could involve histograms or boxplots of the variables. We can use scatter plots to look at relationships between predictors. For easier comparison we can also use correlation matrices to show statistically linear relationships.\n\n\n\n\n\nFirst off we can see that temperature and realfeel has a strong and linear relationship. The correlation is 0.96. This could mean that one variable is a function of and associated with the other. Indeed, realfeel is a relationship between temperature and humidity and wind that is mean to incorporate what temperature it feels like to a human. In such a case, we will want to leave out a variable. Either realfeel or the other features that go into it.\nThe distribution plots do not look particularly alarming. And the scatterplots don’t show any other overwhelmingly strong relationships. What we can see, is that there is a positive and nonlinear relationship between temperature and rentals as well as temperature and realfeel. Warmer temperatures are associated with more rentals, but eventually, warm temperatures that result in weather that is too hot for comfort will lead to a decrease in rentals.\nWe can also check these correlations with corrplot.\n\n\n\n\n\nWe’re going to Z-score normalize the temperature feature. Our reason is mostly arbitrary, but one benefit is that after the transformation, the mean will be zero. Positive numbers will represent above average temperatures and negative below average ones.\n\n\n  temperature      \n Min.   :-2.38324  \n 1st Qu.:-0.86479  \n Median : 0.01611  \n Mean   : 0.00000  \n 3rd Qu.: 0.87425  \n Max.   : 2.00098  \n\n\nWe can min-max normalize the wind variable. This will take all values of the feature and cram it into the interval \\([0, 1]\\). It essentially puts a feature into a percent range.\nA very important step, and a very common one required by many learning algorithms, is converting all categorical variables into dummy variables. This can be done many different ways in R. The dummy package does make it easier, however.\nBefore running the dummy() function we had 10 variables in the dataset. The result of the function is a new dataset with only the dummy variables generated from the factor variables in bikes. At this point we can replace the factor variables with the dummy ones."
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-6",
    "href": "posts/Problem Set 2/index.html#step-6",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 6",
    "text": "Step 6\nWe’re going to perform a penalized form of regression known as LASSO to find a decent predictive model. We’ll need to do a few things first. We need to get rid variables we don’t intend to have as predictors. The date and realfeel features will be removed.\nNormally, for a linear regression, you’d need to remove one dummy variable from a categorical variable. For example, season has 4 values (Winter, Spring, Fall, and Summer). We have dummy variable for each, but we need to omit one in order for it to work. But with LASSO, its okay and actually better to include them all and let the algorithm decide which to eliminate.\n\n\n\n\n\n\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                         seg84\nintercept          7129.477106\ntemperature         973.652665\nhumidity          -2908.777178\nwindspeed         -1819.172215\nseason_Winter      -718.644052\nseason_Spring       -70.054566\nseason_Summer         8.026927\nseason_Fall         297.688661\nholiday_Yes         404.154109\nholiday_No            .       \nweekday_Sunday     -237.481119\nweekday_Monday      -78.567823\nweekday_Tuesday       .       \nweekday_Wednesday     .       \nweekday_Thursday      .       \nweekday_Friday        .       \nweekday_Saturday     46.330329\nweather_1           254.947013\nweather_2             .       \nweather_3         -1627.856643\ntemperature2       -516.515981"
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-7",
    "href": "posts/Problem Set 2/index.html#step-7",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 7",
    "text": "Step 7\nNow we are going to create a visual representation that will compare our predicted rentals to our actual rentals over time.\n\n\n\n\n\nWe can see that the relationship of this distribution resulted in a weak positive and linear relationship. When interpreting our data, we can say that that our data is not to biased for there seems to be a pretty even variance from the regression line in the points in the model."
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-8",
    "href": "posts/Problem Set 2/index.html#step-8",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 8",
    "text": "Step 8\nThe features present in our dataset all have a sense of importance when developing predictions. Each feature adds a layer of specification and helps us develop a more reliable model. As we attempt to balance out bias and variance, making sure we are covering all aspects to help us make predictions is a must. As features are shifted to fit the appropriate data types they represent, being able to specify information into our model is then open to use. We cn then incorporate appropriate useage of out data and get a more in-depth predition as our model continues to learn more."
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-9",
    "href": "posts/Problem Set 2/index.html#step-9",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 9",
    "text": "Step 9\nWhen it comes to training a model to the data I had prepared, it connects back to all the changes and developments we had made to the feautures and data types in our original dataset. We added new data types, excluded variables, filled in missing values, created dummy variables, and even simplified much of the data we had changed into factors As new things were discovered and parameters were adjusted, the model learned new things. The model was capable of approaching a proper prediction as it shifted and adapted. This training we did to our model led it away from its bias and helped balance out the model in both bias and variance, thus creating a more dependable, yet not perfect, model for prediction."
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-10",
    "href": "posts/Problem Set 2/index.html#step-10",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 10",
    "text": "Step 10\nI feel as if creating dummy variables out of the features we had changed into factors were necessary for this prediction model. These numbers needed to represent the nominal data that they were intentionally marked down for and not used the same way as all of the numeric values present in the dataset. Specifiying these features that were changed into facors were also a big step in gettting more in-depth with our dataset as well. Filling in missing values for realfeel was a step that was I thought was not strictly required for our dataset for we decided that realfeel was a feauture to not use as a predictor. Though I do think it is important to include this step when dealing with other possible datasets. We imputed the median of realfeel into the spots of each missing value. If we would have thrown these 27 observations out, we could have possibly thrown out some important information, thus imputing these missing values allowed us to maintain this data and allowed us to take another step towards specification. Each model we had created regarding relationships, distributions, and even just overall summaries of our features within our datasets all helped us determine which data to include or to not include. Each step had an importance as we learn how to properly train and test our data."
  },
  {
    "objectID": "posts/Demo Post 1/index.html",
    "href": "posts/Demo Post 1/index.html",
    "title": "Demo Post 1",
    "section": "",
    "text": "This is a demo post in which we begin the blog. The idea here is that you create one post with this quarto document. The quarto document for a post will be named “index.qmd” insides of folder with the name of the post. For example, if I wanted my post to be titled “Demo Post 1” then I would do the following.\nAfter doing that, you can then edit the index.qmd document for that new post to your heart’s content. Lets do a little of that now so you can see how this might work."
  },
  {
    "objectID": "posts/Demo Post 1/index.html#including-resources",
    "href": "posts/Demo Post 1/index.html#including-resources",
    "title": "Demo Post 1",
    "section": "Including Resources",
    "text": "Including Resources\nSuppose you wanted to discuss something, like the CRISP-DM process for analytics projects. You might wish to refer to an image of the process and you could include the image in the “Demo Post 1” folder and reference it here in the document.\n\n\n\n\n\nYou can easily insert the image through the visual editor in Posit / RStudio."
  },
  {
    "objectID": "posts/Demo Post 1/index.html#data-and-output",
    "href": "posts/Demo Post 1/index.html#data-and-output",
    "title": "Demo Post 1",
    "section": "Data and Output",
    "text": "Data and Output\nLets look at some data.\n\nlibrary(tidyverse)\nlibrary(ggthemes)\ndata(\"USArrests\")\n\nUSArrests %&gt;%\n  ggplot(aes(x = Assault, y = Murder)) +\n  geom_point(pch = 21, color = \"coral3\", bg = \"coral\", size=3) +\n  labs(title = \"Arrests for Murder vs. Assault in US States\",\n       x = \"Arrests for assault per 100,000\",\n       y = \"Arrests for murder per 100,000\") +\n  theme_clean()\n\n\n\n\nThis would show us a relationship that we could then spend some paragraphs analyzing and interpreting."
  },
  {
    "objectID": "posts/Demo Post 2/index.html",
    "href": "posts/Demo Post 2/index.html",
    "title": "Demo Post 2",
    "section": "",
    "text": "We are looking at arrests data by state. The data set has 50 rows (one for each state) and four variables.\n\nglimpse(USArrests)\n\nRows: 50\nColumns: 4\n$ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\n\n\nEach of the variables are a numeric-continuous data type. We have arrests per 100,000 people for three violent crimes: assault, murder, and rape. We also have a column indicating the degree of urban population in that state. Before preceding with prediction, we note that tree-based techniques can be more unstable if the variables are too correlated with one another. We can also see if there are any extreme skews in the data.\n\nlibrary(GGally)\nggpairs(USArrests)\n\n\n\n\nWe do see some positive relationships and stronger correlations, but mayne not quite enough to get us in trouble.\nNow lets try and predict Murder using the other features.\n\ndt = rpart(Murder ~.,\n           data=USArrests)\nrpart.plot(dt)\n\n\n\n\nWe can calculate a kind of R-squared measure of accuracy by squaring the correlation between the actual Murder values with our predicted ones.\n\nUSArrests %&gt;%\n  mutate(predicted_murder = predict(dt, USArrests)) %&gt;%\n  select(Murder, predicted_murder) %&gt;%\n  cor() -&gt; corrmat\n\nrsq = corrmat[[\"Murder\", \"predicted_murder\"]]^2\nprint(paste(\"The r-square for our model is\", round(rsq,2), sep=\": \"))\n\n[1] \"The r-square for our model is: 0.78\""
  },
  {
    "objectID": "posts/Demo Post 2/index.html#understanding-the-data",
    "href": "posts/Demo Post 2/index.html#understanding-the-data",
    "title": "Demo Post 2",
    "section": "",
    "text": "We are looking at arrests data by state. The data set has 50 rows (one for each state) and four variables.\n\nglimpse(USArrests)\n\nRows: 50\nColumns: 4\n$ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\n\n\nEach of the variables are a numeric-continuous data type. We have arrests per 100,000 people for three violent crimes: assault, murder, and rape. We also have a column indicating the degree of urban population in that state. Before preceding with prediction, we note that tree-based techniques can be more unstable if the variables are too correlated with one another. We can also see if there are any extreme skews in the data.\n\nlibrary(GGally)\nggpairs(USArrests)\n\n\n\n\nWe do see some positive relationships and stronger correlations, but mayne not quite enough to get us in trouble.\nNow lets try and predict Murder using the other features.\n\ndt = rpart(Murder ~.,\n           data=USArrests)\nrpart.plot(dt)\n\n\n\n\nWe can calculate a kind of R-squared measure of accuracy by squaring the correlation between the actual Murder values with our predicted ones.\n\nUSArrests %&gt;%\n  mutate(predicted_murder = predict(dt, USArrests)) %&gt;%\n  select(Murder, predicted_murder) %&gt;%\n  cor() -&gt; corrmat\n\nrsq = corrmat[[\"Murder\", \"predicted_murder\"]]^2\nprint(paste(\"The r-square for our model is\", round(rsq,2), sep=\": \"))\n\n[1] \"The r-square for our model is: 0.78\""
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects are different than posts. Projects should be more expansive, impressive and generally more professional in nature compared to posts. Posts can be works in progress. Small ideas or things you did that you thought were interesting. Projects should really showcase your professional abilities. You don’t need to have too many, just make them good. And try to always have one “in the works” so that employers and collaborators can see that you’re driven.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Problem Set 3\n\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nAnnika G. Lee\n\n\n\n\n\n\n  \n\n\n\n\nData Mining: Problem Set 2\n\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2023\n\n\nAnnika G. Lee\n\n\n\n\n\n\n  \n\n\n\n\nDemo Post 2\n\n\n\n\n\n\n\ndecision trees\n\n\nmachine learning\n\n\narrests\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nJane Doe\n\n\n\n\n\n\n  \n\n\n\n\nDemo Post 1\n\n\n\n\n\n\n\nquarto\n\n\ncrisp-dm\n\n\nscatterplot\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nJane Doe\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Problem Set 3/index.html",
    "href": "posts/Problem Set 3/index.html",
    "title": "Problem Set 3",
    "section": "",
    "text": "rm(list = ls())\nlibrary(tidyverse)\nlibrary(glmnet)\nlibrary(lubridate)\nlibrary(caret)\nlibrary(dummy)\nlibrary(gamlr)\nlibrary(rmarkdown)\nlibrary(GGally)\nlibrary(rpart)\nlibrary(rpart.plot)"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#section",
    "href": "posts/Problem Set 3/index.html#section",
    "title": "Problem Set 3",
    "section": "",
    "text": "rm(list = ls())\nlibrary(tidyverse)\nlibrary(glmnet)\nlibrary(lubridate)\nlibrary(caret)\nlibrary(dummy)\nlibrary(gamlr)\nlibrary(rmarkdown)\nlibrary(GGally)\nlibrary(rpart)\nlibrary(rpart.plot)"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-2",
    "href": "posts/Problem Set 3/index.html#part-2",
    "title": "Problem Set 3",
    "section": "Part 2",
    "text": "Part 2\nWithin this dataset there are a total of 1,436 observations and 39 columns of different features. 36 our of the 39 features are numeric date types, whereas 3 are categorical data types.\n\ncars = read_csv(\"ToyotaCorolla.csv\")\n\nRows: 1436 Columns: 39\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Model, Fuel_Type, Color\ndbl (36): Id, Price, Age_08_04, Mfg_Month, Mfg_Year, KM, HP, Met_Color, Auto...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(cars)\n\nRows: 1,436\nColumns: 39\n$ Id                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ Model             &lt;chr&gt; \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\", \"TO…\n$ Price             &lt;dbl&gt; 13500, 13750, 13950, 14950, 13750, 12950, 16900, 186…\n$ Age_08_04         &lt;dbl&gt; 23, 23, 24, 26, 30, 32, 27, 30, 27, 23, 25, 22, 25, …\n$ Mfg_Month         &lt;dbl&gt; 10, 10, 9, 7, 3, 1, 6, 3, 6, 10, 8, 11, 8, 2, 1, 5, …\n$ Mfg_Year          &lt;dbl&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002…\n$ KM                &lt;dbl&gt; 46986, 72937, 41711, 48000, 38500, 61000, 94612, 758…\n$ Fuel_Type         &lt;chr&gt; \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"D…\n$ HP                &lt;dbl&gt; 90, 90, 90, 90, 90, 90, 90, 90, 192, 69, 192, 192, 1…\n$ Met_Color         &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1…\n$ Color             &lt;chr&gt; \"Blue\", \"Silver\", \"Blue\", \"Black\", \"Black\", \"White\",…\n$ Automatic         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ CC                &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 1800…\n$ Doors             &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ Cylinders         &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4…\n$ Gears             &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 5, 5…\n$ Quarterly_Tax     &lt;dbl&gt; 210, 210, 210, 210, 210, 210, 210, 210, 100, 185, 10…\n$ Weight            &lt;dbl&gt; 1165, 1165, 1165, 1165, 1170, 1170, 1245, 1245, 1185…\n$ Mfr_Guarantee     &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0…\n$ BOVAG_Guarantee   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0…\n$ Guarantee_Period  &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 12, 3, 3, 3, 3, 3, 3, …\n$ ABS               &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Airbag_1          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Airbag_2          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0…\n$ Airco             &lt;dbl&gt; 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Automatic_airco   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0…\n$ Boardcomputer     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0…\n$ CD_Player         &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0…\n$ Central_Lock      &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Powered_Windows   &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Power_Steering    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Radio             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ Mistlamps         &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0…\n$ Sport_Model       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0…\n$ Backseat_Divider  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0…\n$ Metallic_Rim      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0…\n$ Radio_cassette    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ Parking_Assistant &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Tow_Bar           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n\n\nWhen looking at our data using glimpse(), we can assume that the features with little importance are Id, Model, Mfg_Month, and Cylinders. These features do not have much of an affect on generating predictions of prices for used Toyota Corollas within this dataset. Therefore removing these features will allow us a more simplified dataset to use to develop predictions. I have also renamed the Age_08_04 feature into Age.\n\ncars = cars %&gt;%\n  select(-Id, -Model, -Mfg_Month, -Cylinders) %&gt;%\n  rename(Age = Age_08_04)\n\nSome of our features are better represented as nominal data types. As we change those features into categorical and nominal data, we will change them into factor data. With now numeric and factor data types present, combining all the data back into the the cars dataset will allow us to look for missing values and help us determine where to impute the feature’s median into spots for missing values.\n\ncars_fct = cars %&gt;%\n  select(-Price, -Age, -KM, -HP, -CC, -Weight, -Quarterly_Tax) %&gt;%\n  mutate_all(.funs = factor)\n\ncars_num = cars %&gt;%\n  select(Price, Age, KM, HP, CC, Weight, Quarterly_Tax)\n\ncars = bind_cols(cars_num, cars_fct)\n\nOur dataset shows that each numeric feature has no missing values, therefore not needing to impute any median values into spots of missing values.\n\nsummary(cars_num)\n\n     Price            Age              KM               HP       \n Min.   : 4350   Min.   : 1.00   Min.   :     1   Min.   : 69.0  \n 1st Qu.: 8450   1st Qu.:44.00   1st Qu.: 43000   1st Qu.: 90.0  \n Median : 9900   Median :61.00   Median : 63390   Median :110.0  \n Mean   :10731   Mean   :55.95   Mean   : 68533   Mean   :101.5  \n 3rd Qu.:11950   3rd Qu.:70.00   3rd Qu.: 87021   3rd Qu.:110.0  \n Max.   :32500   Max.   :80.00   Max.   :243000   Max.   :192.0  \n       CC            Weight     Quarterly_Tax   \n Min.   : 1300   Min.   :1000   Min.   : 19.00  \n 1st Qu.: 1400   1st Qu.:1040   1st Qu.: 69.00  \n Median : 1600   Median :1070   Median : 85.00  \n Mean   : 1577   Mean   :1072   Mean   : 87.12  \n 3rd Qu.: 1600   3rd Qu.:1085   3rd Qu.: 85.00  \n Max.   :16000   Max.   :1615   Max.   :283.00"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-3",
    "href": "posts/Problem Set 3/index.html#part-3",
    "title": "Problem Set 3",
    "section": "Part 3",
    "text": "Part 3\nAfter looking at the variable Price, we can look at its distribution and can determine that Price is appropriate for a linear regression model for it does not have any missing values and does not have low variability within its data The distribution does have a slight right skew and a tiny amount of outliers. When lookong at our Linear Regression model that has been developed below, the Min and Max of the data set are not too different from one another and the same goes for 1Q and 3Q of the dataset as well. These are good signs of normal distribution that we want. The only slight concern is that the model’s Median is quite a far distance from 0.\n\nlm_Price = train(Price ~ .,\n            data = cars,\n            method = \"lm\")\n\nlm_Price\n\nLinear Regression \n\n1436 samples\n  34 predictor\n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 1436, 1436, 1436, 1436, 1436, 1436, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  1469.441  0.8338681  848.2138\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\nsummary(lm_Price)\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5225.2  -620.6   -41.6   575.3  6192.5 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        -2.234e+03  1.796e+03  -1.244 0.213732    \nAge                -2.902e+01  8.801e+00  -3.297 0.001003 ** \nKM                 -1.589e-02  1.085e-03 -14.642  &lt; 2e-16 ***\nHP                  2.230e+01  3.116e+00   7.158 1.33e-12 ***\nCC                 -6.027e-02  7.374e-02  -0.817 0.413890    \nWeight              7.533e+00  1.166e+00   6.460 1.45e-10 ***\nQuarterly_Tax       1.122e+01  1.655e+00   6.779 1.78e-11 ***\nMfg_Year1999        7.023e+02  1.320e+02   5.322 1.20e-07 ***\nMfg_Year2000        1.541e+03  2.315e+02   6.659 3.98e-11 ***\nMfg_Year2001        2.313e+03  3.284e+02   7.044 2.95e-12 ***\nMfg_Year2002        4.489e+03  4.482e+02  10.018  &lt; 2e-16 ***\nMfg_Year2003        6.025e+03  5.338e+02  11.286  &lt; 2e-16 ***\nMfg_Year2004        7.967e+03  6.491e+02  12.274  &lt; 2e-16 ***\nFuel_TypeDiesel     1.002e+03  3.098e+02   3.233 0.001255 ** \nFuel_TypePetrol     1.502e+03  3.242e+02   4.635 3.91e-06 ***\nMet_Color1         -6.757e+01  6.820e+01  -0.991 0.322000    \nColorBlack          6.014e+02  6.166e+02   0.975 0.329563    \nColorBlue           4.780e+02  6.158e+02   0.776 0.437672    \nColorGreen          3.340e+02  6.169e+02   0.541 0.588304    \nColorGrey           6.352e+02  6.160e+02   1.031 0.302672    \nColorRed            4.733e+02  6.159e+02   0.768 0.442344    \nColorSilver         5.582e+02  6.211e+02   0.899 0.368984    \nColorViolet         2.224e+02  8.171e+02   0.272 0.785563    \nColorWhite         -2.128e+02  6.450e+02  -0.330 0.741514    \nColorYellow         5.609e+02  8.670e+02   0.647 0.517744    \nAutomatic1          4.620e+02  1.325e+02   3.488 0.000503 ***\nDoors3             -4.380e+02  7.626e+02  -0.574 0.565842    \nDoors4             -4.341e+02  7.679e+02  -0.565 0.571948    \nDoors5             -2.798e+02  7.641e+02  -0.366 0.714255    \nGears4             -1.128e+02  1.328e+03  -0.085 0.932337    \nGears5              5.834e+02  7.879e+02   0.741 0.459109    \nGears6              1.011e+03  8.101e+02   1.247 0.212449    \nMfr_Guarantee1      2.968e+02  6.310e+01   4.704 2.81e-06 ***\nBOVAG_Guarantee1    4.583e+02  1.102e+02   4.158 3.40e-05 ***\nGuarantee_Period6   5.197e+02  1.669e+02   3.114 0.001884 ** \nGuarantee_Period12  7.091e+02  1.671e+02   4.244 2.34e-05 ***\nGuarantee_Period13  3.974e+03  1.097e+03   3.622 0.000303 ***\nGuarantee_Period18  2.862e+03  1.078e+03   2.655 0.008023 ** \nGuarantee_Period20  1.387e+03  1.079e+03   1.285 0.198945    \nGuarantee_Period24  4.352e+02  5.627e+02   0.773 0.439421    \nGuarantee_Period28  1.668e+03  1.080e+03   1.545 0.122632    \nGuarantee_Period36  1.138e+02  5.743e+02   0.198 0.842910    \nABS1               -4.491e+01  1.126e+02  -0.399 0.689972    \nAirbag_11           1.535e+02  2.156e+02   0.712 0.476656    \nAirbag_21          -1.855e+00  1.189e+02  -0.016 0.987554    \nAirco1              2.356e+02  7.626e+01   3.089 0.002047 ** \nAutomatic_airco1    1.918e+03  1.683e+02  11.395  &lt; 2e-16 ***\nBoardcomputer1     -1.630e+02  1.092e+02  -1.493 0.135603    \nCD_Player1          2.170e+02  8.486e+01   2.557 0.010676 *  \nCentral_Lock1      -9.588e+01  1.210e+02  -0.792 0.428242    \nPowered_Windows1    3.032e+02  1.214e+02   2.497 0.012649 *  \nPower_Steering1    -1.597e+02  2.421e+02  -0.660 0.509435    \nRadio1              5.816e+02  6.244e+02   0.931 0.351791    \nMistlamps1          2.806e+01  9.299e+01   0.302 0.762902    \nSport_Model1       -3.963e+01  8.145e+01  -0.487 0.626634    \nBackseat_Divider1  -4.920e+01  1.308e+02  -0.376 0.706793    \nMetallic_Rim1       1.146e+02  8.199e+01   1.398 0.162472    \nRadio_cassette1    -6.200e+02  6.239e+02  -0.994 0.320488    \nParking_Assistant1 -4.225e+02  5.400e+02  -0.782 0.434109    \nTow_Bar1           -1.724e+02  6.709e+01  -2.570 0.010263 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1049 on 1376 degrees of freedom\nMultiple R-squared:  0.9197,    Adjusted R-squared:  0.9163 \nF-statistic: 267.2 on 59 and 1376 DF,  p-value: &lt; 2.2e-16\n\n\n\nhistogram(cars$Price)"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-4",
    "href": "posts/Problem Set 3/index.html#part-4",
    "title": "Problem Set 3",
    "section": "Part 4",
    "text": "Part 4\n\ncaret::featurePlot(keep(cars, is.numeric), cars$Price, plot = \"scatter\")"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-5",
    "href": "posts/Problem Set 3/index.html#part-5",
    "title": "Problem Set 3",
    "section": "Part 5",
    "text": "Part 5\n\ncars %&gt;%\n  keep(is.numeric) %&gt;%\n  ggpairs()"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-6",
    "href": "posts/Problem Set 3/index.html#part-6",
    "title": "Problem Set 3",
    "section": "Part 6",
    "text": "Part 6\n\ncars_dum = dummy(cars, int = TRUE)\ncars_num = cars %&gt;%\n  keep(is.numeric)\ncars = bind_cols(cars_num, cars_dum)\nrm(cars_dum, cars_num)\n\n\n#DATA PARTITION \nset.seed(4532)\nsamp = createDataPartition(cars$Price, p=0.7, list = FALSE)\ntraining = cars[samp, ]\ntesting = cars[-samp, ]\nrm(samp)"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-7",
    "href": "posts/Problem Set 3/index.html#part-7",
    "title": "Problem Set 3",
    "section": "Part 7",
    "text": "Part 7\nPre-pruning sets limitations and boundaries to the tree and limits the overall complexity of it. Whereas post-pruning allows the tree to continue growing nwith justifications being made to it. As we justify which data to use, we get to tune and retrain the tree.\n\n#TRAIN/CONTROL DATA\ntrain_ctrl = trainControl(method = \"repeatedcv\", number = 20, repeats = 10)\ntree = train(Price ~ .,\n             data = training,\n             method = \"rpart\",\n             trControl = train_ctrl,\n             tuneGrid = expand.grid(cp = seq(0.0, 0.1, 0.01)),\n             control = rpart.control(method = \"anova\", minsplit = 1, minbucket = 1)\n             )\ntree \n\nCART \n\n1007 samples\n  87 predictor\n\nNo pre-processing\nResampling: Cross-Validated (20 fold, repeated 10 times) \nSummary of sample sizes: 956, 956, 955, 957, 956, 958, ... \nResampling results across tuning parameters:\n\n  cp    RMSE      Rsquared   MAE     \n  0.00  1374.770  0.8540659  1050.899\n  0.01  1372.104  0.8561630  1014.247\n  0.02  1627.509  0.7985036  1222.259\n  0.03  1644.343  0.7947001  1237.561\n  0.04  1727.284  0.7781115  1258.505\n  0.05  1729.339  0.7765759  1263.543\n  0.06  1729.339  0.7765759  1263.543\n  0.07  1729.339  0.7765759  1263.543\n  0.08  1729.339  0.7765759  1263.543\n  0.09  1729.339  0.7765759  1263.543\n  0.10  1729.339  0.7765759  1263.543\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.01.\n\n\n\nctrl = rpart::rpart.control(cp= 0.01, \n                            minbucket = 7,\n                            maxdepth = 30)\n\ntraining_regression_tree = rpart(Price ~ .,\n           data = training,\n           method = \"anova\",\n           control = ctrl)\n\npred = predict(training_regression_tree, training)\n\npred.rmse = caret::RMSE(pred = pred, obs = training$Price)\n\npaste(\"Sample Mean:\", mean(training$Price))\n\n[1] \"Sample Mean: 10717.1559086395\"\n\npaste(\"Sample Stdev:\", sd(training$Price))\n\n[1] \"Sample Stdev: 3625.85903560054\"\n\npaste(\"Model RMSE:\", pred.rmse)\n\n[1] \"Model RMSE: 1292.32730208874\"\n\n\n\nrpart.plot(training_regression_tree)"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-8",
    "href": "posts/Problem Set 3/index.html#part-8",
    "title": "Problem Set 3",
    "section": "Part 8",
    "text": "Part 8\nWhen looking at our feature importance, we can determine that Age, KM, and `Weight’ are the only three variables that are difference in importance when compared to all other variables. We could possibly remove all other variables in the dataset.\n\nlibrary(iml)\nlibrary(patchwork)\ntree_predictor = iml::Predictor$new(tree, data = training)\ntree_imp = iml::FeatureImp$new(tree_predictor, loss = \"rmse\", compare = \"ratio\", n.repetitions = 10)\nplot(tree_imp)\n\n\n\n\n\ntree_imp$results\n\n               feature importance.05 importance importance.95 permutation.error\n1                  Age      3.598087   3.682139      3.728545          4544.057\n2               Weight      1.327703   1.337346      1.377178          1650.393\n3                   KM      1.126270   1.176474      1.208962          1451.864\n4                   HP      1.000000   1.000000      1.000000          1234.081\n5                   CC      1.000000   1.000000      1.000000          1234.081\n6        Quarterly_Tax      1.000000   1.000000      1.000000          1234.081\n7        Mfg_Year_1998      1.000000   1.000000      1.000000          1234.081\n8        Mfg_Year_1999      1.000000   1.000000      1.000000          1234.081\n9        Mfg_Year_2000      1.000000   1.000000      1.000000          1234.081\n10       Mfg_Year_2001      1.000000   1.000000      1.000000          1234.081\n11       Mfg_Year_2002      1.000000   1.000000      1.000000          1234.081\n12       Mfg_Year_2003      1.000000   1.000000      1.000000          1234.081\n13       Mfg_Year_2004      1.000000   1.000000      1.000000          1234.081\n14       Fuel_Type_CNG      1.000000   1.000000      1.000000          1234.081\n15    Fuel_Type_Diesel      1.000000   1.000000      1.000000          1234.081\n16    Fuel_Type_Petrol      1.000000   1.000000      1.000000          1234.081\n17         Met_Color_0      1.000000   1.000000      1.000000          1234.081\n18         Met_Color_1      1.000000   1.000000      1.000000          1234.081\n19         Color_Beige      1.000000   1.000000      1.000000          1234.081\n20         Color_Black      1.000000   1.000000      1.000000          1234.081\n21          Color_Blue      1.000000   1.000000      1.000000          1234.081\n22         Color_Green      1.000000   1.000000      1.000000          1234.081\n23          Color_Grey      1.000000   1.000000      1.000000          1234.081\n24           Color_Red      1.000000   1.000000      1.000000          1234.081\n25        Color_Silver      1.000000   1.000000      1.000000          1234.081\n26        Color_Violet      1.000000   1.000000      1.000000          1234.081\n27         Color_White      1.000000   1.000000      1.000000          1234.081\n28        Color_Yellow      1.000000   1.000000      1.000000          1234.081\n29         Automatic_0      1.000000   1.000000      1.000000          1234.081\n30         Automatic_1      1.000000   1.000000      1.000000          1234.081\n31             Doors_2      1.000000   1.000000      1.000000          1234.081\n32             Doors_3      1.000000   1.000000      1.000000          1234.081\n33             Doors_4      1.000000   1.000000      1.000000          1234.081\n34             Doors_5      1.000000   1.000000      1.000000          1234.081\n35             Gears_3      1.000000   1.000000      1.000000          1234.081\n36             Gears_4      1.000000   1.000000      1.000000          1234.081\n37             Gears_5      1.000000   1.000000      1.000000          1234.081\n38             Gears_6      1.000000   1.000000      1.000000          1234.081\n39     Mfr_Guarantee_0      1.000000   1.000000      1.000000          1234.081\n40     Mfr_Guarantee_1      1.000000   1.000000      1.000000          1234.081\n41   BOVAG_Guarantee_0      1.000000   1.000000      1.000000          1234.081\n42   BOVAG_Guarantee_1      1.000000   1.000000      1.000000          1234.081\n43  Guarantee_Period_3      1.000000   1.000000      1.000000          1234.081\n44  Guarantee_Period_6      1.000000   1.000000      1.000000          1234.081\n45 Guarantee_Period_12      1.000000   1.000000      1.000000          1234.081\n46 Guarantee_Period_13      1.000000   1.000000      1.000000          1234.081\n47 Guarantee_Period_18      1.000000   1.000000      1.000000          1234.081\n48 Guarantee_Period_20      1.000000   1.000000      1.000000          1234.081\n49 Guarantee_Period_24      1.000000   1.000000      1.000000          1234.081\n50 Guarantee_Period_28      1.000000   1.000000      1.000000          1234.081\n51 Guarantee_Period_36      1.000000   1.000000      1.000000          1234.081\n52               ABS_0      1.000000   1.000000      1.000000          1234.081\n53               ABS_1      1.000000   1.000000      1.000000          1234.081\n54          Airbag_1_0      1.000000   1.000000      1.000000          1234.081\n55          Airbag_1_1      1.000000   1.000000      1.000000          1234.081\n56          Airbag_2_0      1.000000   1.000000      1.000000          1234.081\n57          Airbag_2_1      1.000000   1.000000      1.000000          1234.081\n58             Airco_0      1.000000   1.000000      1.000000          1234.081\n59             Airco_1      1.000000   1.000000      1.000000          1234.081\n60   Automatic_airco_0      1.000000   1.000000      1.000000          1234.081\n61   Automatic_airco_1      1.000000   1.000000      1.000000          1234.081\n62     Boardcomputer_0      1.000000   1.000000      1.000000          1234.081\n63     Boardcomputer_1      1.000000   1.000000      1.000000          1234.081\n64         CD_Player_0      1.000000   1.000000      1.000000          1234.081\n65         CD_Player_1      1.000000   1.000000      1.000000          1234.081\n66      Central_Lock_0      1.000000   1.000000      1.000000          1234.081\n67      Central_Lock_1      1.000000   1.000000      1.000000          1234.081\n68   Powered_Windows_0      1.000000   1.000000      1.000000          1234.081\n69   Powered_Windows_1      1.000000   1.000000      1.000000          1234.081\n70    Power_Steering_0      1.000000   1.000000      1.000000          1234.081\n71    Power_Steering_1      1.000000   1.000000      1.000000          1234.081\n72             Radio_0      1.000000   1.000000      1.000000          1234.081\n73             Radio_1      1.000000   1.000000      1.000000          1234.081\n74         Mistlamps_0      1.000000   1.000000      1.000000          1234.081\n75         Mistlamps_1      1.000000   1.000000      1.000000          1234.081\n76       Sport_Model_0      1.000000   1.000000      1.000000          1234.081\n77       Sport_Model_1      1.000000   1.000000      1.000000          1234.081\n78  Backseat_Divider_0      1.000000   1.000000      1.000000          1234.081\n79  Backseat_Divider_1      1.000000   1.000000      1.000000          1234.081\n80      Metallic_Rim_0      1.000000   1.000000      1.000000          1234.081\n81      Metallic_Rim_1      1.000000   1.000000      1.000000          1234.081\n82    Radio_cassette_0      1.000000   1.000000      1.000000          1234.081\n83    Radio_cassette_1      1.000000   1.000000      1.000000          1234.081\n84 Parking_Assistant_0      1.000000   1.000000      1.000000          1234.081\n85 Parking_Assistant_1      1.000000   1.000000      1.000000          1234.081\n86           Tow_Bar_0      1.000000   1.000000      1.000000          1234.081\n87           Tow_Bar_1      1.000000   1.000000      1.000000          1234.081"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-9",
    "href": "posts/Problem Set 3/index.html#part-9",
    "title": "Problem Set 3",
    "section": "Part 9",
    "text": "Part 9\n\nctrl = rpart::rpart.control(cp= 0.01, \n                            minbucket = 7,\n                            maxdepth = 30)\nregression_tree2 = rpart(Price ~ KM + Age + Weight,\n           data = training,\n           method = \"anova\",\n           control = ctrl)\n\npred = predict(regression_tree2, training)\n\npred.rmse = caret::RMSE(pred = pred, obs = training$Price)\n\npaste(\"Sample Mean:\", mean(training$Price))\n\n[1] \"Sample Mean: 10717.1559086395\"\n\npaste(\"Sample Stdev:\", sd(training$Price))\n\n[1] \"Sample Stdev: 3625.85903560054\"\n\npaste(\"Model RMSE:\", pred.rmse)\n\n[1] \"Model RMSE: 1292.32730208874\"\n\n\n\nrpart.plot(regression_tree2)"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-10",
    "href": "posts/Problem Set 3/index.html#part-10",
    "title": "Problem Set 3",
    "section": "Part 10",
    "text": "Part 10\n\ntest_predictions = predict(regression_tree2, newdata = testing)\ntest_rmse = postResample(pred = test_predictions,\n                         obs = testing$Price)\ntest_rmse\n\n        RMSE     Rsquared          MAE \n1524.6855721    0.8265925 1077.5923760 \n\n\nWe can see from the training and testing of our dataset that the RMSE from the cross validation error with the most optimal model with the smallest complexity model is 1372.104. The RMSE for the testing data is 1495.690. We can see this due to our usage of less variables in when developing predictions with the testing data. Less variables means less accuracy to the model, which makes sense in this instance."
  }
]