[
  {
    "objectID": "posts/Problem Set 2/index.html",
    "href": "posts/Problem Set 2/index.html",
    "title": "Data Mining: Problem Set 2",
    "section": "",
    "text": "Rows: 731\nColumns: 10\n$ date        &lt;date&gt; 2011-01-01, 2011-01-02, 2011-01-03, 2011-01-04, 2011-01-0…\n$ season      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ holiday     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n$ weekday     &lt;dbl&gt; 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4…\n$ weather     &lt;dbl&gt; 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2…\n$ temperature &lt;dbl&gt; 46.71653, 48.35024, 34.21239, 34.52000, 36.80056, 34.88784…\n$ realfeel    &lt;dbl&gt; 46.39865, 45.22419, 25.70131, 28.40009, 30.43728, NA, 28.0…\n$ humidity    &lt;dbl&gt; 0.805833, 0.696087, 0.437273, 0.590435, 0.436957, 0.518261…\n$ windspeed   &lt;dbl&gt; 6.679665, 10.347140, 10.337565, 6.673420, 7.780994, 3.7287…\n$ rentals     &lt;dbl&gt; 985, 801, 1349, 1562, 1600, 1606, 1510, 959, 822, 1321, 12…"
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-1",
    "href": "posts/Problem Set 2/index.html#step-1",
    "title": "Data Mining: Problem Set 2",
    "section": "",
    "text": "Rows: 731\nColumns: 10\n$ date        &lt;date&gt; 2011-01-01, 2011-01-02, 2011-01-03, 2011-01-04, 2011-01-0…\n$ season      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ holiday     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n$ weekday     &lt;dbl&gt; 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4…\n$ weather     &lt;dbl&gt; 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2…\n$ temperature &lt;dbl&gt; 46.71653, 48.35024, 34.21239, 34.52000, 36.80056, 34.88784…\n$ realfeel    &lt;dbl&gt; 46.39865, 45.22419, 25.70131, 28.40009, 30.43728, NA, 28.0…\n$ humidity    &lt;dbl&gt; 0.805833, 0.696087, 0.437273, 0.590435, 0.436957, 0.518261…\n$ windspeed   &lt;dbl&gt; 6.679665, 10.347140, 10.337565, 6.673420, 7.780994, 3.7287…\n$ rentals     &lt;dbl&gt; 985, 801, 1349, 1562, 1600, 1606, 1510, 959, 822, 1321, 12…"
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-2",
    "href": "posts/Problem Set 2/index.html#step-2",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 2",
    "text": "Step 2\nWhen looking at our data that has been marked as numeric data types, there are a few variables that are not truly represented as numeric. These variables are season, holiday, weekday and weather. For example, the table below represents the numbers of observations for the variable season, for the total of observations within each month and season is shown. The numbers are better off added up to see the total observations within each category, not to compare the results and summmaries of each.\n\n\n      month\nseason  1  2  3  4  5  6  7  8  9 10 11 12\n     1 62 57 40  0  0  0  0  0  0  0  0 22\n     2  0  0 22 60 62 40  0  0  0  0  0  0\n     3  0  0  0  0  0 20 62 62 44  0  0  0\n     4  0  0  0  0  0  0  0  0 16 62 60 40\n\n\nFrom the table above, we can turn these variables into factors and get even more specific with season by specifying which season such as Winter, Spring, Summer and Fall is assigned to which number. I have assigned the seasons as, Winter = 1, Spring - 2, Summer = 3 and Fall = 4, for the table helps create the assumptions on which numbers would best fit with specific months and seasons. The variables of holiday, weekday and weather are currently numeric but better off represented as categorical data. These values represents a specific answer and are known as nominal data types. When it comes to holiday we can see that the numbers of 0 and 1 indicates that the data is either considered a holiday or not, so creating two groups for those numbers were easy to replace and specify. Now for weekday, changing the the numeric values into the days of the week such as: Sunday = 0, Monday = 1, Tuesday - 2, Wednesday = 3, Thursday = 4, Friday = 5 and Saturday = 6, will help indicate specific days that rentals took place throughout the week."
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-3",
    "href": "posts/Problem Set 2/index.html#step-3",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 3",
    "text": "Step 3\nNow that we’ve got everything properly recognized as numeric or factor, we can use summary() to look at some basic statistics and also scout out missing values. To make things easier to read, we’ll divide summaries by numeric and factor data types.\nThis data is showcasing our numeric values.\n\n\n  temperature       realfeel         humidity        windspeed      \n Min.   :22.60   Min.   : 12.59   Min.   :0.0000   Min.   : 0.9322  \n 1st Qu.:46.12   1st Qu.: 43.38   1st Qu.:0.5200   1st Qu.: 5.6182  \n Median :59.76   Median : 61.25   Median :0.6267   Median : 7.5343  \n Mean   :59.51   Mean   : 59.60   Mean   :0.6279   Mean   : 7.9303  \n 3rd Qu.:73.05   3rd Qu.: 75.43   3rd Qu.:0.7302   3rd Qu.: 9.7092  \n Max.   :90.50   Max.   :103.10   Max.   :0.9725   Max.   :21.1266  \n                 NA's   :27                                         \n    rentals    \n Min.   :  22  \n 1st Qu.:3152  \n Median :4548  \n Mean   :4504  \n 3rd Qu.:5956  \n Max.   :8714  \n               \n\n\nThis data is showcasing our factor values that we had just developed.\n\n\n    season    holiday        weekday    weather\n Winter:181   Yes:710   Sunday   :105   1:463  \n Spring:184   No : 21   Monday   :105   2:247  \n Summer:188             Tuesday  :104   3: 21  \n Fall  :178             Wednesday:104          \n                        Thursday :104          \n                        Friday   :104          \n                        Saturday :105          \n\n\nFor the realfeel variable in the set of numeric variables, we are missing 27 values. These missing values arr originally shown in our dataset as NA. We will impute those missing values, meaning we will fill in numbers in the blank spots.\nNow, lets impute the missing values and compare to our original data.\nNow we can compare the resulting distributions.\n\n\n    realfeel      realfeel_orig   \n Min.   : 12.59   Min.   : 12.59  \n 1st Qu.: 43.80   1st Qu.: 43.38  \n Median : 61.25   Median : 61.25  \n Mean   : 59.66   Mean   : 59.60  \n 3rd Qu.: 74.98   3rd Qu.: 75.43  \n Max.   :103.10   Max.   :103.10  \n                  NA's   :27      \n\n\nLooking at the above distributions, we see that realfeel doesn’t have any missing values and has the same median and a very similar mean. Nothing else has changed expect for the 1st and 3rd quartile values have shifted a bit.\nStep 4\nRentals appears to encode the total numbers of bike rentals that occurred on a given date. This is count data. We can use both descriptive statistics as well as a histogram to get a visual of this data. Additionally, we can look at a picture of rentals over time to see if there is some trends or outliers present within our dataset.\n\n\n    rentals    \n Min.   :  22  \n 1st Qu.:3152  \n Median :4548  \n Mean   :4504  \n 3rd Qu.:5956  \n Max.   :8714  \n\n\nThe lowest recorded number is 22 rentals, and the highest recorded number is 8,714 rentals. Across the data the mean is about 4500 rentals and the median is only a little higher, meaning the model shouldn’t have a big skew and is fairly symmetric.\n\n\n\n\n\nWe can see that we don’t have a huge number of outliers and the distribution is not highly skewed in either direction. However, one thing to note is that it is a tri-model looking distribution. There are peaks in the data which suggest that there might be three different normal distributions over-lapping with one another."
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-5",
    "href": "posts/Problem Set 2/index.html#step-5",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 5",
    "text": "Step 5\nMany of the supervised learning algorithms can be helped or hurt by the relationships between features that will be used as predictors. We need to understand the distributions of each variable, looking for skew, outliers, and any other weirdness. This could involve histograms or boxplots of the variables. We can use scatter plots to look at relationships between predictors. For easier comparison we can also use correlation matrices to show statistically linear relationships.\n\n\n\n\n\nFirst off we can see that temperature and realfeel has a strong and linear relationship. The correlation is 0.96. This could mean that one variable is a function of and associated with the other. Indeed, realfeel is a relationship between temperature and humidity and wind that is mean to incorporate what temperature it feels like to a human. In such a case, we will want to leave out a variable. Either realfeel or the other features that go into it.\nThe distribution plots do not look particularly alarming. And the scatterplots don’t show any other overwhelmingly strong relationships. What we can see, is that there is a positive and nonlinear relationship between temperature and rentals as well as temperature and realfeel. Warmer temperatures are associated with more rentals, but eventually, warm temperatures that result in weather that is too hot for comfort will lead to a decrease in rentals.\nWe can also check these correlations with corrplot.\n\n\n\n\n\nWe’re going to Z-score normalize the temperature feature. Our reason is mostly arbitrary, but one benefit is that after the transformation, the mean will be zero. Positive numbers will represent above average temperatures and negative below average ones.\n\n\n  temperature      \n Min.   :-2.38324  \n 1st Qu.:-0.86479  \n Median : 0.01611  \n Mean   : 0.00000  \n 3rd Qu.: 0.87425  \n Max.   : 2.00098  \n\n\nWe can min-max normalize the wind variable. This will take all values of the feature and cram it into the interval \\([0, 1]\\). It essentially puts a feature into a percent range.\nA very important step, and a very common one required by many learning algorithms, is converting all categorical variables into dummy variables. This can be done many different ways in R. The dummy package does make it easier, however.\nBefore running the dummy() function we had 10 variables in the dataset. The result of the function is a new dataset with only the dummy variables generated from the factor variables in bikes. At this point we can replace the factor variables with the dummy ones."
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-6",
    "href": "posts/Problem Set 2/index.html#step-6",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 6",
    "text": "Step 6\nWe’re going to perform a penalized form of regression known as LASSO to find a decent predictive model. We’ll need to do a few things first. We need to get rid variables we don’t intend to have as predictors. The date and realfeel features will be removed.\nNormally, for a linear regression, you’d need to remove one dummy variable from a categorical variable. For example, season has 4 values (Winter, Spring, Fall, and Summer). We have dummy variable for each, but we need to omit one in order for it to work. But with LASSO, its okay and actually better to include them all and let the algorithm decide which to eliminate.\n\n\n\n\n\n\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                         seg84\nintercept          7129.477106\ntemperature         973.652665\nhumidity          -2908.777178\nwindspeed         -1819.172215\nseason_Winter      -718.644052\nseason_Spring       -70.054566\nseason_Summer         8.026927\nseason_Fall         297.688661\nholiday_Yes         404.154109\nholiday_No            .       \nweekday_Sunday     -237.481119\nweekday_Monday      -78.567823\nweekday_Tuesday       .       \nweekday_Wednesday     .       \nweekday_Thursday      .       \nweekday_Friday        .       \nweekday_Saturday     46.330329\nweather_1           254.947013\nweather_2             .       \nweather_3         -1627.856643\ntemperature2       -516.515981"
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-7",
    "href": "posts/Problem Set 2/index.html#step-7",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 7",
    "text": "Step 7\nNow we are going to create a visual representation that will compare our predicted rentals to our actual rentals over time.\n\n\n\n\n\nWe can see that the relationship of this distribution resulted in a weak positive and linear relationship. When interpreting our data, we can say that that our data is not to biased for there seems to be a pretty even variance from the regression line in the points in the model."
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-8",
    "href": "posts/Problem Set 2/index.html#step-8",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 8",
    "text": "Step 8\nThe features present in our dataset all have a sense of importance when developing predictions. Each feature adds a layer of specification and helps us develop a more reliable model. As we attempt to balance out bias and variance, making sure we are covering all aspects to help us make predictions is a must. As features are shifted to fit the appropriate data types they represent, being able to specify information into our model is then open to use. We cn then incorporate appropriate useage of out data and get a more in-depth predition as our model continues to learn more."
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-9",
    "href": "posts/Problem Set 2/index.html#step-9",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 9",
    "text": "Step 9\nWhen it comes to training a model to the data I had prepared, it connects back to all the changes and developments we had made to the feautures and data types in our original dataset. We added new data types, excluded variables, filled in missing values, created dummy variables, and even simplified much of the data we had changed into factors As new things were discovered and parameters were adjusted, the model learned new things. The model was capable of approaching a proper prediction as it shifted and adapted. This training we did to our model led it away from its bias and helped balance out the model in both bias and variance, thus creating a more dependable, yet not perfect, model for prediction."
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-10",
    "href": "posts/Problem Set 2/index.html#step-10",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 10",
    "text": "Step 10\nI feel as if creating dummy variables out of the features we had changed into factors were necessary for this prediction model. These numbers needed to represent the nominal data that they were intentionally marked down for and not used the same way as all of the numeric values present in the dataset. Specifiying these features that were changed into facors were also a big step in gettting more in-depth with our dataset as well. Filling in missing values for realfeel was a step that was I thought was not strictly required for our dataset for we decided that realfeel was a feauture to not use as a predictor. Though I do think it is important to include this step when dealing with other possible datasets. We imputed the median of realfeel into the spots of each missing value. If we would have thrown these 27 observations out, we could have possibly thrown out some important information, thus imputing these missing values allowed us to maintain this data and allowed us to take another step towards specification. Each model we had created regarding relationships, distributions, and even just overall summaries of our features within our datasets all helped us determine which data to include or to not include. Each step had an importance as we learn how to properly train and test our data."
  },
  {
    "objectID": "posts/Demo Post 1/index.html",
    "href": "posts/Demo Post 1/index.html",
    "title": "Demo Post 1",
    "section": "",
    "text": "This is a demo post in which we begin the blog. The idea here is that you create one post with this quarto document. The quarto document for a post will be named “index.qmd” insides of folder with the name of the post. For example, if I wanted my post to be titled “Demo Post 1” then I would do the following.\nAfter doing that, you can then edit the index.qmd document for that new post to your heart’s content. Lets do a little of that now so you can see how this might work."
  },
  {
    "objectID": "posts/Demo Post 1/index.html#including-resources",
    "href": "posts/Demo Post 1/index.html#including-resources",
    "title": "Demo Post 1",
    "section": "Including Resources",
    "text": "Including Resources\nSuppose you wanted to discuss something, like the CRISP-DM process for analytics projects. You might wish to refer to an image of the process and you could include the image in the “Demo Post 1” folder and reference it here in the document.\n\n\n\n\n\nYou can easily insert the image through the visual editor in Posit / RStudio."
  },
  {
    "objectID": "posts/Demo Post 1/index.html#data-and-output",
    "href": "posts/Demo Post 1/index.html#data-and-output",
    "title": "Demo Post 1",
    "section": "Data and Output",
    "text": "Data and Output\nLets look at some data.\n\nlibrary(tidyverse)\nlibrary(ggthemes)\ndata(\"USArrests\")\n\nUSArrests %&gt;%\n  ggplot(aes(x = Assault, y = Murder)) +\n  geom_point(pch = 21, color = \"coral3\", bg = \"coral\", size=3) +\n  labs(title = \"Arrests for Murder vs. Assault in US States\",\n       x = \"Arrests for assault per 100,000\",\n       y = \"Arrests for murder per 100,000\") +\n  theme_clean()\n\n\n\n\nThis would show us a relationship that we could then spend some paragraphs analyzing and interpreting."
  },
  {
    "objectID": "posts/Demo Post 2/index.html",
    "href": "posts/Demo Post 2/index.html",
    "title": "Demo Post 2",
    "section": "",
    "text": "We are looking at arrests data by state. The data set has 50 rows (one for each state) and four variables.\n\nglimpse(USArrests)\n\nRows: 50\nColumns: 4\n$ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\n\n\nEach of the variables are a numeric-continuous data type. We have arrests per 100,000 people for three violent crimes: assault, murder, and rape. We also have a column indicating the degree of urban population in that state. Before preceding with prediction, we note that tree-based techniques can be more unstable if the variables are too correlated with one another. We can also see if there are any extreme skews in the data.\n\nlibrary(GGally)\nggpairs(USArrests)\n\n\n\n\nWe do see some positive relationships and stronger correlations, but mayne not quite enough to get us in trouble.\nNow lets try and predict Murder using the other features.\n\ndt = rpart(Murder ~.,\n           data=USArrests)\nrpart.plot(dt)\n\n\n\n\nWe can calculate a kind of R-squared measure of accuracy by squaring the correlation between the actual Murder values with our predicted ones.\n\nUSArrests %&gt;%\n  mutate(predicted_murder = predict(dt, USArrests)) %&gt;%\n  select(Murder, predicted_murder) %&gt;%\n  cor() -&gt; corrmat\n\nrsq = corrmat[[\"Murder\", \"predicted_murder\"]]^2\nprint(paste(\"The r-square for our model is\", round(rsq,2), sep=\": \"))\n\n[1] \"The r-square for our model is: 0.78\""
  },
  {
    "objectID": "posts/Demo Post 2/index.html#understanding-the-data",
    "href": "posts/Demo Post 2/index.html#understanding-the-data",
    "title": "Demo Post 2",
    "section": "",
    "text": "We are looking at arrests data by state. The data set has 50 rows (one for each state) and four variables.\n\nglimpse(USArrests)\n\nRows: 50\nColumns: 4\n$ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\n\n\nEach of the variables are a numeric-continuous data type. We have arrests per 100,000 people for three violent crimes: assault, murder, and rape. We also have a column indicating the degree of urban population in that state. Before preceding with prediction, we note that tree-based techniques can be more unstable if the variables are too correlated with one another. We can also see if there are any extreme skews in the data.\n\nlibrary(GGally)\nggpairs(USArrests)\n\n\n\n\nWe do see some positive relationships and stronger correlations, but mayne not quite enough to get us in trouble.\nNow lets try and predict Murder using the other features.\n\ndt = rpart(Murder ~.,\n           data=USArrests)\nrpart.plot(dt)\n\n\n\n\nWe can calculate a kind of R-squared measure of accuracy by squaring the correlation between the actual Murder values with our predicted ones.\n\nUSArrests %&gt;%\n  mutate(predicted_murder = predict(dt, USArrests)) %&gt;%\n  select(Murder, predicted_murder) %&gt;%\n  cor() -&gt; corrmat\n\nrsq = corrmat[[\"Murder\", \"predicted_murder\"]]^2\nprint(paste(\"The r-square for our model is\", round(rsq,2), sep=\": \"))\n\n[1] \"The r-square for our model is: 0.78\""
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects are different than posts. Projects should be more expansive, impressive and generally more professional in nature compared to posts. Posts can be works in progress. Small ideas or things you did that you thought were interesting. Projects should really showcase your professional abilities. You don’t need to have too many, just make them good. And try to always have one “in the works” so that employers and collaborators can see that you’re driven.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Problem Set #7\n\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nAnnika G. Lee\n\n\n\n\n\n\n  \n\n\n\n\nProblem Set 4\n\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2023\n\n\nAnnika G. Lee\n\n\n\n\n\n\n  \n\n\n\n\nProblem Set 3\n\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nAnnika G. Lee\n\n\n\n\n\n\n  \n\n\n\n\nData Mining: Problem Set 2\n\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2023\n\n\nAnnika G. Lee\n\n\n\n\n\n\n  \n\n\n\n\nDemo Post 2\n\n\n\n\n\n\n\ndecision trees\n\n\nmachine learning\n\n\narrests\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nJane Doe\n\n\n\n\n\n\n  \n\n\n\n\nDemo Post 1\n\n\n\n\n\n\n\nquarto\n\n\ncrisp-dm\n\n\nscatterplot\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nJane Doe\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Problem Set 3/index.html",
    "href": "posts/Problem Set 3/index.html",
    "title": "Problem Set 3",
    "section": "",
    "text": "rm(list = ls())\nlibrary(tidyverse)\nlibrary(glmnet)\nlibrary(lubridate)\nlibrary(caret)\nlibrary(dummy)\nlibrary(gamlr)\nlibrary(rmarkdown)\nlibrary(GGally)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(corrplot)"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#section",
    "href": "posts/Problem Set 3/index.html#section",
    "title": "Problem Set 3",
    "section": "",
    "text": "rm(list = ls())\nlibrary(tidyverse)\nlibrary(glmnet)\nlibrary(lubridate)\nlibrary(caret)\nlibrary(dummy)\nlibrary(gamlr)\nlibrary(rmarkdown)\nlibrary(GGally)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(corrplot)"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-2",
    "href": "posts/Problem Set 3/index.html#part-2",
    "title": "Problem Set 3",
    "section": "Part 2",
    "text": "Part 2\nWithin this dataset there are a total of 1,436 observations and 39 columns of different features. 36 our of the 39 features are numeric date types, whereas 3 are categorical data types.\n\ncars = read_csv(\"ToyotaCorolla.csv\")\n\nRows: 1436 Columns: 39\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Model, Fuel_Type, Color\ndbl (36): Id, Price, Age_08_04, Mfg_Month, Mfg_Year, KM, HP, Met_Color, Auto...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(cars)\n\nRows: 1,436\nColumns: 39\n$ Id                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ Model             &lt;chr&gt; \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\", \"TO…\n$ Price             &lt;dbl&gt; 13500, 13750, 13950, 14950, 13750, 12950, 16900, 186…\n$ Age_08_04         &lt;dbl&gt; 23, 23, 24, 26, 30, 32, 27, 30, 27, 23, 25, 22, 25, …\n$ Mfg_Month         &lt;dbl&gt; 10, 10, 9, 7, 3, 1, 6, 3, 6, 10, 8, 11, 8, 2, 1, 5, …\n$ Mfg_Year          &lt;dbl&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002…\n$ KM                &lt;dbl&gt; 46986, 72937, 41711, 48000, 38500, 61000, 94612, 758…\n$ Fuel_Type         &lt;chr&gt; \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"D…\n$ HP                &lt;dbl&gt; 90, 90, 90, 90, 90, 90, 90, 90, 192, 69, 192, 192, 1…\n$ Met_Color         &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1…\n$ Color             &lt;chr&gt; \"Blue\", \"Silver\", \"Blue\", \"Black\", \"Black\", \"White\",…\n$ Automatic         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ CC                &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 1800…\n$ Doors             &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ Cylinders         &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4…\n$ Gears             &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 5, 5…\n$ Quarterly_Tax     &lt;dbl&gt; 210, 210, 210, 210, 210, 210, 210, 210, 100, 185, 10…\n$ Weight            &lt;dbl&gt; 1165, 1165, 1165, 1165, 1170, 1170, 1245, 1245, 1185…\n$ Mfr_Guarantee     &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0…\n$ BOVAG_Guarantee   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0…\n$ Guarantee_Period  &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 12, 3, 3, 3, 3, 3, 3, …\n$ ABS               &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Airbag_1          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Airbag_2          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0…\n$ Airco             &lt;dbl&gt; 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Automatic_airco   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0…\n$ Boardcomputer     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0…\n$ CD_Player         &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0…\n$ Central_Lock      &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Powered_Windows   &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Power_Steering    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Radio             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ Mistlamps         &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0…\n$ Sport_Model       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0…\n$ Backseat_Divider  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0…\n$ Metallic_Rim      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0…\n$ Radio_cassette    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ Parking_Assistant &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Tow_Bar           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n\n\nWhen looking at our data using glimpse(), we can assume that the features with little importance are Id, Model, Mfg_Month, and Cylinders. These features do not have much of an affect on generating predictions of prices for used Toyota Corollas within this dataset. Therefore removing these features will allow us a more simplified dataset to use to develop predictions. I have also renamed the Age_08_04 feature into Age.\n\ncars = cars %&gt;%\n  select(-Id, -Model, -Mfg_Month, -Cylinders) %&gt;%\n  rename(Age = Age_08_04)\n\nSome of our features are better represented as nominal data types. As we change those features into categorical and nominal data, we will change them into factor data. With now numeric and factor data types present, combining all the data back into the the cars dataset will allow us to look for missing values and help us determine where to impute the feature’s median into spots for missing values.\n\ncars_fct = cars %&gt;%\n  select(-Price, -Age, -KM, -HP, -CC, -Weight, -Quarterly_Tax) %&gt;%\n  mutate_all(.funs = factor)\n\ncars_num = cars %&gt;%\n  select(Price, Age, KM, HP, CC, Weight, Quarterly_Tax)\n\ncars = bind_cols(cars_num, cars_fct)\n\nOur dataset shows that each feature has no missing values. This mean we do not need to impute any values into spots of missing values.\n\nsummary(cars)\n\n     Price            Age              KM               HP       \n Min.   : 4350   Min.   : 1.00   Min.   :     1   Min.   : 69.0  \n 1st Qu.: 8450   1st Qu.:44.00   1st Qu.: 43000   1st Qu.: 90.0  \n Median : 9900   Median :61.00   Median : 63390   Median :110.0  \n Mean   :10731   Mean   :55.95   Mean   : 68533   Mean   :101.5  \n 3rd Qu.:11950   3rd Qu.:70.00   3rd Qu.: 87021   3rd Qu.:110.0  \n Max.   :32500   Max.   :80.00   Max.   :243000   Max.   :192.0  \n                                                                 \n       CC            Weight     Quarterly_Tax    Mfg_Year    Fuel_Type   \n Min.   : 1300   Min.   :1000   Min.   : 19.00   1998:392   CNG   :  17  \n 1st Qu.: 1400   1st Qu.:1040   1st Qu.: 69.00   1999:441   Diesel: 155  \n Median : 1600   Median :1070   Median : 85.00   2000:225   Petrol:1264  \n Mean   : 1577   Mean   :1072   Mean   : 87.12   2001:192                \n 3rd Qu.: 1600   3rd Qu.:1085   3rd Qu.: 85.00   2002: 87                \n Max.   :16000   Max.   :1615   Max.   :283.00   2003: 75                \n                                                 2004: 24                \n Met_Color     Color     Automatic Doors   Gears    Mfr_Guarantee\n 0:467     Grey   :301   0:1356    2:  2   3:   2   0:848        \n 1:969     Blue   :283   1:  80    3:622   4:   1   1:588        \n           Red    :278             4:138   5:1390                \n           Green  :220             5:674   6:  43                \n           Black  :191                                           \n           Silver :122                                           \n           (Other): 41                                           \n BOVAG_Guarantee Guarantee_Period ABS      Airbag_1 Airbag_2 Airco  \n 0: 150          3      :1274     0: 268   0:  42   0: 398   0:706  \n 1:1286          6      :  77     1:1168   1:1394   1:1038   1:730  \n                 12     :  73                                       \n                 24     :   4                                       \n                 36     :   4                                       \n                 13     :   1                                       \n                 (Other):   3                                       \n Automatic_airco Boardcomputer CD_Player Central_Lock Powered_Windows\n 0:1355          0:1013        0:1122    0:603        0:629          \n 1:  81          1: 423        1: 314    1:833        1:807          \n                                                                     \n                                                                     \n                                                                     \n                                                                     \n                                                                     \n Power_Steering Radio    Mistlamps Sport_Model Backseat_Divider Metallic_Rim\n 0:  32         0:1226   0:1067    0:1005      0: 330           0:1142      \n 1:1404         1: 210   1: 369    1: 431      1:1106           1: 294      \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n Radio_cassette Parking_Assistant Tow_Bar \n 0:1227         0:1432            0:1037  \n 1: 209         1:   4            1: 399"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-3",
    "href": "posts/Problem Set 3/index.html#part-3",
    "title": "Problem Set 3",
    "section": "Part 3",
    "text": "Part 3\nAfter looking at the variable Price, we can look at its distribution and can determine that Price is appropriate for a linear regression model for it does not have any missing values and does not have low variability within its data. The distribution is right-skewed and has a small amount of outliers. When looking at our Linear Regression model that has been developed below, the Min and Max of the data set are not too different from one another and the same goes for 1Q and 3Q of the dataset as well. These are good signs of normal distribution that we want. The only slight concern is that the model’s Median is quite a far distance from 0.\n\nlm_Price = train(Price ~ .,\n            data = cars,\n            method = \"lm\")\n\nlm_Price\n\nLinear Regression \n\n1436 samples\n  34 predictor\n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 1436, 1436, 1436, 1436, 1436, 1436, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  1721.915  0.7840195  881.3067\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\nsummary(lm_Price)\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5225.2  -620.6   -41.6   575.3  6192.5 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        -2.234e+03  1.796e+03  -1.244 0.213732    \nAge                -2.902e+01  8.801e+00  -3.297 0.001003 ** \nKM                 -1.589e-02  1.085e-03 -14.642  &lt; 2e-16 ***\nHP                  2.230e+01  3.116e+00   7.158 1.33e-12 ***\nCC                 -6.027e-02  7.374e-02  -0.817 0.413890    \nWeight              7.533e+00  1.166e+00   6.460 1.45e-10 ***\nQuarterly_Tax       1.122e+01  1.655e+00   6.779 1.78e-11 ***\nMfg_Year1999        7.023e+02  1.320e+02   5.322 1.20e-07 ***\nMfg_Year2000        1.541e+03  2.315e+02   6.659 3.98e-11 ***\nMfg_Year2001        2.313e+03  3.284e+02   7.044 2.95e-12 ***\nMfg_Year2002        4.489e+03  4.482e+02  10.018  &lt; 2e-16 ***\nMfg_Year2003        6.025e+03  5.338e+02  11.286  &lt; 2e-16 ***\nMfg_Year2004        7.967e+03  6.491e+02  12.274  &lt; 2e-16 ***\nFuel_TypeDiesel     1.002e+03  3.098e+02   3.233 0.001255 ** \nFuel_TypePetrol     1.502e+03  3.242e+02   4.635 3.91e-06 ***\nMet_Color1         -6.757e+01  6.820e+01  -0.991 0.322000    \nColorBlack          6.014e+02  6.166e+02   0.975 0.329563    \nColorBlue           4.780e+02  6.158e+02   0.776 0.437672    \nColorGreen          3.340e+02  6.169e+02   0.541 0.588304    \nColorGrey           6.352e+02  6.160e+02   1.031 0.302672    \nColorRed            4.733e+02  6.159e+02   0.768 0.442344    \nColorSilver         5.582e+02  6.211e+02   0.899 0.368984    \nColorViolet         2.224e+02  8.171e+02   0.272 0.785563    \nColorWhite         -2.128e+02  6.450e+02  -0.330 0.741514    \nColorYellow         5.609e+02  8.670e+02   0.647 0.517744    \nAutomatic1          4.620e+02  1.325e+02   3.488 0.000503 ***\nDoors3             -4.380e+02  7.626e+02  -0.574 0.565842    \nDoors4             -4.341e+02  7.679e+02  -0.565 0.571948    \nDoors5             -2.798e+02  7.641e+02  -0.366 0.714255    \nGears4             -1.128e+02  1.328e+03  -0.085 0.932337    \nGears5              5.834e+02  7.879e+02   0.741 0.459109    \nGears6              1.011e+03  8.101e+02   1.247 0.212449    \nMfr_Guarantee1      2.968e+02  6.310e+01   4.704 2.81e-06 ***\nBOVAG_Guarantee1    4.583e+02  1.102e+02   4.158 3.40e-05 ***\nGuarantee_Period6   5.197e+02  1.669e+02   3.114 0.001884 ** \nGuarantee_Period12  7.091e+02  1.671e+02   4.244 2.34e-05 ***\nGuarantee_Period13  3.974e+03  1.097e+03   3.622 0.000303 ***\nGuarantee_Period18  2.862e+03  1.078e+03   2.655 0.008023 ** \nGuarantee_Period20  1.387e+03  1.079e+03   1.285 0.198945    \nGuarantee_Period24  4.352e+02  5.627e+02   0.773 0.439421    \nGuarantee_Period28  1.668e+03  1.080e+03   1.545 0.122632    \nGuarantee_Period36  1.138e+02  5.743e+02   0.198 0.842910    \nABS1               -4.491e+01  1.126e+02  -0.399 0.689972    \nAirbag_11           1.535e+02  2.156e+02   0.712 0.476656    \nAirbag_21          -1.855e+00  1.189e+02  -0.016 0.987554    \nAirco1              2.356e+02  7.626e+01   3.089 0.002047 ** \nAutomatic_airco1    1.918e+03  1.683e+02  11.395  &lt; 2e-16 ***\nBoardcomputer1     -1.630e+02  1.092e+02  -1.493 0.135603    \nCD_Player1          2.170e+02  8.486e+01   2.557 0.010676 *  \nCentral_Lock1      -9.588e+01  1.210e+02  -0.792 0.428242    \nPowered_Windows1    3.032e+02  1.214e+02   2.497 0.012649 *  \nPower_Steering1    -1.597e+02  2.421e+02  -0.660 0.509435    \nRadio1              5.816e+02  6.244e+02   0.931 0.351791    \nMistlamps1          2.806e+01  9.299e+01   0.302 0.762902    \nSport_Model1       -3.963e+01  8.145e+01  -0.487 0.626634    \nBackseat_Divider1  -4.920e+01  1.308e+02  -0.376 0.706793    \nMetallic_Rim1       1.146e+02  8.199e+01   1.398 0.162472    \nRadio_cassette1    -6.200e+02  6.239e+02  -0.994 0.320488    \nParking_Assistant1 -4.225e+02  5.400e+02  -0.782 0.434109    \nTow_Bar1           -1.724e+02  6.709e+01  -2.570 0.010263 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1049 on 1376 degrees of freedom\nMultiple R-squared:  0.9197,    Adjusted R-squared:  0.9163 \nF-statistic: 267.2 on 59 and 1376 DF,  p-value: &lt; 2.2e-16\n\n\nWhen creating a Histogram, we can take a look at the Price feature.\n\ncars %&gt;%\n  ggplot(aes(Price)) +\n  geom_histogram(color = \"black\", bg = \"skyblue\") +\n  labs(title = \"Distribution of Selling Prices\",\n       x = \"Selling price\",\n       y = \"Count of cars\") +\n  theme_classic()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-4",
    "href": "posts/Problem Set 3/index.html#part-4",
    "title": "Problem Set 3",
    "section": "Part 4",
    "text": "Part 4\nWe can see that Age and KM have quite a strong negative relationship with Price.\n\ncaret::featurePlot(keep(cars, is.numeric), cars$Price, plot = \"scatter\")"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-5",
    "href": "posts/Problem Set 3/index.html#part-5",
    "title": "Problem Set 3",
    "section": "Part 5",
    "text": "Part 5\n\ncars %&gt;%\n  keep(is.numeric) %&gt;%\n  ggpairs()"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-6",
    "href": "posts/Problem Set 3/index.html#part-6",
    "title": "Problem Set 3",
    "section": "Part 6",
    "text": "Part 6\nWe can convert our categorical variables into dummy variables.\n\ncars_dum = dummy(cars, int = TRUE)\ncars_num = cars %&gt;%\n  keep(is.numeric)\ncars = bind_cols(cars_num, cars_dum)\nrm(cars_dum, cars_num)\n\nNow we will partition our data.\n\n#DATA PARTITION \nset.seed(4532)\nsamp = createDataPartition(cars$Price, p=0.7, list = FALSE)\ntraining = cars[samp, ]\ntesting = cars[-samp, ]\nrm(samp)"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-7",
    "href": "posts/Problem Set 3/index.html#part-7",
    "title": "Problem Set 3",
    "section": "Part 7",
    "text": "Part 7\nPre-pruning sets limitations and boundaries to the tree and limits the overall complexity of it. Whereas post-pruning allows the tree to continue growing nwith justifications being made to it. As we justify which data to use, we get to tune and retrain the tree.\n\ntrain_model = train(Price ~ .,\n                   data = training,\n                   method = \"rpart\",\n                   trControl = trainControl(method = \"cv\", number = 10),\n                   tuneGrid = expand.grid(cp = seq(0.0, 0.01, 0.0001)),\n                   control = rpart.control(minbucket = 1)\n                   )\nplot(train_model)\n\n\n\n\n\nrpart.plot(train_model$finalModel)"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-8",
    "href": "posts/Problem Set 3/index.html#part-8",
    "title": "Problem Set 3",
    "section": "Part 8",
    "text": "Part 8\nWhen looking at our feature importance, we can determine that Age, KM, and Weight are the only three variables that have a difference in importance when compared to all other variables. We could possibly remove all other variables in the dataset.\n\nlibrary(iml)\nlibrary(patchwork)\n\ntree_predictor = iml::Predictor$new(train_model,\n                                    data = testing,\n                                    y = testing$Price)\n\ntree_imp = iml::FeatureImp$new(tree_predictor, loss = \"rmse\", compare = \"ratio\")\nplot(tree_imp)\n\n\n\n\nWe can see that Age has the highest importance when compared to other feautures in the dataset.\n\ntree_imp$results %&gt;%\n  filter(importance &gt; 1)\n\n            feature importance.05 importance importance.95 permutation.error\n1               Age     3.1465977   3.253068      3.369671          4212.437\n2            Weight     1.1531487   1.171143      1.203610          1516.527\n3                KM     1.0826273   1.105717      1.116607          1431.806\n4    Metallic_Rim_0     1.0317436   1.079041      1.117105          1397.264\n5 Powered_Windows_0     1.0394016   1.045327      1.057137          1353.607\n6 Automatic_airco_0     1.0110262   1.030380      1.034653          1334.251\n7     Quarterly_Tax     1.0185878   1.023070      1.037765          1324.786\n8 Powered_Windows_1     0.9828574   1.009893      1.029864          1307.722\n9                HP     0.9926295   1.008777      1.014996          1306.278"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-9",
    "href": "posts/Problem Set 3/index.html#part-9",
    "title": "Problem Set 3",
    "section": "Part 9",
    "text": "Part 9\n\ntrain_new = dplyr::select(training, Age, Weight, KM, Metallic_Rim_0, Powered_Windows_0, Quarterly_Tax, Automatic_airco_0, HP, Powered_Windows_0, Price)\n\nnew_tree = caret::train(Price ~ .,\n                        data = train_new,\n                        method = \"rpart\",\n                        trControl = trainControl(method = \"cv\", number = 10),\n                        tuneGrid = expand.grid(cp = seq(0.0, 0.01, 0.0001)),\n                        control = rpart.control(minbucket = 1)\n                        )\nplot(new_tree)\n\n\n\n\n\nrpart.plot(new_tree$finalModel)"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-10",
    "href": "posts/Problem Set 3/index.html#part-10",
    "title": "Problem Set 3",
    "section": "Part 10",
    "text": "Part 10\n\ntrain_error = postResample(predict(new_tree, training), training$Price)[[\"RMSE\"]]\n\ncv_error = min(new_tree$results$RMSE)\n\ntest_error = postResample(predict(new_tree, testing), testing$Price)[[\"RMSE\"]]\n\ndata.frame(\n  \"Error Source\" = c(\"Training\", \"Cross-Validation\", \"Testing\"),\n  \"RMSE\" = c(train_error, cv_error, test_error)\n)\n\n      Error.Source      RMSE\n1         Training  929.0272\n2 Cross-Validation 1164.5327\n3          Testing 1264.3797\n\n\nFrom our results, we can see that the model with the highest RMSE is for our Testing data. This makes sense for our Testing data is not being trained and fitted as well like our Training data. Both the Cross-Validation and Testing RMSE values are quite close to one another."
  },
  {
    "objectID": "posts/Problem Set 4/index.html",
    "href": "posts/Problem Set 4/index.html",
    "title": "Problem Set 4",
    "section": "",
    "text": "rm(list = ls())\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(glmnet)\nlibrary(rpart)\nlibrary(pROC)\nlibrary(ggthemes)\nlibrary(AppliedPredictiveModeling)\nlibrary(performanceEstimation)"
  },
  {
    "objectID": "posts/Problem Set 4/index.html#part-1",
    "href": "posts/Problem Set 4/index.html#part-1",
    "title": "Problem Set 4",
    "section": "Part 1",
    "text": "Part 1\nClassification is the right approach for NVO’s problem. The reason for this is because we are trying to predict discrete values using this dataset. These values are whether or not a person will respond to a mailing. We are not focusing on the predicted numbers and their root mean squared errors like we do with regression for we are looking at the accuracy of the dataset. Our goal is to target and focus on individuals that will likely respond and increase NVO’s response rate.\n\ndonors = read_csv(\"donors.csv\")\n\nRows: 95412 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): state, urbanicity, socioEconomicStatus, gender\ndbl (12): age, numberChildren, incomeRating, wealthRating, mailOrderPurchase...\nlgl  (6): inHouseDonor, plannedGivingDonor, sweepstakesDonor, P3Donor, isHom...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(donors)\n\nRows: 95,412\nColumns: 22\n$ age                     &lt;dbl&gt; 60, 46, NA, 70, 78, NA, 38, NA, NA, 65, NA, 75…\n$ numberChildren          &lt;dbl&gt; NA, 1, NA, NA, 1, NA, 1, NA, NA, NA, NA, NA, 2…\n$ incomeRating            &lt;dbl&gt; NA, 6, 3, 1, 3, NA, 4, 2, 3, NA, 2, 1, 4, NA, …\n$ wealthRating            &lt;dbl&gt; NA, 9, 1, 4, 2, NA, 6, 9, 2, NA, 0, 5, 2, NA, …\n$ mailOrderPurchases      &lt;dbl&gt; 0, 16, 2, 2, 60, 0, 0, 1, 0, 0, 0, 3, 16, 0, 1…\n$ totalGivingAmount       &lt;dbl&gt; 240, 47, 202, 109, 254, 51, 107, 31, 199, 28, …\n$ numberGifts             &lt;dbl&gt; 31, 3, 27, 16, 37, 4, 14, 5, 11, 3, 1, 2, 9, 1…\n$ smallestGiftAmount      &lt;dbl&gt; 5, 10, 2, 2, 3, 10, 3, 5, 10, 3, 20, 10, 4, 5,…\n$ largestGiftAmount       &lt;dbl&gt; 12, 25, 16, 11, 15, 16, 12, 11, 22, 15, 20, 15…\n$ averageGiftAmount       &lt;dbl&gt; 7.741935, 15.666667, 7.481481, 6.812500, 6.864…\n$ yearsSinceFirstDonation &lt;dbl&gt; 8, 3, 7, 10, 11, 3, 10, 3, 9, 3, 1, 1, 8, 5, 4…\n$ monthsSinceLastDonation &lt;dbl&gt; 14, 14, 14, 14, 13, 20, 22, 18, 19, 22, 12, 14…\n$ inHouseDonor            &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE…\n$ plannedGivingDonor      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ sweepstakesDonor        &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ P3Donor                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE…\n$ state                   &lt;chr&gt; \"IL\", \"CA\", \"NC\", \"CA\", \"FL\", \"AL\", \"IN\", \"LA\"…\n$ urbanicity              &lt;chr&gt; \"town\", \"suburb\", \"rural\", \"rural\", \"suburb\", …\n$ socioEconomicStatus     &lt;chr&gt; \"average\", \"highest\", \"average\", \"average\", \"a…\n$ isHomeowner             &lt;lgl&gt; NA, TRUE, NA, NA, TRUE, NA, TRUE, NA, NA, NA, …\n$ gender                  &lt;chr&gt; \"female\", \"male\", \"male\", \"female\", \"female\", …\n$ respondedMailing        &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…"
  },
  {
    "objectID": "posts/Problem Set 4/index.html#part-2",
    "href": "posts/Problem Set 4/index.html#part-2",
    "title": "Problem Set 4",
    "section": "Part 2",
    "text": "Part 2\nThis classifier being built to identify potential donors for NVO will be better to use due to the accuracy it could develop. The reason for this is due to us having the ability to exclude features that may not have much significance to help make predictions in the dataset. As we select certain features to use and to remove, we can train and split the data up further to aim for the accuracy we hope to achieve. We get to focus on the separate classes within the data and not just all of the data within the dataset as a whole.\n\n# Preparing Data\ndonors = donors %&gt;%\n  rename(Responded = `respondedMailing`) %&gt;%\n  mutate(Responded = factor(ifelse(Responded == TRUE, \"1\", \"0\")))\n\n\nsummary(donors)\n\n      age        numberChildren   incomeRating    wealthRating  \n Min.   : 1.00   Min.   :1.00    Min.   :1.000   Min.   :0.00   \n 1st Qu.:48.00   1st Qu.:1.00    1st Qu.:2.000   1st Qu.:3.00   \n Median :62.00   Median :1.00    Median :4.000   Median :6.00   \n Mean   :61.61   Mean   :1.53    Mean   :3.886   Mean   :5.35   \n 3rd Qu.:75.00   3rd Qu.:2.00    3rd Qu.:5.000   3rd Qu.:8.00   \n Max.   :98.00   Max.   :7.00    Max.   :7.000   Max.   :9.00   \n NA's   :23665   NA's   :83026   NA's   :21286   NA's   :44732  \n mailOrderPurchases totalGivingAmount  numberGifts      smallestGiftAmount\n Min.   :  0.000    Min.   :  13.0    Min.   :  1.000   Min.   :   0.000  \n 1st Qu.:  0.000    1st Qu.:  40.0    1st Qu.:  3.000   1st Qu.:   3.000  \n Median :  0.000    Median :  78.0    Median :  7.000   Median :   5.000  \n Mean   :  3.321    Mean   : 104.5    Mean   :  9.602   Mean   :   7.934  \n 3rd Qu.:  3.000    3rd Qu.: 131.0    3rd Qu.: 13.000   3rd Qu.:  10.000  \n Max.   :241.000    Max.   :9485.0    Max.   :237.000   Max.   :1000.000  \n                                                                          \n largestGiftAmount averageGiftAmount  yearsSinceFirstDonation\n Min.   :   5      Min.   :   1.286   Min.   : 0.000         \n 1st Qu.:  14      1st Qu.:   8.385   1st Qu.: 2.000         \n Median :  17      Median :  11.636   Median : 5.000         \n Mean   :  20      Mean   :  13.348   Mean   : 5.596         \n 3rd Qu.:  23      3rd Qu.:  15.478   3rd Qu.: 9.000         \n Max.   :5000      Max.   :1000.000   Max.   :13.000         \n                                                             \n monthsSinceLastDonation inHouseDonor    plannedGivingDonor sweepstakesDonor\n Min.   : 0.00           Mode :logical   Mode :logical      Mode :logical   \n 1st Qu.:12.00           FALSE:88709     FALSE:95298        FALSE:93795     \n Median :14.00           TRUE :6703      TRUE :114          TRUE :1617      \n Mean   :14.36                                                              \n 3rd Qu.:17.00                                                              \n Max.   :23.00                                                              \n                                                                            \n  P3Donor           state            urbanicity        socioEconomicStatus\n Mode :logical   Length:95412       Length:95412       Length:95412       \n FALSE:93395     Class :character   Class :character   Class :character   \n TRUE :2017      Mode  :character   Mode  :character   Mode  :character   \n                                                                          \n                                                                          \n                                                                          \n                                                                          \n isHomeowner       gender          Responded\n Mode:logical   Length:95412       0:90569  \n TRUE:52354     Class :character   1: 4843  \n NA's:43058     Mode  :character"
  },
  {
    "objectID": "posts/Problem Set 4/index.html#part-3",
    "href": "posts/Problem Set 4/index.html#part-3",
    "title": "Problem Set 4",
    "section": "Part 3",
    "text": "Part 3\nThe most important measures from the confusion matrix that I will use to evaluate the classifier performance will be Accuracy, Sensitivity (Recall), and Post Pred Value (Precision). Since our goal is to not miss out on individuals who will actually respond, being more accurate is not going to be the only outstanding factor of our model. Recall and Precision will allow us to see the percentages of actual positives and how often these positive predictions are correct. As we relate the importance of these measures to mailer response rate and maximizing donation opportunities, positive predictions are the individuals that are predicted to achieve our goal, respond to our mailings.\nThere seems to be many missing values within the dataset. We can see that there is a class imbalance, for a very small percentage of people had actually responded.\nI decided to not include the variables of states, numberChildren, and wealthRating due to the large amount of missing values and lack of usefulness they have to our predictions. For variables with only a few missing values, I decided to fill in those missing values with the median of their variable for quantitative variables or dropped the missing variables for categorical variables in the dataset.\n\ndonors = donors %&gt;%\n  select(-numberChildren, -wealthRating, -plannedGivingDonor, -state) %&gt;%\n  mutate(isHomeowner = ifelse(is.na(isHomeowner), \"UNKNOWN\", isHomeowner)) %&gt;%\n  mutate(age = ifelse(is.na(age), median(age, na.rm = TRUE), age)) %&gt;%\n  mutate(incomeRating = ifelse(is.na(incomeRating), median(incomeRating, na.rm = TRUE), incomeRating)) %&gt;%\n  drop_na() %&gt;%\n  mutate_if(is.character, .funs=factor) %&gt;%\n  mutate_if(is.logical, .funs=factor)\n\n\nsummary(donors)\n\n      age         incomeRating   mailOrderPurchases totalGivingAmount\n Min.   : 1.00   Min.   :1.000   Min.   :  0.000    Min.   :  13.0   \n 1st Qu.:52.00   1st Qu.:3.000   1st Qu.:  0.000    1st Qu.:  40.0   \n Median :62.00   Median :4.000   Median :  0.000    Median :  77.0   \n Mean   :61.57   Mean   :3.911   Mean   :  3.449    Mean   : 102.3   \n 3rd Qu.:72.00   3rd Qu.:5.000   3rd Qu.:  3.000    3rd Qu.: 130.0   \n Max.   :98.00   Max.   :7.000   Max.   :241.000    Max.   :5674.9   \n  numberGifts      smallestGiftAmount largestGiftAmount averageGiftAmount\n Min.   :  1.000   Min.   :  0.000    Min.   :   5.00   Min.   :  1.286  \n 1st Qu.:  3.000   1st Qu.:  3.000    1st Qu.:  14.00   1st Qu.:  8.400  \n Median :  7.000   Median :  5.000    Median :  17.00   Median : 11.667  \n Mean   :  9.475   Mean   :  7.946    Mean   :  19.75   Mean   : 13.277  \n 3rd Qu.: 13.000   3rd Qu.: 10.000    3rd Qu.:  23.00   3rd Qu.: 15.500  \n Max.   :237.000   Max.   :500.000    Max.   :1000.00   Max.   :506.500  \n yearsSinceFirstDonation monthsSinceLastDonation inHouseDonor  sweepstakesDonor\n Min.   : 0.000          Min.   : 0.0            FALSE:82937   FALSE:87242     \n 1st Qu.: 2.000          1st Qu.:12.0            TRUE : 5656   TRUE : 1351     \n Median : 5.000          Median :14.0                                          \n Mean   : 5.547          Mean   :14.4                                          \n 3rd Qu.: 9.000          3rd Qu.:17.0                                          \n Max.   :13.000          Max.   :23.0                                          \n  P3Donor       urbanicity    socioEconomicStatus  isHomeowner   \n FALSE:86918   city  :18819   average:46367       TRUE   :50357  \n TRUE : 1675   rural :18777   highest:27050       UNKNOWN:38236  \n               suburb:20848   lowest :15176                      \n               town  :18685                                      \n               urban :11464                                      \n                                                                 \n    gender      Responded\n female:50085   0:84115  \n joint :  354   1: 4478  \n male  :38154            \n                         \n                         \n                         \n\n\n\ntransparentTheme(trans = .9)\nfeaturePlot(x = keep(donors, is.numeric),\n            y = donors$Responded,\n            plot = \"box\",\n            scales = list(y = list(relation = \"free\"),\n                          x = list(rot = 90)),\n            layout = c(4, 1),\n            auto.key = list(columns = 2))\n\n\n\n\n\n\n\n\n\n\n\ndonors %&gt;%\n  keep(is.numeric) %&gt;%\n  cor() %&gt;%\n  corrplot::corrplot(., method = \"number\", type = \"lower\", number.cex = 0.6, tl.cex = 0.7)\n\n\n\n\nDue to high correlations being developed with the use of these variables, we could drop them from the dataset as we try to determine which variables would be most efficient in helping us find predictions.\n\ndonors = donors %&gt;%\n  select(-smallestGiftAmount, -largestGiftAmount)\n\n\n# Partition our data\nset.seed(455)\nsamp = caret::createDataPartition(donors$Responded, p = 0.7, list = FALSE)\ntrain = donors[samp,]\ntest = donors[-samp,]\nrm(samp)\n\n\n# Observing Class Imbalance\ntable(train$Responded)\n\n\n    0     1 \n58881  3135 \n\n\n\n# Smote\nset.seed(4959)\nsmote_train = smote(Responded ~ .,\n                    data = train,\n                    perc.over = 8,\n                    perc.under = 2)\n\ntable(smote_train$Responded)\n\n\n    0     1 \n50160 28215"
  },
  {
    "objectID": "posts/Problem Set 4/index.html#part-4",
    "href": "posts/Problem Set 4/index.html#part-4",
    "title": "Problem Set 4",
    "section": "Part 4",
    "text": "Part 4\n\n# LASSO\ntrain.X = model.matrix(Responded ~ 0 + ., smote_train)\ntrain.Y = as.numeric(smote_train$Responded == \"1\")\n\nlasso = cv.glmnet(x = train.X, y = train.Y, type.measure = \"class\",\n                       nfolds = 20, family = 'binomial')\nplot(lasso)\n\n\n\n\n\ncoef(lasso, s = 'lambda.min')\n\n22 x 1 sparse Matrix of class \"dgCMatrix\"\n                                      s1\n(Intercept)                 0.0464804674\nage                         .           \nincomeRating                0.0429794838\nmailOrderPurchases          .           \ntotalGivingAmount           .           \nnumberGifts                 0.0133683503\naverageGiftAmount          -0.0273379096\nyearsSinceFirstDonation     0.0001557296\nmonthsSinceLastDonation    -0.0424595922\ninHouseDonorFALSE           .           \ninHouseDonorTRUE            .           \nsweepstakesDonorTRUE       -0.2936363281\nP3DonorTRUE                 .           \nurbanicityrural             .           \nurbanicitysuburb            0.0304234865\nurbanicitytown              0.0167131580\nurbanicityurban            -0.0699822042\nsocioEconomicStatushighest  0.0895240586\nsocioEconomicStatuslowest  -0.1593449705\nisHomeownerUNKNOWN          .           \ngenderjoint                 .           \ngendermale                  ."
  },
  {
    "objectID": "posts/Problem Set 4/index.html#part-5",
    "href": "posts/Problem Set 4/index.html#part-5",
    "title": "Problem Set 4",
    "section": "Part 5",
    "text": "Part 5\n\n# Smote Tree\nctrl = caret::trainControl(method = \"cv\", number = 10)\nsmote_tree = caret::train(Responded ~ .,\n             data = smote_train,\n             method = \"rpart\",\n             metric = \"Accuracy\",\n             trControl = ctrl,\n             tuneGrid = expand.grid(cp = seq(0.0, 0.1, 0.005)),\n             control = rpart.control(maxdepth = 10, minsplit = 1, minbucket = 30))\n\nplot(smote_tree)\n\n\n\n\n\nrpart.plot::rpart.plot(smote_tree$finalModel)"
  },
  {
    "objectID": "posts/Problem Set 4/index.html#part-6",
    "href": "posts/Problem Set 4/index.html#part-6",
    "title": "Problem Set 4",
    "section": "Part 6",
    "text": "Part 6\n\n# Smote Evaluation\ntest.X = model.matrix(Responded ~ 0 + ., test)\nlasso.pred.class = as.factor(predict(lasso, test.X, s = 'lambda.min', type = 'class'))\ntree.pred.class = as.factor(predict(smote_tree, test))\n\ncm.lasso = confusionMatrix(data = lasso.pred.class,\n                           reference = test$Responded,\n                           positive = \"1\")\n\ncm.tree = confusionMatrix(data = tree.pred.class,\n                          reference = test$Responded,\n                          positive = \"1\")\n\n\nprint(cm.lasso)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 24624  1261\n         1   610    82\n                                          \n               Accuracy : 0.9296          \n                 95% CI : (0.9265, 0.9326)\n    No Information Rate : 0.9495          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.0479          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.061057        \n            Specificity : 0.975826        \n         Pos Pred Value : 0.118497        \n         Neg Pred Value : 0.951285        \n             Prevalence : 0.050532        \n         Detection Rate : 0.003085        \n   Detection Prevalence : 0.026038        \n      Balanced Accuracy : 0.518442        \n                                          \n       'Positive' Class : 1               \n                                          \n\n\n\nprint(cm.tree)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 25166  1341\n         1    68     2\n                                          \n               Accuracy : 0.947           \n                 95% CI : (0.9442, 0.9496)\n    No Information Rate : 0.9495          \n    P-Value [Acc &gt; NIR] : 0.968           \n                                          \n                  Kappa : -0.0022         \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 1.489e-03       \n            Specificity : 9.973e-01       \n         Pos Pred Value : 2.857e-02       \n         Neg Pred Value : 9.494e-01       \n             Prevalence : 5.053e-02       \n         Detection Rate : 7.525e-05       \n   Detection Prevalence : 2.634e-03       \n      Balanced Accuracy : 4.994e-01       \n                                          \n       'Positive' Class : 1"
  },
  {
    "objectID": "posts/Problem Set 4/index.html#part-7",
    "href": "posts/Problem Set 4/index.html#part-7",
    "title": "Problem Set 4",
    "section": "Part 7",
    "text": "Part 7\n\nlasso.probs = predict(lasso, test.X, s = 'lambda.min', type = 'response')\ntree.probs = predict(smote_tree, test, type = 'prob')[,2]\n\nThis ROC plot can help me tell NVO that the LASSO model ended up performing better than the Decision Tree model. Both are pretty close when it comes to their results and their placement to the baseline on the plot, but the LASSO curve is shown to be further away from the baseline, thus having a better predictive performance.\n\npar(pty = \"s\")\nlasso_roc = roc(test$Responded ~ lasso.probs,\n                plot = TRUE, print.auc = TRUE, print.auc.x = 0.3, print.auc.y = 0.35,\n                col = \"skyblue\", lwd = 3, legacy.axes = TRUE)\n\ntree_roc = roc(test$Responded ~ tree.probs,\n                plot = TRUE, print.auc = TRUE, print.auc.x = 0.3, print.auc.y = 0.28,\n                col = \"magenta\", lwd = 3, legacy.axes = TRUE, add = TRUE)\n\nlegend(\"bottomright\", legend = c(\"LASSO\", \"Decision Tree\"),\n       col = c(\"skyblue\", \"magenta\"), lwd = 3)"
  },
  {
    "objectID": "posts/Problem Set 4/index.html#part-8",
    "href": "posts/Problem Set 4/index.html#part-8",
    "title": "Problem Set 4",
    "section": "Part 8",
    "text": "Part 8\nPrecision-Recall Chart\n\nprc = pROC::coords(lasso_roc, ret = c(\"threshold\", \"precision\", \"recall\"))\n\nprc %&gt;%\n  filter(recall &gt; 0.01) %&gt;%\n  ggplot(aes(x = recall, y = precision)) +\n  geom_line(linewidth = 2, color = \"skyblue\") +\n  geom_hline(yintercept = 0.05, color = \"black\", linetype = 'dashed') +\n  labs(title = \"Precision-Recall Curve\",\n       x = \"Recall / Sensitivity / True Positive Rate\",\n       y = \"Precision (Response Rate\") +\n  theme_clean()\n\n\n\n\nCumulative Gain Chart\n\ntest$prob = lasso.probs = predict(lasso, test.X, s = 'lambda.min', type = 'response')\n\n\ntest.cg = test %&gt;%\n  select(Responded, prob) %&gt;%\n  mutate(Responded = ifelse(Responded == \"1\", 1, 0)) %&gt;%\n  arrange(desc(prob)) %&gt;%\n  mutate(pct_dat = row_number() /n(),\n         pct_pos = cumsum(Responded)/sum(Responded))\n\ntest.cg %&gt;%\n  ggplot(aes(x = pct_dat, y = pct_pos)) +\n  geom_abline(intercept = 0, slope = 1,\n              linetype = \"dashed\") +\n  geom_vline(xintercept = 0.5, linetype = \"dotted\", color = \"black\") +\n  geom_line(linewidth = 1.5, color = \"skyblue\") +\n  labs(title = \"Cumulative Gains for LASSO Model\",\n       x = \"Percent of Data Contacted\",\n       y = \"Percent of Responders Captured\") +\n  theme_clean()"
  },
  {
    "objectID": "posts/Problem Set 4/index.html#part-9",
    "href": "posts/Problem Set 4/index.html#part-9",
    "title": "Problem Set 4",
    "section": "Part 9",
    "text": "Part 9\nUsing the charts from parts 6 and 7, the model that resulted as the most accurate was the Decision Tree model, whereas the LASSO model ended up with a higher Kappa. When looking at the Recall (Sensitivity) and Precision (Pos Pred Value), our business problem is trying to identify people who will respond and increase NVO’s response rate as well. While we do aim to not waste materials and effort, we also don’t want to miss out on opportunities. The two measures Sensitivity and Pos Pred Value will help us look at the trade-off we face here.\nThe LASSO model has a higher sensitivity and precision despite a lower accuracy. However, its specificity is lower and its Neg Pred Value is higher. Both of them have small precisions (Pos Pred Value) which means our response rates could be lower. And their sensitivities are low, but data balancing allowed us to improve our model’s ability to learn rules that help us sense the positive class and miss out on opportunities. Both models have high specificity, showing that they can predict who is not gping to respond at a higher rate.\nWe can see that much of the data being found using the LASSO model seems to perform better than the Decision Tree model. If using the data we have developed using the LASSO model data, we can assume that only 6% of our predictions will be correct and that 11.8% of our predicted responders are going to actually respond.\nThe ROC plot also determines that the LASSO model performs better. Both are pretty close when it comes to their results and their placement to the baseline on the plot, but the LASSO curve is shown to be further away from the baseline, thus having a better predictive performance. With such numbers, we determine a bit better on whether or not we want to risk for more opportunities. Based off our numbers, it looks like we will go for more precision than opportunities due to such low numbers."
  },
  {
    "objectID": "posts/Problem Set 7/index.html",
    "href": "posts/Problem Set 7/index.html",
    "title": "Problem Set #7",
    "section": "",
    "text": "rm(list = ls())\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(glmnet)\nlibrary(rpart)\nlibrary(pROC)\nlibrary(ggthemes)\nlibrary(AppliedPredictiveModeling)\nlibrary(performanceEstimation)\npost_crisis = read_csv(\"PostCrisisCV.csv\")\n\nRows: 1657 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (18): Property, LandValue, BuildingValue, Acres, AboveSpace, Basement, D...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(post_crisis)\n\nRows: 1,657\nColumns: 18\n$ Property      &lt;dbl&gt; 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987, 39…\n$ LandValue     &lt;dbl&gt; 18200, 15700, 18400, 16600, 18100, 25900, 23100, 35600, …\n$ BuildingValue &lt;dbl&gt; 81300, 67000, 91500, 88500, 75100, 88900, 159300, 110800…\n$ Acres         &lt;dbl&gt; 0.17, 0.15, 0.53, 0.15, 0.26, 0.30, 0.21, 0.32, 0.16, 0.…\n$ AboveSpace    &lt;dbl&gt; 892, 692, 1068, 1278, 939, 1073, 1136, 1500, 1270, 672, …\n$ Basement      &lt;dbl&gt; 0, 0, 350, 0, 200, 0, 750, 300, 350, 0, 600, 0, 0, 700, …\n$ Deck          &lt;dbl&gt; 364, 0, 0, 160, 192, 0, 100, 0, 0, 0, 48, 0, 0, 0, 0, 0,…\n$ Baths         &lt;dbl&gt; 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 3, 1, 2, 1, 1, 1, 2, 2, 1,…\n$ Toilets       &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…\n$ Fireplaces    &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,…\n$ Beds          &lt;dbl&gt; 2, 2, 3, 4, 3, 3, 2, 4, 4, 2, 4, 2, 3, 2, 2, 1, 3, 4, 2,…\n$ Rooms         &lt;dbl&gt; 3, 2, 2, 2, 2, 2, 4, 3, 2, 2, 2, 2, 4, 2, 2, 2, 3, 4, 3,…\n$ AC            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,…\n$ Age           &lt;dbl&gt; 68.9, 53.8, 52.8, 92.9, 52.9, 13.9, 5.8, 37.8, 61.9, 56.…\n$ Car           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 400, 484, 0, 0, 0, 0, 440, 368, 0, 0, …\n$ PoorCondition &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ GoodCondition &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,…\n$ Price         &lt;dbl&gt; 54175, 92589, 122040, 112289, 95544, 115146, 159569, 158…\npre_crisis = read_csv(\"PreCrisisCV.csv\")\n\nRows: 1978 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (18): Property, LandValue, BuildingValue, Acres, AboveSpace, Basement, D...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(pre_crisis)\n\nRows: 1,978\nColumns: 18\n$ Property      &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ LandValue     &lt;dbl&gt; 15200, 16100, 17900, 25400, 23000, 25900, 23000, 9700, 2…\n$ BuildingValue &lt;dbl&gt; 52400, 123300, 63900, 132400, 99800, 118100, 119100, 602…\n$ Acres         &lt;dbl&gt; 0.26, 0.28, 0.50, 0.26, 0.18, 0.29, 0.26, 0.13, 0.28, 0.…\n$ AboveSpace    &lt;dbl&gt; 695, 1054, 891, 1080, 1020, 1317, 1802, 720, 912, 1019, …\n$ Basement      &lt;dbl&gt; 0, 450, 0, 450, 500, 0, 360, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Deck          &lt;dbl&gt; 0, 180, 180, 64, 0, 100, 0, 0, 0, 310, 0, 168, 120, 0, 0…\n$ Baths         &lt;dbl&gt; 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1,…\n$ Toilets       &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 1, 0,…\n$ Fireplaces    &lt;dbl&gt; 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,…\n$ Beds          &lt;dbl&gt; 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 4, 4, 3, 3, 2, 3, 4, 3, 3,…\n$ Rooms         &lt;dbl&gt; 2, 2, 3, 2, 2, 2, 4, 2, 2, 2, 4, 4, 2, 2, 3, 2, 3, 3, 2,…\n$ AC            &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Age           &lt;dbl&gt; 50.8, 30.8, 53.8, 30.9, 30.7, 16.7, 68.0, 54.7, 62.9, 48…\n$ Car           &lt;dbl&gt; 0, 0, 576, 330, 0, 400, 0, 0, 0, 0, 0, 462, 0, 0, 440, 0…\n$ PoorCondition &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ GoodCondition &lt;dbl&gt; 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,…\n$ Price         &lt;dbl&gt; 74818, 137462, 56850, 137462, 131008, 139973, 141880, 69…\non_market_test = read_csv(\"OnMarketTest-1.csv\")\n\nRows: 2000 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (18): Property, LandValue, BuildingValue, Acres, AboveSpace, Basement, D...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(on_market_test)\n\nRows: 2,000\nColumns: 18\n$ Property      &lt;dbl&gt; 3636, 3637, 3638, 3639, 3640, 3641, 3642, 3643, 3644, 36…\n$ LandValue     &lt;dbl&gt; 15500, 20300, 20100, 4000, 10400, 7300, 16200, 2900, 165…\n$ BuildingValue &lt;dbl&gt; 73700, 69600, 56700, 28800, 39300, 41700, 38700, 34500, …\n$ Acres         &lt;dbl&gt; 0.160, 0.178, 0.223, 0.131, 0.152, 0.081, 0.152, 0.058, …\n$ AboveSpace    &lt;dbl&gt; 912, 768, 660, 865, 617, 768, 792, 918, 905, 704, 876, 9…\n$ Basement      &lt;dbl&gt; 200, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 298, 0,…\n$ Deck          &lt;dbl&gt; 0, 180, 0, 112, 0, 0, 0, 0, 0, 224, 328, 108, 510, 316, …\n$ Baths         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3,…\n$ Toilets       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,…\n$ Fireplaces    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3,…\n$ Beds          &lt;dbl&gt; 3, 2, 1, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3,…\n$ Rooms         &lt;dbl&gt; 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 3, 4,…\n$ AC            &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,…\n$ Age           &lt;dbl&gt; 55.77808, 49.77260, 61.78082, 88.80000, 58.78082, 90.802…\n$ Car           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 591, 0, 0, 0, 0, 0, …\n$ PoorCondition &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ GoodCondition &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,…\n$ Price         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…"
  },
  {
    "objectID": "posts/Problem Set 7/index.html#part-1",
    "href": "posts/Problem Set 7/index.html#part-1",
    "title": "Problem Set #7",
    "section": "Part 1",
    "text": "Part 1"
  },
  {
    "objectID": "posts/Problem Set 7/index.html#part-2",
    "href": "posts/Problem Set 7/index.html#part-2",
    "title": "Problem Set #7",
    "section": "Part 2",
    "text": "Part 2\n\nAre the locations of these homes also being included and used as a factor to determine predictions of selling prices in the datatset?\nHow much does it cost to generally flip a home? What cost are involved and how much time is it going to take to complete the overall process?"
  },
  {
    "objectID": "posts/Problem Set 7/index.html#part-3",
    "href": "posts/Problem Set 7/index.html#part-3",
    "title": "Problem Set #7",
    "section": "Part 3",
    "text": "Part 3"
  },
  {
    "objectID": "posts/Problem Set 7/index.html#part-4",
    "href": "posts/Problem Set 7/index.html#part-4",
    "title": "Problem Set #7",
    "section": "Part 4",
    "text": "Part 4"
  },
  {
    "objectID": "posts/Problem Set 7/index.html#part-5",
    "href": "posts/Problem Set 7/index.html#part-5",
    "title": "Problem Set #7",
    "section": "Part 5",
    "text": "Part 5"
  },
  {
    "objectID": "posts/Problem Set 7/index.html#part-6",
    "href": "posts/Problem Set 7/index.html#part-6",
    "title": "Problem Set #7",
    "section": "Part 6",
    "text": "Part 6"
  }
]