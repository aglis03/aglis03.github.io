[
  {
    "objectID": "posts/Problem Set 2/index.html",
    "href": "posts/Problem Set 2/index.html",
    "title": "Data Mining: Problem Set 2",
    "section": "",
    "text": "Rows: 731\nColumns: 10\n$ date        &lt;date&gt; 2011-01-01, 2011-01-02, 2011-01-03, 2011-01-04, 2011-01-0…\n$ season      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ holiday     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n$ weekday     &lt;dbl&gt; 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4…\n$ weather     &lt;dbl&gt; 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2…\n$ temperature &lt;dbl&gt; 46.71653, 48.35024, 34.21239, 34.52000, 36.80056, 34.88784…\n$ realfeel    &lt;dbl&gt; 46.39865, 45.22419, 25.70131, 28.40009, 30.43728, NA, 28.0…\n$ humidity    &lt;dbl&gt; 0.805833, 0.696087, 0.437273, 0.590435, 0.436957, 0.518261…\n$ windspeed   &lt;dbl&gt; 6.679665, 10.347140, 10.337565, 6.673420, 7.780994, 3.7287…\n$ rentals     &lt;dbl&gt; 985, 801, 1349, 1562, 1600, 1606, 1510, 959, 822, 1321, 12…"
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-1",
    "href": "posts/Problem Set 2/index.html#step-1",
    "title": "Data Mining: Problem Set 2",
    "section": "",
    "text": "Rows: 731\nColumns: 10\n$ date        &lt;date&gt; 2011-01-01, 2011-01-02, 2011-01-03, 2011-01-04, 2011-01-0…\n$ season      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ holiday     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n$ weekday     &lt;dbl&gt; 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4…\n$ weather     &lt;dbl&gt; 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2…\n$ temperature &lt;dbl&gt; 46.71653, 48.35024, 34.21239, 34.52000, 36.80056, 34.88784…\n$ realfeel    &lt;dbl&gt; 46.39865, 45.22419, 25.70131, 28.40009, 30.43728, NA, 28.0…\n$ humidity    &lt;dbl&gt; 0.805833, 0.696087, 0.437273, 0.590435, 0.436957, 0.518261…\n$ windspeed   &lt;dbl&gt; 6.679665, 10.347140, 10.337565, 6.673420, 7.780994, 3.7287…\n$ rentals     &lt;dbl&gt; 985, 801, 1349, 1562, 1600, 1606, 1510, 959, 822, 1321, 12…"
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-2",
    "href": "posts/Problem Set 2/index.html#step-2",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 2",
    "text": "Step 2\nWhen looking at our data that has been marked as numeric data types, there are a few variables that are not truly represented as numeric. These variables are season, holiday, weekday and weather. For example, the table below represents the numbers of observations for the variable season, for the total of observations within each month and season is shown. The numbers are better off added up to see the total observations within each category, not to compare the results and summmaries of each.\n\n\n      month\nseason  1  2  3  4  5  6  7  8  9 10 11 12\n     1 62 57 40  0  0  0  0  0  0  0  0 22\n     2  0  0 22 60 62 40  0  0  0  0  0  0\n     3  0  0  0  0  0 20 62 62 44  0  0  0\n     4  0  0  0  0  0  0  0  0 16 62 60 40\n\n\nFrom the table above, we can turn these variables into factors and get even more specific with season by specifying which season such as Winter, Spring, Summer and Fall is assigned to which number. I have assigned the seasons as, Winter = 1, Spring - 2, Summer = 3 and Fall = 4, for the table helps create the assumptions on which numbers would best fit with specific months and seasons. The variables of holiday, weekday and weather are currently numeric but better off represented as categorical data. These values represents a specific answer and are known as nominal data types. When it comes to holiday we can see that the numbers of 0 and 1 indicates that the data is either considered a holiday or not, so creating two groups for those numbers were easy to replace and specify. Now for weekday, changing the the numeric values into the days of the week such as: Sunday = 0, Monday = 1, Tuesday - 2, Wednesday = 3, Thursday = 4, Friday = 5 and Saturday = 6, will help indicate specific days that rentals took place throughout the week."
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-3",
    "href": "posts/Problem Set 2/index.html#step-3",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 3",
    "text": "Step 3\nNow that we’ve got everything properly recognized as numeric or factor, we can use summary() to look at some basic statistics and also scout out missing values. To make things easier to read, we’ll divide summaries by numeric and factor data types.\nThis data is showcasing our numeric values.\n\n\n  temperature       realfeel         humidity        windspeed      \n Min.   :22.60   Min.   : 12.59   Min.   :0.0000   Min.   : 0.9322  \n 1st Qu.:46.12   1st Qu.: 43.38   1st Qu.:0.5200   1st Qu.: 5.6182  \n Median :59.76   Median : 61.25   Median :0.6267   Median : 7.5343  \n Mean   :59.51   Mean   : 59.60   Mean   :0.6279   Mean   : 7.9303  \n 3rd Qu.:73.05   3rd Qu.: 75.43   3rd Qu.:0.7302   3rd Qu.: 9.7092  \n Max.   :90.50   Max.   :103.10   Max.   :0.9725   Max.   :21.1266  \n                 NA's   :27                                         \n    rentals    \n Min.   :  22  \n 1st Qu.:3152  \n Median :4548  \n Mean   :4504  \n 3rd Qu.:5956  \n Max.   :8714  \n               \n\n\nThis data is showcasing our factor values that we had just developed.\n\n\n    season    holiday        weekday    weather\n Winter:181   Yes:710   Sunday   :105   1:463  \n Spring:184   No : 21   Monday   :105   2:247  \n Summer:188             Tuesday  :104   3: 21  \n Fall  :178             Wednesday:104          \n                        Thursday :104          \n                        Friday   :104          \n                        Saturday :105          \n\n\nFor the realfeel variable in the set of numeric variables, we are missing 27 values. These missing values arr originally shown in our dataset as NA. We will impute those missing values, meaning we will fill in numbers in the blank spots.\nNow, lets impute the missing values and compare to our original data.\nNow we can compare the resulting distributions.\n\n\n    realfeel      realfeel_orig   \n Min.   : 12.59   Min.   : 12.59  \n 1st Qu.: 43.80   1st Qu.: 43.38  \n Median : 61.25   Median : 61.25  \n Mean   : 59.66   Mean   : 59.60  \n 3rd Qu.: 74.98   3rd Qu.: 75.43  \n Max.   :103.10   Max.   :103.10  \n                  NA's   :27      \n\n\nLooking at the above distributions, we see that realfeel doesn’t have any missing values and has the same median and a very similar mean. Nothing else has changed expect for the 1st and 3rd quartile values have shifted a bit.\nStep 4\nRentals appears to encode the total numbers of bike rentals that occurred on a given date. This is count data. We can use both descriptive statistics as well as a histogram to get a visual of this data. Additionally, we can look at a picture of rentals over time to see if there is some trends or outliers present within our dataset.\n\n\n    rentals    \n Min.   :  22  \n 1st Qu.:3152  \n Median :4548  \n Mean   :4504  \n 3rd Qu.:5956  \n Max.   :8714  \n\n\nThe lowest recorded number is 22 rentals, and the highest recorded number is 8,714 rentals. Across the data the mean is about 4500 rentals and the median is only a little higher, meaning the model shouldn’t have a big skew and is fairly symmetric.\n\n\n\n\n\nWe can see that we don’t have a huge number of outliers and the distribution is not highly skewed in either direction. However, one thing to note is that it is a tri-model looking distribution. There are peaks in the data which suggest that there might be three different normal distributions over-lapping with one another."
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-5",
    "href": "posts/Problem Set 2/index.html#step-5",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 5",
    "text": "Step 5\nMany of the supervised learning algorithms can be helped or hurt by the relationships between features that will be used as predictors. We need to understand the distributions of each variable, looking for skew, outliers, and any other weirdness. This could involve histograms or boxplots of the variables. We can use scatter plots to look at relationships between predictors. For easier comparison we can also use correlation matrices to show statistically linear relationships.\n\n\n\n\n\nFirst off we can see that temperature and realfeel has a strong and linear relationship. The correlation is 0.96. This could mean that one variable is a function of and associated with the other. Indeed, realfeel is a relationship between temperature and humidity and wind that is mean to incorporate what temperature it feels like to a human. In such a case, we will want to leave out a variable. Either realfeel or the other features that go into it.\nThe distribution plots do not look particularly alarming. And the scatterplots don’t show any other overwhelmingly strong relationships. What we can see, is that there is a positive and nonlinear relationship between temperature and rentals as well as temperature and realfeel. Warmer temperatures are associated with more rentals, but eventually, warm temperatures that result in weather that is too hot for comfort will lead to a decrease in rentals.\nWe can also check these correlations with corrplot.\n\n\n\n\n\nWe’re going to Z-score normalize the temperature feature. Our reason is mostly arbitrary, but one benefit is that after the transformation, the mean will be zero. Positive numbers will represent above average temperatures and negative below average ones.\n\n\n  temperature      \n Min.   :-2.38324  \n 1st Qu.:-0.86479  \n Median : 0.01611  \n Mean   : 0.00000  \n 3rd Qu.: 0.87425  \n Max.   : 2.00098  \n\n\nWe can min-max normalize the wind variable. This will take all values of the feature and cram it into the interval \\([0, 1]\\). It essentially puts a feature into a percent range.\nA very important step, and a very common one required by many learning algorithms, is converting all categorical variables into dummy variables. This can be done many different ways in R. The dummy package does make it easier, however.\nBefore running the dummy() function we had 10 variables in the dataset. The result of the function is a new dataset with only the dummy variables generated from the factor variables in bikes. At this point we can replace the factor variables with the dummy ones."
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-6",
    "href": "posts/Problem Set 2/index.html#step-6",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 6",
    "text": "Step 6\nWe’re going to perform a penalized form of regression known as LASSO to find a decent predictive model. We’ll need to do a few things first. We need to get rid variables we don’t intend to have as predictors. The date and realfeel features will be removed.\nNormally, for a linear regression, you’d need to remove one dummy variable from a categorical variable. For example, season has 4 values (Winter, Spring, Fall, and Summer). We have dummy variable for each, but we need to omit one in order for it to work. But with LASSO, its okay and actually better to include them all and let the algorithm decide which to eliminate.\n\n\n\n\n\n\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                         seg84\nintercept          7129.477106\ntemperature         973.652665\nhumidity          -2908.777178\nwindspeed         -1819.172215\nseason_Winter      -718.644052\nseason_Spring       -70.054566\nseason_Summer         8.026927\nseason_Fall         297.688661\nholiday_Yes         404.154109\nholiday_No            .       \nweekday_Sunday     -237.481119\nweekday_Monday      -78.567823\nweekday_Tuesday       .       \nweekday_Wednesday     .       \nweekday_Thursday      .       \nweekday_Friday        .       \nweekday_Saturday     46.330329\nweather_1           254.947013\nweather_2             .       \nweather_3         -1627.856643\ntemperature2       -516.515981"
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-7",
    "href": "posts/Problem Set 2/index.html#step-7",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 7",
    "text": "Step 7\nNow we are going to create a visual representation that will compare our predicted rentals to our actual rentals over time.\n\n\n\n\n\nWe can see that the relationship of this distribution resulted in a weak positive and linear relationship. When interpreting our data, we can say that that our data is not to biased for there seems to be a pretty even variance from the regression line in the points in the model."
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-8",
    "href": "posts/Problem Set 2/index.html#step-8",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 8",
    "text": "Step 8\nThe features present in our dataset all have a sense of importance when developing predictions. Each feature adds a layer of specification and helps us develop a more reliable model. As we attempt to balance out bias and variance, making sure we are covering all aspects to help us make predictions is a must. As features are shifted to fit the appropriate data types they represent, being able to specify information into our model is then open to use. We cn then incorporate appropriate useage of out data and get a more in-depth predition as our model continues to learn more."
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-9",
    "href": "posts/Problem Set 2/index.html#step-9",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 9",
    "text": "Step 9\nWhen it comes to training a model to the data I had prepared, it connects back to all the changes and developments we had made to the feautures and data types in our original dataset. We added new data types, excluded variables, filled in missing values, created dummy variables, and even simplified much of the data we had changed into factors As new things were discovered and parameters were adjusted, the model learned new things. The model was capable of approaching a proper prediction as it shifted and adapted. This training we did to our model led it away from its bias and helped balance out the model in both bias and variance, thus creating a more dependable, yet not perfect, model for prediction."
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-10",
    "href": "posts/Problem Set 2/index.html#step-10",
    "title": "Data Mining: Problem Set 2",
    "section": "Step 10",
    "text": "Step 10\nI feel as if creating dummy variables out of the features we had changed into factors were necessary for this prediction model. These numbers needed to represent the nominal data that they were intentionally marked down for and not used the same way as all of the numeric values present in the dataset. Specifiying these features that were changed into facors were also a big step in gettting more in-depth with our dataset as well. Filling in missing values for realfeel was a step that was I thought was not strictly required for our dataset for we decided that realfeel was a feauture to not use as a predictor. Though I do think it is important to include this step when dealing with other possible datasets. We imputed the median of realfeel into the spots of each missing value. If we would have thrown these 27 observations out, we could have possibly thrown out some important information, thus imputing these missing values allowed us to maintain this data and allowed us to take another step towards specification. Each model we had created regarding relationships, distributions, and even just overall summaries of our features within our datasets all helped us determine which data to include or to not include. Each step had an importance as we learn how to properly train and test our data."
  },
  {
    "objectID": "posts/Demo Post 1/index.html",
    "href": "posts/Demo Post 1/index.html",
    "title": "Demo Post 1",
    "section": "",
    "text": "This is a demo post in which we begin the blog. The idea here is that you create one post with this quarto document. The quarto document for a post will be named “index.qmd” insides of folder with the name of the post. For example, if I wanted my post to be titled “Demo Post 1” then I would do the following.\nAfter doing that, you can then edit the index.qmd document for that new post to your heart’s content. Lets do a little of that now so you can see how this might work."
  },
  {
    "objectID": "posts/Demo Post 1/index.html#including-resources",
    "href": "posts/Demo Post 1/index.html#including-resources",
    "title": "Demo Post 1",
    "section": "Including Resources",
    "text": "Including Resources\nSuppose you wanted to discuss something, like the CRISP-DM process for analytics projects. You might wish to refer to an image of the process and you could include the image in the “Demo Post 1” folder and reference it here in the document.\n\n\n\n\n\nYou can easily insert the image through the visual editor in Posit / RStudio."
  },
  {
    "objectID": "posts/Demo Post 1/index.html#data-and-output",
    "href": "posts/Demo Post 1/index.html#data-and-output",
    "title": "Demo Post 1",
    "section": "Data and Output",
    "text": "Data and Output\nLets look at some data.\n\nlibrary(tidyverse)\nlibrary(ggthemes)\ndata(\"USArrests\")\n\nUSArrests %&gt;%\n  ggplot(aes(x = Assault, y = Murder)) +\n  geom_point(pch = 21, color = \"coral3\", bg = \"coral\", size=3) +\n  labs(title = \"Arrests for Murder vs. Assault in US States\",\n       x = \"Arrests for assault per 100,000\",\n       y = \"Arrests for murder per 100,000\") +\n  theme_clean()\n\n\n\n\nThis would show us a relationship that we could then spend some paragraphs analyzing and interpreting."
  },
  {
    "objectID": "posts/Demo Post 2/index.html",
    "href": "posts/Demo Post 2/index.html",
    "title": "Demo Post 2",
    "section": "",
    "text": "We are looking at arrests data by state. The data set has 50 rows (one for each state) and four variables.\n\nglimpse(USArrests)\n\nRows: 50\nColumns: 4\n$ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\n\n\nEach of the variables are a numeric-continuous data type. We have arrests per 100,000 people for three violent crimes: assault, murder, and rape. We also have a column indicating the degree of urban population in that state. Before preceding with prediction, we note that tree-based techniques can be more unstable if the variables are too correlated with one another. We can also see if there are any extreme skews in the data.\n\nlibrary(GGally)\nggpairs(USArrests)\n\n\n\n\nWe do see some positive relationships and stronger correlations, but mayne not quite enough to get us in trouble.\nNow lets try and predict Murder using the other features.\n\ndt = rpart(Murder ~.,\n           data=USArrests)\nrpart.plot(dt)\n\n\n\n\nWe can calculate a kind of R-squared measure of accuracy by squaring the correlation between the actual Murder values with our predicted ones.\n\nUSArrests %&gt;%\n  mutate(predicted_murder = predict(dt, USArrests)) %&gt;%\n  select(Murder, predicted_murder) %&gt;%\n  cor() -&gt; corrmat\n\nrsq = corrmat[[\"Murder\", \"predicted_murder\"]]^2\nprint(paste(\"The r-square for our model is\", round(rsq,2), sep=\": \"))\n\n[1] \"The r-square for our model is: 0.78\""
  },
  {
    "objectID": "posts/Demo Post 2/index.html#understanding-the-data",
    "href": "posts/Demo Post 2/index.html#understanding-the-data",
    "title": "Demo Post 2",
    "section": "",
    "text": "We are looking at arrests data by state. The data set has 50 rows (one for each state) and four variables.\n\nglimpse(USArrests)\n\nRows: 50\nColumns: 4\n$ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\n\n\nEach of the variables are a numeric-continuous data type. We have arrests per 100,000 people for three violent crimes: assault, murder, and rape. We also have a column indicating the degree of urban population in that state. Before preceding with prediction, we note that tree-based techniques can be more unstable if the variables are too correlated with one another. We can also see if there are any extreme skews in the data.\n\nlibrary(GGally)\nggpairs(USArrests)\n\n\n\n\nWe do see some positive relationships and stronger correlations, but mayne not quite enough to get us in trouble.\nNow lets try and predict Murder using the other features.\n\ndt = rpart(Murder ~.,\n           data=USArrests)\nrpart.plot(dt)\n\n\n\n\nWe can calculate a kind of R-squared measure of accuracy by squaring the correlation between the actual Murder values with our predicted ones.\n\nUSArrests %&gt;%\n  mutate(predicted_murder = predict(dt, USArrests)) %&gt;%\n  select(Murder, predicted_murder) %&gt;%\n  cor() -&gt; corrmat\n\nrsq = corrmat[[\"Murder\", \"predicted_murder\"]]^2\nprint(paste(\"The r-square for our model is\", round(rsq,2), sep=\": \"))\n\n[1] \"The r-square for our model is: 0.78\""
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects are different than posts. Projects should be more expansive, impressive and generally more professional in nature compared to posts. Posts can be works in progress. Small ideas or things you did that you thought were interesting. Projects should really showcase your professional abilities. You don’t need to have too many, just make them good. And try to always have one “in the works” so that employers and collaborators can see that you’re driven.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Problem Set #8\n\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\nAnnika G. Lee\n\n\n\n\n\n\n  \n\n\n\n\nProblem Set #6\n\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nAnnika G. Lee\n\n\n\n\n\n\n  \n\n\n\n\nProblem Set #7\n\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\nAnnika G. Lee\n\n\n\n\n\n\n  \n\n\n\n\nProblem Set 4\n\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2023\n\n\nAnnika G. Lee\n\n\n\n\n\n\n  \n\n\n\n\nProblem Set 3\n\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nAnnika G. Lee\n\n\n\n\n\n\n  \n\n\n\n\nData Mining: Problem Set 2\n\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2023\n\n\nAnnika G. Lee\n\n\n\n\n\n\n  \n\n\n\n\nDemo Post 2\n\n\n\n\n\n\n\ndecision trees\n\n\nmachine learning\n\n\narrests\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nJane Doe\n\n\n\n\n\n\n  \n\n\n\n\nDemo Post 1\n\n\n\n\n\n\n\nquarto\n\n\ncrisp-dm\n\n\nscatterplot\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nJane Doe\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Problem Set 3/index.html",
    "href": "posts/Problem Set 3/index.html",
    "title": "Problem Set 3",
    "section": "",
    "text": "rm(list = ls())\nlibrary(tidyverse)\nlibrary(glmnet)\nlibrary(lubridate)\nlibrary(caret)\nlibrary(dummy)\nlibrary(gamlr)\nlibrary(rmarkdown)\nlibrary(GGally)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(corrplot)"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#section",
    "href": "posts/Problem Set 3/index.html#section",
    "title": "Problem Set 3",
    "section": "",
    "text": "rm(list = ls())\nlibrary(tidyverse)\nlibrary(glmnet)\nlibrary(lubridate)\nlibrary(caret)\nlibrary(dummy)\nlibrary(gamlr)\nlibrary(rmarkdown)\nlibrary(GGally)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(corrplot)"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-2",
    "href": "posts/Problem Set 3/index.html#part-2",
    "title": "Problem Set 3",
    "section": "Part 2",
    "text": "Part 2\nWithin this dataset there are a total of 1,436 observations and 39 columns of different features. 36 our of the 39 features are numeric date types, whereas 3 are categorical data types.\n\ncars = read_csv(\"ToyotaCorolla.csv\")\n\nRows: 1436 Columns: 39\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Model, Fuel_Type, Color\ndbl (36): Id, Price, Age_08_04, Mfg_Month, Mfg_Year, KM, HP, Met_Color, Auto...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(cars)\n\nRows: 1,436\nColumns: 39\n$ Id                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ Model             &lt;chr&gt; \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\", \"TO…\n$ Price             &lt;dbl&gt; 13500, 13750, 13950, 14950, 13750, 12950, 16900, 186…\n$ Age_08_04         &lt;dbl&gt; 23, 23, 24, 26, 30, 32, 27, 30, 27, 23, 25, 22, 25, …\n$ Mfg_Month         &lt;dbl&gt; 10, 10, 9, 7, 3, 1, 6, 3, 6, 10, 8, 11, 8, 2, 1, 5, …\n$ Mfg_Year          &lt;dbl&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002…\n$ KM                &lt;dbl&gt; 46986, 72937, 41711, 48000, 38500, 61000, 94612, 758…\n$ Fuel_Type         &lt;chr&gt; \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"D…\n$ HP                &lt;dbl&gt; 90, 90, 90, 90, 90, 90, 90, 90, 192, 69, 192, 192, 1…\n$ Met_Color         &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1…\n$ Color             &lt;chr&gt; \"Blue\", \"Silver\", \"Blue\", \"Black\", \"Black\", \"White\",…\n$ Automatic         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ CC                &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 1800…\n$ Doors             &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ Cylinders         &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4…\n$ Gears             &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 5, 5…\n$ Quarterly_Tax     &lt;dbl&gt; 210, 210, 210, 210, 210, 210, 210, 210, 100, 185, 10…\n$ Weight            &lt;dbl&gt; 1165, 1165, 1165, 1165, 1170, 1170, 1245, 1245, 1185…\n$ Mfr_Guarantee     &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0…\n$ BOVAG_Guarantee   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0…\n$ Guarantee_Period  &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 12, 3, 3, 3, 3, 3, 3, …\n$ ABS               &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Airbag_1          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Airbag_2          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0…\n$ Airco             &lt;dbl&gt; 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Automatic_airco   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0…\n$ Boardcomputer     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0…\n$ CD_Player         &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0…\n$ Central_Lock      &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Powered_Windows   &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Power_Steering    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Radio             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ Mistlamps         &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0…\n$ Sport_Model       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0…\n$ Backseat_Divider  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0…\n$ Metallic_Rim      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0…\n$ Radio_cassette    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ Parking_Assistant &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Tow_Bar           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n\n\nWhen looking at our data using glimpse(), we can assume that the features with little importance are Id, Model, Mfg_Month, and Cylinders. These features do not have much of an affect on generating predictions of prices for used Toyota Corollas within this dataset. Therefore removing these features will allow us a more simplified dataset to use to develop predictions. I have also renamed the Age_08_04 feature into Age.\n\ncars = cars %&gt;%\n  select(-Id, -Model, -Mfg_Month, -Cylinders) %&gt;%\n  rename(Age = Age_08_04)\n\nSome of our features are better represented as nominal data types. As we change those features into categorical and nominal data, we will change them into factor data. With now numeric and factor data types present, combining all the data back into the the cars dataset will allow us to look for missing values and help us determine where to impute the feature’s median into spots for missing values.\n\ncars_fct = cars %&gt;%\n  select(-Price, -Age, -KM, -HP, -CC, -Weight, -Quarterly_Tax) %&gt;%\n  mutate_all(.funs = factor)\n\ncars_num = cars %&gt;%\n  select(Price, Age, KM, HP, CC, Weight, Quarterly_Tax)\n\ncars = bind_cols(cars_num, cars_fct)\n\nOur dataset shows that each feature has no missing values. This mean we do not need to impute any values into spots of missing values.\n\nsummary(cars)\n\n     Price            Age              KM               HP       \n Min.   : 4350   Min.   : 1.00   Min.   :     1   Min.   : 69.0  \n 1st Qu.: 8450   1st Qu.:44.00   1st Qu.: 43000   1st Qu.: 90.0  \n Median : 9900   Median :61.00   Median : 63390   Median :110.0  \n Mean   :10731   Mean   :55.95   Mean   : 68533   Mean   :101.5  \n 3rd Qu.:11950   3rd Qu.:70.00   3rd Qu.: 87021   3rd Qu.:110.0  \n Max.   :32500   Max.   :80.00   Max.   :243000   Max.   :192.0  \n                                                                 \n       CC            Weight     Quarterly_Tax    Mfg_Year    Fuel_Type   \n Min.   : 1300   Min.   :1000   Min.   : 19.00   1998:392   CNG   :  17  \n 1st Qu.: 1400   1st Qu.:1040   1st Qu.: 69.00   1999:441   Diesel: 155  \n Median : 1600   Median :1070   Median : 85.00   2000:225   Petrol:1264  \n Mean   : 1577   Mean   :1072   Mean   : 87.12   2001:192                \n 3rd Qu.: 1600   3rd Qu.:1085   3rd Qu.: 85.00   2002: 87                \n Max.   :16000   Max.   :1615   Max.   :283.00   2003: 75                \n                                                 2004: 24                \n Met_Color     Color     Automatic Doors   Gears    Mfr_Guarantee\n 0:467     Grey   :301   0:1356    2:  2   3:   2   0:848        \n 1:969     Blue   :283   1:  80    3:622   4:   1   1:588        \n           Red    :278             4:138   5:1390                \n           Green  :220             5:674   6:  43                \n           Black  :191                                           \n           Silver :122                                           \n           (Other): 41                                           \n BOVAG_Guarantee Guarantee_Period ABS      Airbag_1 Airbag_2 Airco  \n 0: 150          3      :1274     0: 268   0:  42   0: 398   0:706  \n 1:1286          6      :  77     1:1168   1:1394   1:1038   1:730  \n                 12     :  73                                       \n                 24     :   4                                       \n                 36     :   4                                       \n                 13     :   1                                       \n                 (Other):   3                                       \n Automatic_airco Boardcomputer CD_Player Central_Lock Powered_Windows\n 0:1355          0:1013        0:1122    0:603        0:629          \n 1:  81          1: 423        1: 314    1:833        1:807          \n                                                                     \n                                                                     \n                                                                     \n                                                                     \n                                                                     \n Power_Steering Radio    Mistlamps Sport_Model Backseat_Divider Metallic_Rim\n 0:  32         0:1226   0:1067    0:1005      0: 330           0:1142      \n 1:1404         1: 210   1: 369    1: 431      1:1106           1: 294      \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n Radio_cassette Parking_Assistant Tow_Bar \n 0:1227         0:1432            0:1037  \n 1: 209         1:   4            1: 399"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-3",
    "href": "posts/Problem Set 3/index.html#part-3",
    "title": "Problem Set 3",
    "section": "Part 3",
    "text": "Part 3\nAfter looking at the variable Price, we can look at its distribution and can determine that Price is appropriate for a linear regression model for it does not have any missing values and does not have low variability within its data. The distribution is right-skewed and has a small amount of outliers. When looking at our Linear Regression model that has been developed below, the Min and Max of the data set are not too different from one another and the same goes for 1Q and 3Q of the dataset as well. These are good signs of normal distribution that we want. The only slight concern is that the model’s Median is quite a far distance from 0.\n\nlm_Price = train(Price ~ .,\n            data = cars,\n            method = \"lm\")\n\nlm_Price\n\nLinear Regression \n\n1436 samples\n  34 predictor\n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 1436, 1436, 1436, 1436, 1436, 1436, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  1721.915  0.7840195  881.3067\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\nsummary(lm_Price)\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5225.2  -620.6   -41.6   575.3  6192.5 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        -2.234e+03  1.796e+03  -1.244 0.213732    \nAge                -2.902e+01  8.801e+00  -3.297 0.001003 ** \nKM                 -1.589e-02  1.085e-03 -14.642  &lt; 2e-16 ***\nHP                  2.230e+01  3.116e+00   7.158 1.33e-12 ***\nCC                 -6.027e-02  7.374e-02  -0.817 0.413890    \nWeight              7.533e+00  1.166e+00   6.460 1.45e-10 ***\nQuarterly_Tax       1.122e+01  1.655e+00   6.779 1.78e-11 ***\nMfg_Year1999        7.023e+02  1.320e+02   5.322 1.20e-07 ***\nMfg_Year2000        1.541e+03  2.315e+02   6.659 3.98e-11 ***\nMfg_Year2001        2.313e+03  3.284e+02   7.044 2.95e-12 ***\nMfg_Year2002        4.489e+03  4.482e+02  10.018  &lt; 2e-16 ***\nMfg_Year2003        6.025e+03  5.338e+02  11.286  &lt; 2e-16 ***\nMfg_Year2004        7.967e+03  6.491e+02  12.274  &lt; 2e-16 ***\nFuel_TypeDiesel     1.002e+03  3.098e+02   3.233 0.001255 ** \nFuel_TypePetrol     1.502e+03  3.242e+02   4.635 3.91e-06 ***\nMet_Color1         -6.757e+01  6.820e+01  -0.991 0.322000    \nColorBlack          6.014e+02  6.166e+02   0.975 0.329563    \nColorBlue           4.780e+02  6.158e+02   0.776 0.437672    \nColorGreen          3.340e+02  6.169e+02   0.541 0.588304    \nColorGrey           6.352e+02  6.160e+02   1.031 0.302672    \nColorRed            4.733e+02  6.159e+02   0.768 0.442344    \nColorSilver         5.582e+02  6.211e+02   0.899 0.368984    \nColorViolet         2.224e+02  8.171e+02   0.272 0.785563    \nColorWhite         -2.128e+02  6.450e+02  -0.330 0.741514    \nColorYellow         5.609e+02  8.670e+02   0.647 0.517744    \nAutomatic1          4.620e+02  1.325e+02   3.488 0.000503 ***\nDoors3             -4.380e+02  7.626e+02  -0.574 0.565842    \nDoors4             -4.341e+02  7.679e+02  -0.565 0.571948    \nDoors5             -2.798e+02  7.641e+02  -0.366 0.714255    \nGears4             -1.128e+02  1.328e+03  -0.085 0.932337    \nGears5              5.834e+02  7.879e+02   0.741 0.459109    \nGears6              1.011e+03  8.101e+02   1.247 0.212449    \nMfr_Guarantee1      2.968e+02  6.310e+01   4.704 2.81e-06 ***\nBOVAG_Guarantee1    4.583e+02  1.102e+02   4.158 3.40e-05 ***\nGuarantee_Period6   5.197e+02  1.669e+02   3.114 0.001884 ** \nGuarantee_Period12  7.091e+02  1.671e+02   4.244 2.34e-05 ***\nGuarantee_Period13  3.974e+03  1.097e+03   3.622 0.000303 ***\nGuarantee_Period18  2.862e+03  1.078e+03   2.655 0.008023 ** \nGuarantee_Period20  1.387e+03  1.079e+03   1.285 0.198945    \nGuarantee_Period24  4.352e+02  5.627e+02   0.773 0.439421    \nGuarantee_Period28  1.668e+03  1.080e+03   1.545 0.122632    \nGuarantee_Period36  1.138e+02  5.743e+02   0.198 0.842910    \nABS1               -4.491e+01  1.126e+02  -0.399 0.689972    \nAirbag_11           1.535e+02  2.156e+02   0.712 0.476656    \nAirbag_21          -1.855e+00  1.189e+02  -0.016 0.987554    \nAirco1              2.356e+02  7.626e+01   3.089 0.002047 ** \nAutomatic_airco1    1.918e+03  1.683e+02  11.395  &lt; 2e-16 ***\nBoardcomputer1     -1.630e+02  1.092e+02  -1.493 0.135603    \nCD_Player1          2.170e+02  8.486e+01   2.557 0.010676 *  \nCentral_Lock1      -9.588e+01  1.210e+02  -0.792 0.428242    \nPowered_Windows1    3.032e+02  1.214e+02   2.497 0.012649 *  \nPower_Steering1    -1.597e+02  2.421e+02  -0.660 0.509435    \nRadio1              5.816e+02  6.244e+02   0.931 0.351791    \nMistlamps1          2.806e+01  9.299e+01   0.302 0.762902    \nSport_Model1       -3.963e+01  8.145e+01  -0.487 0.626634    \nBackseat_Divider1  -4.920e+01  1.308e+02  -0.376 0.706793    \nMetallic_Rim1       1.146e+02  8.199e+01   1.398 0.162472    \nRadio_cassette1    -6.200e+02  6.239e+02  -0.994 0.320488    \nParking_Assistant1 -4.225e+02  5.400e+02  -0.782 0.434109    \nTow_Bar1           -1.724e+02  6.709e+01  -2.570 0.010263 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1049 on 1376 degrees of freedom\nMultiple R-squared:  0.9197,    Adjusted R-squared:  0.9163 \nF-statistic: 267.2 on 59 and 1376 DF,  p-value: &lt; 2.2e-16\n\n\nWhen creating a Histogram, we can take a look at the Price feature.\n\ncars %&gt;%\n  ggplot(aes(Price)) +\n  geom_histogram(color = \"black\", bg = \"skyblue\") +\n  labs(title = \"Distribution of Selling Prices\",\n       x = \"Selling price\",\n       y = \"Count of cars\") +\n  theme_classic()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-4",
    "href": "posts/Problem Set 3/index.html#part-4",
    "title": "Problem Set 3",
    "section": "Part 4",
    "text": "Part 4\nWe can see that Age and KM have quite a strong negative relationship with Price.\n\ncaret::featurePlot(keep(cars, is.numeric), cars$Price, plot = \"scatter\")"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-5",
    "href": "posts/Problem Set 3/index.html#part-5",
    "title": "Problem Set 3",
    "section": "Part 5",
    "text": "Part 5\n\ncars %&gt;%\n  keep(is.numeric) %&gt;%\n  ggpairs()"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-6",
    "href": "posts/Problem Set 3/index.html#part-6",
    "title": "Problem Set 3",
    "section": "Part 6",
    "text": "Part 6\nWe can convert our categorical variables into dummy variables.\n\ncars_dum = dummy(cars, int = TRUE)\ncars_num = cars %&gt;%\n  keep(is.numeric)\ncars = bind_cols(cars_num, cars_dum)\nrm(cars_dum, cars_num)\n\nNow we will partition our data.\n\n#DATA PARTITION \nset.seed(4532)\nsamp = createDataPartition(cars$Price, p=0.7, list = FALSE)\ntraining = cars[samp, ]\ntesting = cars[-samp, ]\nrm(samp)"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-7",
    "href": "posts/Problem Set 3/index.html#part-7",
    "title": "Problem Set 3",
    "section": "Part 7",
    "text": "Part 7\nPre-pruning sets limitations and boundaries to the tree and limits the overall complexity of it. Whereas post-pruning allows the tree to continue growing nwith justifications being made to it. As we justify which data to use, we get to tune and retrain the tree.\n\ntrain_model = train(Price ~ .,\n                   data = training,\n                   method = \"rpart\",\n                   trControl = trainControl(method = \"cv\", number = 10),\n                   tuneGrid = expand.grid(cp = seq(0.0, 0.01, 0.0001)),\n                   control = rpart.control(minbucket = 1)\n                   )\nplot(train_model)\n\n\n\n\n\nrpart.plot(train_model$finalModel)"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-8",
    "href": "posts/Problem Set 3/index.html#part-8",
    "title": "Problem Set 3",
    "section": "Part 8",
    "text": "Part 8\nWhen looking at our feature importance, we can determine that Age, KM, and Weight are the only three variables that have a difference in importance when compared to all other variables. We could possibly remove all other variables in the dataset.\n\nlibrary(iml)\nlibrary(patchwork)\n\ntree_predictor = iml::Predictor$new(train_model,\n                                    data = testing,\n                                    y = testing$Price)\n\ntree_imp = iml::FeatureImp$new(tree_predictor, loss = \"rmse\", compare = \"ratio\")\nplot(tree_imp)\n\n\n\n\nWe can see that Age has the highest importance when compared to other feautures in the dataset.\n\ntree_imp$results %&gt;%\n  filter(importance &gt; 1)\n\n            feature importance.05 importance importance.95 permutation.error\n1               Age     3.1465977   3.253068      3.369671          4212.437\n2            Weight     1.1531487   1.171143      1.203610          1516.527\n3                KM     1.0826273   1.105717      1.116607          1431.806\n4    Metallic_Rim_0     1.0317436   1.079041      1.117105          1397.264\n5 Powered_Windows_0     1.0394016   1.045327      1.057137          1353.607\n6 Automatic_airco_0     1.0110262   1.030380      1.034653          1334.251\n7     Quarterly_Tax     1.0185878   1.023070      1.037765          1324.786\n8 Powered_Windows_1     0.9828574   1.009893      1.029864          1307.722\n9                HP     0.9926295   1.008777      1.014996          1306.278"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-9",
    "href": "posts/Problem Set 3/index.html#part-9",
    "title": "Problem Set 3",
    "section": "Part 9",
    "text": "Part 9\n\ntrain_new = dplyr::select(training, Age, Weight, KM, Metallic_Rim_0, Powered_Windows_0, Quarterly_Tax, Automatic_airco_0, HP, Powered_Windows_0, Price)\n\nnew_tree = caret::train(Price ~ .,\n                        data = train_new,\n                        method = \"rpart\",\n                        trControl = trainControl(method = \"cv\", number = 10),\n                        tuneGrid = expand.grid(cp = seq(0.0, 0.01, 0.0001)),\n                        control = rpart.control(minbucket = 1)\n                        )\nplot(new_tree)\n\n\n\n\n\nrpart.plot(new_tree$finalModel)"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#part-10",
    "href": "posts/Problem Set 3/index.html#part-10",
    "title": "Problem Set 3",
    "section": "Part 10",
    "text": "Part 10\n\ntrain_error = postResample(predict(new_tree, training), training$Price)[[\"RMSE\"]]\n\ncv_error = min(new_tree$results$RMSE)\n\ntest_error = postResample(predict(new_tree, testing), testing$Price)[[\"RMSE\"]]\n\ndata.frame(\n  \"Error Source\" = c(\"Training\", \"Cross-Validation\", \"Testing\"),\n  \"RMSE\" = c(train_error, cv_error, test_error)\n)\n\n      Error.Source      RMSE\n1         Training  929.0272\n2 Cross-Validation 1164.5327\n3          Testing 1264.3797\n\n\nFrom our results, we can see that the model with the highest RMSE is for our Testing data. This makes sense for our Testing data is not being trained and fitted as well like our Training data. Both the Cross-Validation and Testing RMSE values are quite close to one another."
  },
  {
    "objectID": "posts/Problem Set 4/index.html",
    "href": "posts/Problem Set 4/index.html",
    "title": "Problem Set 4",
    "section": "",
    "text": "rm(list = ls())\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(glmnet)\nlibrary(rpart)\nlibrary(pROC)\nlibrary(ggthemes)\nlibrary(AppliedPredictiveModeling)\nlibrary(performanceEstimation)"
  },
  {
    "objectID": "posts/Problem Set 4/index.html#part-1",
    "href": "posts/Problem Set 4/index.html#part-1",
    "title": "Problem Set 4",
    "section": "Part 1",
    "text": "Part 1\nClassification is the right approach for NVO’s problem. The reason for this is because we are trying to predict discrete values using this dataset. These values are whether or not a person will respond to a mailing. We are not focusing on the predicted numbers and their root mean squared errors like we do with regression for we are looking at the accuracy of the dataset. Our goal is to target and focus on individuals that will likely respond and increase NVO’s response rate.\n\ndonors = read_csv(\"donors.csv\")\n\nRows: 95412 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): state, urbanicity, socioEconomicStatus, gender\ndbl (12): age, numberChildren, incomeRating, wealthRating, mailOrderPurchase...\nlgl  (6): inHouseDonor, plannedGivingDonor, sweepstakesDonor, P3Donor, isHom...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(donors)\n\nRows: 95,412\nColumns: 22\n$ age                     &lt;dbl&gt; 60, 46, NA, 70, 78, NA, 38, NA, NA, 65, NA, 75…\n$ numberChildren          &lt;dbl&gt; NA, 1, NA, NA, 1, NA, 1, NA, NA, NA, NA, NA, 2…\n$ incomeRating            &lt;dbl&gt; NA, 6, 3, 1, 3, NA, 4, 2, 3, NA, 2, 1, 4, NA, …\n$ wealthRating            &lt;dbl&gt; NA, 9, 1, 4, 2, NA, 6, 9, 2, NA, 0, 5, 2, NA, …\n$ mailOrderPurchases      &lt;dbl&gt; 0, 16, 2, 2, 60, 0, 0, 1, 0, 0, 0, 3, 16, 0, 1…\n$ totalGivingAmount       &lt;dbl&gt; 240, 47, 202, 109, 254, 51, 107, 31, 199, 28, …\n$ numberGifts             &lt;dbl&gt; 31, 3, 27, 16, 37, 4, 14, 5, 11, 3, 1, 2, 9, 1…\n$ smallestGiftAmount      &lt;dbl&gt; 5, 10, 2, 2, 3, 10, 3, 5, 10, 3, 20, 10, 4, 5,…\n$ largestGiftAmount       &lt;dbl&gt; 12, 25, 16, 11, 15, 16, 12, 11, 22, 15, 20, 15…\n$ averageGiftAmount       &lt;dbl&gt; 7.741935, 15.666667, 7.481481, 6.812500, 6.864…\n$ yearsSinceFirstDonation &lt;dbl&gt; 8, 3, 7, 10, 11, 3, 10, 3, 9, 3, 1, 1, 8, 5, 4…\n$ monthsSinceLastDonation &lt;dbl&gt; 14, 14, 14, 14, 13, 20, 22, 18, 19, 22, 12, 14…\n$ inHouseDonor            &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE…\n$ plannedGivingDonor      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ sweepstakesDonor        &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ P3Donor                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE…\n$ state                   &lt;chr&gt; \"IL\", \"CA\", \"NC\", \"CA\", \"FL\", \"AL\", \"IN\", \"LA\"…\n$ urbanicity              &lt;chr&gt; \"town\", \"suburb\", \"rural\", \"rural\", \"suburb\", …\n$ socioEconomicStatus     &lt;chr&gt; \"average\", \"highest\", \"average\", \"average\", \"a…\n$ isHomeowner             &lt;lgl&gt; NA, TRUE, NA, NA, TRUE, NA, TRUE, NA, NA, NA, …\n$ gender                  &lt;chr&gt; \"female\", \"male\", \"male\", \"female\", \"female\", …\n$ respondedMailing        &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…"
  },
  {
    "objectID": "posts/Problem Set 4/index.html#part-2",
    "href": "posts/Problem Set 4/index.html#part-2",
    "title": "Problem Set 4",
    "section": "Part 2",
    "text": "Part 2\nThis classifier being built to identify potential donors for NVO will be better to use due to the accuracy it could develop. The reason for this is due to us having the ability to exclude features that may not have much significance to help make predictions in the dataset. As we select certain features to use and to remove, we can train and split the data up further to aim for the accuracy we hope to achieve. We get to focus on the separate classes within the data and not just all of the data within the dataset as a whole.\n\n# Preparing Data\ndonors = donors %&gt;%\n  rename(Responded = `respondedMailing`) %&gt;%\n  mutate(Responded = factor(ifelse(Responded == TRUE, \"1\", \"0\")))\n\n\nsummary(donors)\n\n      age        numberChildren   incomeRating    wealthRating  \n Min.   : 1.00   Min.   :1.00    Min.   :1.000   Min.   :0.00   \n 1st Qu.:48.00   1st Qu.:1.00    1st Qu.:2.000   1st Qu.:3.00   \n Median :62.00   Median :1.00    Median :4.000   Median :6.00   \n Mean   :61.61   Mean   :1.53    Mean   :3.886   Mean   :5.35   \n 3rd Qu.:75.00   3rd Qu.:2.00    3rd Qu.:5.000   3rd Qu.:8.00   \n Max.   :98.00   Max.   :7.00    Max.   :7.000   Max.   :9.00   \n NA's   :23665   NA's   :83026   NA's   :21286   NA's   :44732  \n mailOrderPurchases totalGivingAmount  numberGifts      smallestGiftAmount\n Min.   :  0.000    Min.   :  13.0    Min.   :  1.000   Min.   :   0.000  \n 1st Qu.:  0.000    1st Qu.:  40.0    1st Qu.:  3.000   1st Qu.:   3.000  \n Median :  0.000    Median :  78.0    Median :  7.000   Median :   5.000  \n Mean   :  3.321    Mean   : 104.5    Mean   :  9.602   Mean   :   7.934  \n 3rd Qu.:  3.000    3rd Qu.: 131.0    3rd Qu.: 13.000   3rd Qu.:  10.000  \n Max.   :241.000    Max.   :9485.0    Max.   :237.000   Max.   :1000.000  \n                                                                          \n largestGiftAmount averageGiftAmount  yearsSinceFirstDonation\n Min.   :   5      Min.   :   1.286   Min.   : 0.000         \n 1st Qu.:  14      1st Qu.:   8.385   1st Qu.: 2.000         \n Median :  17      Median :  11.636   Median : 5.000         \n Mean   :  20      Mean   :  13.348   Mean   : 5.596         \n 3rd Qu.:  23      3rd Qu.:  15.478   3rd Qu.: 9.000         \n Max.   :5000      Max.   :1000.000   Max.   :13.000         \n                                                             \n monthsSinceLastDonation inHouseDonor    plannedGivingDonor sweepstakesDonor\n Min.   : 0.00           Mode :logical   Mode :logical      Mode :logical   \n 1st Qu.:12.00           FALSE:88709     FALSE:95298        FALSE:93795     \n Median :14.00           TRUE :6703      TRUE :114          TRUE :1617      \n Mean   :14.36                                                              \n 3rd Qu.:17.00                                                              \n Max.   :23.00                                                              \n                                                                            \n  P3Donor           state            urbanicity        socioEconomicStatus\n Mode :logical   Length:95412       Length:95412       Length:95412       \n FALSE:93395     Class :character   Class :character   Class :character   \n TRUE :2017      Mode  :character   Mode  :character   Mode  :character   \n                                                                          \n                                                                          \n                                                                          \n                                                                          \n isHomeowner       gender          Responded\n Mode:logical   Length:95412       0:90569  \n TRUE:52354     Class :character   1: 4843  \n NA's:43058     Mode  :character"
  },
  {
    "objectID": "posts/Problem Set 4/index.html#part-3",
    "href": "posts/Problem Set 4/index.html#part-3",
    "title": "Problem Set 4",
    "section": "Part 3",
    "text": "Part 3\nThe most important measures from the confusion matrix that I will use to evaluate the classifier performance will be Accuracy, Sensitivity (Recall), and Post Pred Value (Precision). Since our goal is to not miss out on individuals who will actually respond, being more accurate is not going to be the only outstanding factor of our model. Recall and Precision will allow us to see the percentages of actual positives and how often these positive predictions are correct. As we relate the importance of these measures to mailer response rate and maximizing donation opportunities, positive predictions are the individuals that are predicted to achieve our goal, respond to our mailings.\nThere seems to be many missing values within the dataset. We can see that there is a class imbalance, for a very small percentage of people had actually responded.\nI decided to not include the variables of states, numberChildren, and wealthRating due to the large amount of missing values and lack of usefulness they have to our predictions. For variables with only a few missing values, I decided to fill in those missing values with the median of their variable for quantitative variables or dropped the missing variables for categorical variables in the dataset.\n\ndonors = donors %&gt;%\n  select(-numberChildren, -wealthRating, -plannedGivingDonor, -state) %&gt;%\n  mutate(isHomeowner = ifelse(is.na(isHomeowner), \"UNKNOWN\", isHomeowner)) %&gt;%\n  mutate(age = ifelse(is.na(age), median(age, na.rm = TRUE), age)) %&gt;%\n  mutate(incomeRating = ifelse(is.na(incomeRating), median(incomeRating, na.rm = TRUE), incomeRating)) %&gt;%\n  drop_na() %&gt;%\n  mutate_if(is.character, .funs=factor) %&gt;%\n  mutate_if(is.logical, .funs=factor)\n\n\nsummary(donors)\n\n      age         incomeRating   mailOrderPurchases totalGivingAmount\n Min.   : 1.00   Min.   :1.000   Min.   :  0.000    Min.   :  13.0   \n 1st Qu.:52.00   1st Qu.:3.000   1st Qu.:  0.000    1st Qu.:  40.0   \n Median :62.00   Median :4.000   Median :  0.000    Median :  77.0   \n Mean   :61.57   Mean   :3.911   Mean   :  3.449    Mean   : 102.3   \n 3rd Qu.:72.00   3rd Qu.:5.000   3rd Qu.:  3.000    3rd Qu.: 130.0   \n Max.   :98.00   Max.   :7.000   Max.   :241.000    Max.   :5674.9   \n  numberGifts      smallestGiftAmount largestGiftAmount averageGiftAmount\n Min.   :  1.000   Min.   :  0.000    Min.   :   5.00   Min.   :  1.286  \n 1st Qu.:  3.000   1st Qu.:  3.000    1st Qu.:  14.00   1st Qu.:  8.400  \n Median :  7.000   Median :  5.000    Median :  17.00   Median : 11.667  \n Mean   :  9.475   Mean   :  7.946    Mean   :  19.75   Mean   : 13.277  \n 3rd Qu.: 13.000   3rd Qu.: 10.000    3rd Qu.:  23.00   3rd Qu.: 15.500  \n Max.   :237.000   Max.   :500.000    Max.   :1000.00   Max.   :506.500  \n yearsSinceFirstDonation monthsSinceLastDonation inHouseDonor  sweepstakesDonor\n Min.   : 0.000          Min.   : 0.0            FALSE:82937   FALSE:87242     \n 1st Qu.: 2.000          1st Qu.:12.0            TRUE : 5656   TRUE : 1351     \n Median : 5.000          Median :14.0                                          \n Mean   : 5.547          Mean   :14.4                                          \n 3rd Qu.: 9.000          3rd Qu.:17.0                                          \n Max.   :13.000          Max.   :23.0                                          \n  P3Donor       urbanicity    socioEconomicStatus  isHomeowner   \n FALSE:86918   city  :18819   average:46367       TRUE   :50357  \n TRUE : 1675   rural :18777   highest:27050       UNKNOWN:38236  \n               suburb:20848   lowest :15176                      \n               town  :18685                                      \n               urban :11464                                      \n                                                                 \n    gender      Responded\n female:50085   0:84115  \n joint :  354   1: 4478  \n male  :38154            \n                         \n                         \n                         \n\n\n\ntransparentTheme(trans = .9)\nfeaturePlot(x = keep(donors, is.numeric),\n            y = donors$Responded,\n            plot = \"box\",\n            scales = list(y = list(relation = \"free\"),\n                          x = list(rot = 90)),\n            layout = c(4, 1),\n            auto.key = list(columns = 2))\n\n\n\n\n\n\n\n\n\n\n\ndonors %&gt;%\n  keep(is.numeric) %&gt;%\n  cor() %&gt;%\n  corrplot::corrplot(., method = \"number\", type = \"lower\", number.cex = 0.6, tl.cex = 0.7)\n\n\n\n\nDue to high correlations being developed with the use of these variables, we could drop them from the dataset as we try to determine which variables would be most efficient in helping us find predictions.\n\ndonors = donors %&gt;%\n  select(-smallestGiftAmount, -largestGiftAmount)\n\n\n# Partition our data\nset.seed(455)\nsamp = caret::createDataPartition(donors$Responded, p = 0.7, list = FALSE)\ntrain = donors[samp,]\ntest = donors[-samp,]\nrm(samp)\n\n\n# Observing Class Imbalance\ntable(train$Responded)\n\n\n    0     1 \n58881  3135 \n\n\n\n# Smote\nset.seed(4959)\nsmote_train = smote(Responded ~ .,\n                    data = train,\n                    perc.over = 8,\n                    perc.under = 2)\n\ntable(smote_train$Responded)\n\n\n    0     1 \n50160 28215"
  },
  {
    "objectID": "posts/Problem Set 4/index.html#part-4",
    "href": "posts/Problem Set 4/index.html#part-4",
    "title": "Problem Set 4",
    "section": "Part 4",
    "text": "Part 4\n\n# LASSO\ntrain.X = model.matrix(Responded ~ 0 + ., smote_train)\ntrain.Y = as.numeric(smote_train$Responded == \"1\")\n\nlasso = cv.glmnet(x = train.X, y = train.Y, type.measure = \"class\",\n                       nfolds = 20, family = 'binomial')\nplot(lasso)\n\n\n\n\n\ncoef(lasso, s = 'lambda.min')\n\n22 x 1 sparse Matrix of class \"dgCMatrix\"\n                                      s1\n(Intercept)                 0.0464804674\nage                         .           \nincomeRating                0.0429794838\nmailOrderPurchases          .           \ntotalGivingAmount           .           \nnumberGifts                 0.0133683503\naverageGiftAmount          -0.0273379096\nyearsSinceFirstDonation     0.0001557296\nmonthsSinceLastDonation    -0.0424595922\ninHouseDonorFALSE           .           \ninHouseDonorTRUE            .           \nsweepstakesDonorTRUE       -0.2936363281\nP3DonorTRUE                 .           \nurbanicityrural             .           \nurbanicitysuburb            0.0304234865\nurbanicitytown              0.0167131580\nurbanicityurban            -0.0699822042\nsocioEconomicStatushighest  0.0895240586\nsocioEconomicStatuslowest  -0.1593449705\nisHomeownerUNKNOWN          .           \ngenderjoint                 .           \ngendermale                  ."
  },
  {
    "objectID": "posts/Problem Set 4/index.html#part-5",
    "href": "posts/Problem Set 4/index.html#part-5",
    "title": "Problem Set 4",
    "section": "Part 5",
    "text": "Part 5\n\n# Smote Tree\nctrl = caret::trainControl(method = \"cv\", number = 10)\nsmote_tree = caret::train(Responded ~ .,\n             data = smote_train,\n             method = \"rpart\",\n             metric = \"Accuracy\",\n             trControl = ctrl,\n             tuneGrid = expand.grid(cp = seq(0.0, 0.1, 0.005)),\n             control = rpart.control(maxdepth = 10, minsplit = 1, minbucket = 30))\n\nplot(smote_tree)\n\n\n\n\n\nrpart.plot::rpart.plot(smote_tree$finalModel)"
  },
  {
    "objectID": "posts/Problem Set 4/index.html#part-6",
    "href": "posts/Problem Set 4/index.html#part-6",
    "title": "Problem Set 4",
    "section": "Part 6",
    "text": "Part 6\n\n# Smote Evaluation\ntest.X = model.matrix(Responded ~ 0 + ., test)\nlasso.pred.class = as.factor(predict(lasso, test.X, s = 'lambda.min', type = 'class'))\ntree.pred.class = as.factor(predict(smote_tree, test))\n\ncm.lasso = confusionMatrix(data = lasso.pred.class,\n                           reference = test$Responded,\n                           positive = \"1\")\n\ncm.tree = confusionMatrix(data = tree.pred.class,\n                          reference = test$Responded,\n                          positive = \"1\")\n\n\nprint(cm.lasso)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 24624  1261\n         1   610    82\n                                          \n               Accuracy : 0.9296          \n                 95% CI : (0.9265, 0.9326)\n    No Information Rate : 0.9495          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.0479          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.061057        \n            Specificity : 0.975826        \n         Pos Pred Value : 0.118497        \n         Neg Pred Value : 0.951285        \n             Prevalence : 0.050532        \n         Detection Rate : 0.003085        \n   Detection Prevalence : 0.026038        \n      Balanced Accuracy : 0.518442        \n                                          \n       'Positive' Class : 1               \n                                          \n\n\n\nprint(cm.tree)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 25166  1341\n         1    68     2\n                                          \n               Accuracy : 0.947           \n                 95% CI : (0.9442, 0.9496)\n    No Information Rate : 0.9495          \n    P-Value [Acc &gt; NIR] : 0.968           \n                                          \n                  Kappa : -0.0022         \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 1.489e-03       \n            Specificity : 9.973e-01       \n         Pos Pred Value : 2.857e-02       \n         Neg Pred Value : 9.494e-01       \n             Prevalence : 5.053e-02       \n         Detection Rate : 7.525e-05       \n   Detection Prevalence : 2.634e-03       \n      Balanced Accuracy : 4.994e-01       \n                                          \n       'Positive' Class : 1"
  },
  {
    "objectID": "posts/Problem Set 4/index.html#part-7",
    "href": "posts/Problem Set 4/index.html#part-7",
    "title": "Problem Set 4",
    "section": "Part 7",
    "text": "Part 7\n\nlasso.probs = predict(lasso, test.X, s = 'lambda.min', type = 'response')\ntree.probs = predict(smote_tree, test, type = 'prob')[,2]\n\nThis ROC plot can help me tell NVO that the LASSO model ended up performing better than the Decision Tree model. Both are pretty close when it comes to their results and their placement to the baseline on the plot, but the LASSO curve is shown to be further away from the baseline, thus having a better predictive performance.\n\npar(pty = \"s\")\nlasso_roc = roc(test$Responded ~ lasso.probs,\n                plot = TRUE, print.auc = TRUE, print.auc.x = 0.3, print.auc.y = 0.35,\n                col = \"skyblue\", lwd = 3, legacy.axes = TRUE)\n\ntree_roc = roc(test$Responded ~ tree.probs,\n                plot = TRUE, print.auc = TRUE, print.auc.x = 0.3, print.auc.y = 0.28,\n                col = \"magenta\", lwd = 3, legacy.axes = TRUE, add = TRUE)\n\nlegend(\"bottomright\", legend = c(\"LASSO\", \"Decision Tree\"),\n       col = c(\"skyblue\", \"magenta\"), lwd = 3)"
  },
  {
    "objectID": "posts/Problem Set 4/index.html#part-8",
    "href": "posts/Problem Set 4/index.html#part-8",
    "title": "Problem Set 4",
    "section": "Part 8",
    "text": "Part 8\nPrecision-Recall Chart\n\nprc = pROC::coords(lasso_roc, ret = c(\"threshold\", \"precision\", \"recall\"))\n\nprc %&gt;%\n  filter(recall &gt; 0.01) %&gt;%\n  ggplot(aes(x = recall, y = precision)) +\n  geom_line(linewidth = 2, color = \"skyblue\") +\n  geom_hline(yintercept = 0.05, color = \"black\", linetype = 'dashed') +\n  labs(title = \"Precision-Recall Curve\",\n       x = \"Recall / Sensitivity / True Positive Rate\",\n       y = \"Precision (Response Rate\") +\n  theme_clean()\n\n\n\n\nCumulative Gain Chart\n\ntest$prob = lasso.probs = predict(lasso, test.X, s = 'lambda.min', type = 'response')\n\n\ntest.cg = test %&gt;%\n  select(Responded, prob) %&gt;%\n  mutate(Responded = ifelse(Responded == \"1\", 1, 0)) %&gt;%\n  arrange(desc(prob)) %&gt;%\n  mutate(pct_dat = row_number() /n(),\n         pct_pos = cumsum(Responded)/sum(Responded))\n\ntest.cg %&gt;%\n  ggplot(aes(x = pct_dat, y = pct_pos)) +\n  geom_abline(intercept = 0, slope = 1,\n              linetype = \"dashed\") +\n  geom_vline(xintercept = 0.5, linetype = \"dotted\", color = \"black\") +\n  geom_line(linewidth = 1.5, color = \"skyblue\") +\n  labs(title = \"Cumulative Gains for LASSO Model\",\n       x = \"Percent of Data Contacted\",\n       y = \"Percent of Responders Captured\") +\n  theme_clean()"
  },
  {
    "objectID": "posts/Problem Set 4/index.html#part-9",
    "href": "posts/Problem Set 4/index.html#part-9",
    "title": "Problem Set 4",
    "section": "Part 9",
    "text": "Part 9\nUsing the charts from parts 6 and 7, the model that resulted as the most accurate was the Decision Tree model, whereas the LASSO model ended up with a higher Kappa. When looking at the Recall (Sensitivity) and Precision (Pos Pred Value), our business problem is trying to identify people who will respond and increase NVO’s response rate as well. While we do aim to not waste materials and effort, we also don’t want to miss out on opportunities. The two measures Sensitivity and Pos Pred Value will help us look at the trade-off we face here.\nThe LASSO model has a higher sensitivity and precision despite a lower accuracy. However, its specificity is lower and its Neg Pred Value is higher. Both of them have small precisions (Pos Pred Value) which means our response rates could be lower. And their sensitivities are low, but data balancing allowed us to improve our model’s ability to learn rules that help us sense the positive class and miss out on opportunities. Both models have high specificity, showing that they can predict who is not gping to respond at a higher rate.\nWe can see that much of the data being found using the LASSO model seems to perform better than the Decision Tree model. If using the data we have developed using the LASSO model data, we can assume that only 6% of our predictions will be correct and that 11.8% of our predicted responders are going to actually respond.\nThe ROC plot also determines that the LASSO model performs better. Both are pretty close when it comes to their results and their placement to the baseline on the plot, but the LASSO curve is shown to be further away from the baseline, thus having a better predictive performance. With such numbers, we determine a bit better on whether or not we want to risk for more opportunities. Based off our numbers, it looks like we will go for more precision than opportunities due to such low numbers."
  },
  {
    "objectID": "posts/Problem Set 7/index.html",
    "href": "posts/Problem Set 7/index.html",
    "title": "Problem Set #7",
    "section": "",
    "text": "LHBA’s big-picture business problem is to use supervised data to help predict selling prices of homes. What features are playing roles in determining such prices? How are the different datasets showing a significance when market conditions are always changing?"
  },
  {
    "objectID": "posts/Problem Set 7/index.html#part-1",
    "href": "posts/Problem Set 7/index.html#part-1",
    "title": "Problem Set #7",
    "section": "",
    "text": "LHBA’s big-picture business problem is to use supervised data to help predict selling prices of homes. What features are playing roles in determining such prices? How are the different datasets showing a significance when market conditions are always changing?"
  },
  {
    "objectID": "posts/Problem Set 7/index.html#part-2",
    "href": "posts/Problem Set 7/index.html#part-2",
    "title": "Problem Set #7",
    "section": "Part 2",
    "text": "Part 2\n\nAre the locations of these homes also being included and used as a factor to determine predictions of selling prices in the datatset?\nHow much does it cost to generally flip a home? What cost are involved and how much time is it going to take to complete the overall process?\nUpgrading features within a home is a common thing. With that being said, will updates on home upgrades also be included along with the dataset?"
  },
  {
    "objectID": "posts/Problem Set 7/index.html#part-3",
    "href": "posts/Problem Set 7/index.html#part-3",
    "title": "Problem Set #7",
    "section": "Part 3",
    "text": "Part 3\nLHBA’s business problem is the need to predict the selling prices of homes. As we approach this problem as a regression problem, we can use this supervised dataset to tune and develop a model to help with predicting home prices. We can view error and evaluate the model, also known as RMSE, to determine the performances of the developed predictions. This RMSE shows us the average distance between predicted values from the model to the actual values in the dataset. As we see error, we can determine whether we should use such models or if we need to continue tuning them. Error represents uncertainty within a model. As we build a model and find error, we can then end it off with visualizing such uncertainty."
  },
  {
    "objectID": "posts/Problem Set 7/index.html#part-4",
    "href": "posts/Problem Set 7/index.html#part-4",
    "title": "Problem Set #7",
    "section": "Part 4",
    "text": "Part 4\nIn order to help develop predictions for our business problem, we need to tune a model that best fits our data. As we aim to develop small values of error within our model, we can have that “data science” to rely on as we aim to make decisions and prediction of prices for homes.\nWe will not be using these models determine absolute prices for there are some important features that affect the prices of homes that are not in the dataset.\nThese predictions will help use determine where and how we could possibly advertise certain homes. Finding a proper target audience that fits not just home features, but also income, community, and so many other features that do not revolve around a home is the easiest way to a successfully achieving the business problem."
  },
  {
    "objectID": "posts/Problem Set 7/index.html#part-5",
    "href": "posts/Problem Set 7/index.html#part-5",
    "title": "Problem Set #7",
    "section": "Part 5",
    "text": "Part 5\nWe will start off by making sure each of our steps are explained and that they connect and have relation to the target solution. Being able to understand the data being used is also important when trying to provide a solution. We will begin with deciding on which features we would like to have. Features such as house size, number of rooms, bathrooms, appliances, age of home, neighborhood, location, and crime rate would be perfect examples for providing our solution. As we rely on data, trying to get such desired features in our datasets would be the goal.\nWe will determine what features and data is present and being covered and will then prepare our data. We will start by cleaning the data and remove and edit features we may find relevant to finding a solution and then create new datasets by combining data from multiple sources. We will then re-format our data in a way that will help provide our solution before building and tuning models that will then assess and evaluate such decisions."
  },
  {
    "objectID": "posts/Problem Set 7/index.html#part-6",
    "href": "posts/Problem Set 7/index.html#part-6",
    "title": "Problem Set #7",
    "section": "Part 6",
    "text": "Part 6\n\nrm(list = ls())\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(glmnet)\nlibrary(rpart)\nlibrary(pROC)\nlibrary(ggthemes)\nlibrary(AppliedPredictiveModeling)\nlibrary(performanceEstimation)\nlibrary(dummy)\n\n\npost = read_csv(\"PostCrisisCV.csv\")\n\nRows: 1657 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (18): Property, LandValue, BuildingValue, Acres, AboveSpace, Basement, D...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npre = read_csv(\"PreCrisisCV.csv\")\n\nRows: 1978 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (18): Property, LandValue, BuildingValue, Acres, AboveSpace, Basement, D...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\non_market = read_csv(\"OnMarketTest-1.csv\")\n\nRows: 2000 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (18): Property, LandValue, BuildingValue, Acres, AboveSpace, Basement, D...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nmerged_data = merge(post, pre, all = TRUE)\n\nmerged_fct = merged_data %&gt;%\n  select(-Price, -Property, -LandValue, -BuildingValue, -Acres, -AboveSpace, -Basement, -Deck, -Baths, -Toilets, -Beds, -Rooms, -Age, -Car) %&gt;%\n  mutate_all(.funs = factor)\n\nmerged_num = merged_data %&gt;%\n  select(Price, Property, LandValue, BuildingValue, Acres, AboveSpace, Basement, Deck, Baths, Toilets, Beds, Rooms, Age, Car)\n\nmerged_data = bind_cols(merged_num, merged_fct)\n\n\nmerged_data %&gt;%\n  keep(is.numeric) %&gt;%\n  cor() %&gt;%\n  corrplot::corrplot(., method = \"number\", type = \"lower\", number.cex = 0.6, tl.cex = 0.7)\n\n\n\n\n\nmerged_data = merged_data %&gt;%\n  select(-BuildingValue, -LandValue, -AboveSpace)\n\n\nmerged_data %&gt;%\n  ggplot(aes(Price)) +\n  geom_histogram(color = \"black\", bg = \"skyblue\") +\n  labs(title = \"Distribution of Selling Prices\",\n       x = \"Selling price\",\n       y = \"Count of homes\") +\n  theme_classic()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nmerged_dum = dummy(merged_data, int = TRUE)\nmerged_num = merged_data %&gt;%\n  keep(is.numeric)\nmerged_data = bind_cols(merged_num, merged_dum)\nrm(merged_dum, merged_num)\n\n\n#Partition Data\nset.seed(123)\nidx = createDataPartition(merged_data$Price, p = 0.8, list = FALSE)\ntrain = merged_data[idx, ]\ntest = merged_data[-idx, ]\nrm(idx)\n\n\ntrain_model = train(Price ~ .,\n                   data = train,\n                   method = \"rpart\",\n                   trControl = trainControl(method = \"cv\", number = 10),\n                   tuneGrid = expand.grid(cp = seq(0.0, 0.01, 0.0001)),\n                   control = rpart.control(minbucket = 1)\n                   )\nplot(train_model)\n\n\n\n\n\nlibrary(rpart.plot)\nrpart.plot(train_model$finalModel)\n\n\n\n\n\nlibrary(iml)\nlibrary(patchwork)\n\ntree_predictor = iml::Predictor$new(train_model,\n                                    data = test,\n                                    y = test$Price)\n\ntree_imp = iml::FeatureImp$new(tree_predictor, loss = \"rmse\", compare = \"ratio\")\nplot(tree_imp)\n\n\n\n\n\ntree_imp$results %&gt;%\n  filter(importance &gt; 1)\n\n           feature importance.05 importance importance.95 permutation.error\n1     Fireplaces_0     1.2207100   1.266226      1.299777          44498.53\n2          Toilets     1.0343090   1.126590      1.218834          39591.35\n3     Fireplaces_1     1.0670219   1.103266      1.111387          38771.70\n4            Baths     1.0094141   1.089840      1.218106          38299.86\n5            Acres     1.0474995   1.066549      1.115988          37481.37\n6             AC_0     1.0496945   1.061062      1.069745          37288.53\n7             Beds     1.0417803   1.054364      1.061769          37053.17\n8              Age     1.0238457   1.049559      1.082720          36884.28\n9  PoorCondition_0     1.0311803   1.032313      1.052531          36278.20\n10           Rooms     0.9649791   1.031915      1.103644          36264.25\n11        Property     1.0102540   1.026174      1.032801          36062.48\n12 GoodCondition_0     1.0181497   1.023707      1.041996          35975.78\n13        Basement     1.0124812   1.018716      1.023605          35800.39\n14             Car     1.0011205   1.004414      1.010416          35297.78\n\n\n\ntrain_new = dplyr::select(train, Fireplaces_0, Toilets, Fireplaces_1, Baths, Acres, AC_0, Beds, Age, PoorCondition_0, Rooms, Price)\n\nnew_tree = caret::train(Price ~ .,\n                        data = train_new,\n                        method = \"rpart\",\n                        trControl = trainControl(method = \"cv\", number = 10),\n                        tuneGrid = expand.grid(cp = seq(0.0, 0.01, 0.0001)),\n                        control = rpart.control(minbucket = 1)\n                        )\nplot(new_tree)\n\n\n\n\n\nrpart.plot(new_tree$finalModel)\n\n\n\n\n\ntrain_error = postResample(predict(new_tree, train), train$Price)[[\"RMSE\"]]\n\ncv_error = min(new_tree$results$RMSE)\n\ntest_error = postResample(predict(new_tree, test), test$Price)[[\"RMSE\"]]\n\ndata.frame(\n  \"Error Source\" = c(\"Training\", \"Cross-Validation\", \"Testing\"),\n  \"RMSE\" = c(train_error, cv_error, test_error)\n)\n\n      Error.Source     RMSE\n1         Training 28746.09\n2 Cross-Validation 34783.00\n3          Testing 36510.79"
  },
  {
    "objectID": "posts/Problem Set 6/index.html",
    "href": "posts/Problem Set 6/index.html",
    "title": "Problem Set #6",
    "section": "",
    "text": "rm(list = ls())\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(performanceEstimation)\nlibrary(pROC)\n\nType 'citation(\"pROC\")' for a citation.\n\nAttaching package: 'pROC'\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(dummy)\n\ndummy 0.1.3\ndummyNews()\n\nlibrary(ada)\nbank = read_csv(\"UniversalBank.csv\")\n\nRows: 5000 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): ID, Age, Experience, Income, ZIP Code, Family, CCAvg, Education, M...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(bank)\n\nRows: 5,000\nColumns: 14\n$ ID                   &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15…\n$ Age                  &lt;dbl&gt; 25, 45, 39, 35, 35, 37, 53, 50, 35, 34, 65, 29, 4…\n$ Experience           &lt;dbl&gt; 1, 19, 15, 9, 8, 13, 27, 24, 10, 9, 39, 5, 23, 32…\n$ Income               &lt;dbl&gt; 49, 34, 11, 100, 45, 29, 72, 22, 81, 180, 105, 45…\n$ `ZIP Code`           &lt;dbl&gt; 91107, 90089, 94720, 94112, 91330, 92121, 91711, …\n$ Family               &lt;dbl&gt; 4, 3, 1, 1, 4, 4, 2, 1, 3, 1, 4, 3, 2, 4, 1, 1, 4…\n$ CCAvg                &lt;dbl&gt; 1.6, 1.5, 1.0, 2.7, 1.0, 0.4, 1.5, 0.3, 0.6, 8.9,…\n$ Education            &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 1, 3, 3…\n$ Mortgage             &lt;dbl&gt; 0, 0, 0, 0, 0, 155, 0, 0, 104, 0, 0, 0, 0, 0, 0, …\n$ `Personal Loan`      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1…\n$ `Securities Account` &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0…\n$ `CD Account`         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Online               &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0…\n$ CreditCard           &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0…\nRemove any unnecessary features, rename features, and change any categorical variables into factors. We can also change the variables of our target feature into Positive or Negative.\nbank = bank %&gt;%\n  select(-ID, -`ZIP Code`, -Experience) %&gt;%\n  rename(Loan = `Personal Loan`,\n         Securities = `Securities Account`,\n         CD = `CD Account`) %&gt;%\n  mutate(Loan = ifelse(Loan == 1, \"positive\", \"negtive\")) %&gt;%\n  mutate(Loan = factor(Loan)) %&gt;%\n  mutate_at(vars(Education, Securities, CD, Online, CreditCard), .funs = factor)\nLook for any missing values within the dataset.\n#Calculate percent of missing values for features\nmissing_df = as.numeric(purrr::map(bank, ~mean(is.na(.)))) * 100\n\n#Assign values to data frame to easily view\ndf = data.frame(PercentMissing = missing_df,\n                row.names = names(bank)) %&gt;%\n  arrange(desc(PercentMissing))\n\nprint(df)\n\n           PercentMissing\nAge                     0\nIncome                  0\nFamily                  0\nCCAvg                   0\nEducation               0\nMortgage                0\nLoan                    0\nSecurities              0\nCD                      0\nOnline                  0\nCreditCard              0\nWe can see that our dataset does not have any missing values."
  },
  {
    "objectID": "posts/Problem Set 6/index.html#part-1-partition-the-data",
    "href": "posts/Problem Set 6/index.html#part-1-partition-the-data",
    "title": "Problem Set #6",
    "section": "Part 1: Partition the Data",
    "text": "Part 1: Partition the Data\n\nset.seed(453)\nidx = createDataPartition(bank$Loan, p = 0.7, list = FALSE)\ntrain = bank[idx, ]\ntest = bank[-idx, ]\nrm(idx)\n\nAddress the class imbalance.\n\ntable(train$Loan)\n\n\n negtive positive \n    3164      336 \n\n\nWe will use SMOTE to help reduce the large imbalance that is present. I will first start off by converting all factors into dummy variables and make sure that each has one of its category dummies removed.\n\ndum = select(train, -Loan) %&gt;%\n  dummy(., int = TRUE) %&gt;%\n  select(-Education_1)\n\ntrain.dum = keep(train, is.numeric) %&gt;%\n  bind_cols(., dum)\n\ntrain.dum[\"Loan\"] = train$Loan\n\n\nsmote_train = smote(Loan ~ .,\n                    data = train.dum,\n                    perc.over = 7,\n                    perc.under = 1.3)\n\ntable(smote_train$Loan)\n\n\n negtive positive \n    3057     2688"
  },
  {
    "objectID": "posts/Problem Set 6/index.html#part-2-decision-tree",
    "href": "posts/Problem Set 6/index.html#part-2-decision-tree",
    "title": "Problem Set #6",
    "section": "Part 2: Decision Tree",
    "text": "Part 2: Decision Tree\n\nctrl = caret::trainControl(method = 'cv', \n                           number = 5,\n                           summaryFunction = twoClassSummary,\n                           classProbs = TRUE)\n\nset.seed(345)\ntree = train(Loan ~ .,\n            data = smote_train,\n            method = \"rpart\",\n            metric = \"ROC\",\n            trControl = ctrl,\n            tuneGrid = expand.grid(cp = seq(0.0, 0.01, 0.0001)),\n            control = rpart.control(minsplit = 1, minbucket = 1, maxdepth = 9)\n             )\n\nplot(tree)\n\n\n\n\n\nrpart.plot(tree$finalModel)"
  },
  {
    "objectID": "posts/Problem Set 6/index.html#part-3-random-forest",
    "href": "posts/Problem Set 6/index.html#part-3-random-forest",
    "title": "Problem Set #6",
    "section": "Part 3: Random Forest",
    "text": "Part 3: Random Forest\n\nset.seed(345)\nforest = train(Loan ~ .,\n               data = smote_train,\n               method = \"rf\",\n               metric = \"ROC\",\n               trControl = ctrl,\n               ntree = 500,\n               tuneGrid = expand.grid(.mtry = seq(2,8,1))\n               )\n\nplot(forest)"
  },
  {
    "objectID": "posts/Problem Set 6/index.html#part-4-gradient-boosting-machine",
    "href": "posts/Problem Set 6/index.html#part-4-gradient-boosting-machine",
    "title": "Problem Set #6",
    "section": "Part 4: Gradient Boosting Machine",
    "text": "Part 4: Gradient Boosting Machine\n\nboost_grid = expand.grid(\n  maxdepth = c(4, 5, 6, 7, 8),\n  iter = c(100, 150, 200, 250),\n  nu = 0.1\n)\n\nboost_ctrl = trainControl(method = \"cv\",\n                          number = 5,\n                          summaryFunction = twoClassSummary,\n                          classProbs = TRUE,\n                          allowParallel = TRUE)\n\nset.seed(345)\nboosted_trees = train(Loan ~ .,\n                      data = smote_train,\n                      trControl = boost_ctrl,\n                      tuneGrid = boost_grid,\n                      method = \"ada\",\n                      metric = \"ROC\")\n\n\nplot(boosted_trees)"
  },
  {
    "objectID": "posts/Problem Set 6/index.html#part-5-compare-precision-and-sensitivity",
    "href": "posts/Problem Set 6/index.html#part-5-compare-precision-and-sensitivity",
    "title": "Problem Set #6",
    "section": "Part 5: Compare Precision and Sensitivity",
    "text": "Part 5: Compare Precision and Sensitivity\n\ndum = select(test, -Loan) %&gt;%\n  dummy(., int = TRUE) %&gt;%\n  select(-Education_1)\n\ntest.dum = keep(test, is.numeric) %&gt;%\n  bind_cols(., dum)\n\ntest.dum[\"Loan\"] = test$Loan\n\n\nlibrary(DALEX)\n\nWelcome to DALEX (version: 2.4.3).\nFind examples and detailed introduction at: http://ema.drwhy.ai/\nAdditional features will be available after installation of: ggpubr.\nUse 'install_dependencies()' to get all suggested dependencies\n\n\n\nAttaching package: 'DALEX'\n\n\nThe following object is masked from 'package:dplyr':\n\n    explain\n\ntree_explain = DALEX::explain(\n  tree,\n  data = test.dum,\n  y = as.numeric(test.dum$Loan == \"positive\"),\n  type = \"classification\",\n  label = \"Decision Tree\"\n)\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  Decision Tree \n  -&gt; data              :  1500  rows  16  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  1500  values \n  -&gt; predict function  :  yhat.train  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package caret , ver. 6.0.94 , task classification (  default  ) \n  -&gt; model_info        :  type set to  classification \n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.1138532 , max =  1  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9798387 , mean =  -0.01785323 , max =  0.875  \n  A new explainer has been created!  \n\nforest_explain = DALEX::explain(\n  forest,\n  data = test.dum,\n  y = as.numeric(test.dum$Loan == \"positive\"),\n  type = \"classification\",\n  label = \"Random Forest\"\n)\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  Random Forest \n  -&gt; data              :  1500  rows  16  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  1500  values \n  -&gt; predict function  :  yhat.train  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package caret , ver. 6.0.94 , task classification (  default  ) \n  -&gt; model_info        :  type set to  classification \n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.1124773 , max =  1  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.888 , mean =  -0.01647733 , max =  0.9  \n  A new explainer has been created!  \n\nadaboost_explain = DALEX::explain(\n  boosted_trees,\n  data = test.dum,\n  y = as.numeric(test.dum$Loan == \"positive\"),\n  type = \"classification\",\n  label = \"AdaBoost\"\n)\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  AdaBoost \n  -&gt; data              :  1500  rows  16  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  1500  values \n  -&gt; predict function  :  yhat.train  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package caret , ver. 6.0.94 , task classification (  default  ) \n  -&gt; model_info        :  type set to  classification \n  -&gt; predicted values  :  numerical, min =  6.433978e-15 , mean =  0.1020199 , max =  1  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9999994 , mean =  -0.006019928 , max =  1  \n  A new explainer has been created!  \n\n\n\n# Model performance\ntree_perf = DALEX::model_performance(tree_explain)\nforest_perf = DALEX::model_performance(forest_explain)\nadaboost_perf = DALEX::model_performance(adaboost_explain)\n\n# Plot the Precision Recall Curve\nplot(tree_perf, forest_perf, adaboost_perf, geom = 'prc')"
  },
  {
    "objectID": "posts/Problem Set 6/index.html#part-6-roc-plot",
    "href": "posts/Problem Set 6/index.html#part-6-roc-plot",
    "title": "Problem Set #6",
    "section": "Part 6: ROC Plot",
    "text": "Part 6: ROC Plot\n\n# Plot the Receiver Operator Characteristic\nplot(tree_perf, forest_perf, adaboost_perf, geom = 'roc')\n\n\n\n\n\n# Compare the AUCs\nmatrix(c(\"Model\",\n         \"Decision Tree\",\n         \"Random Forest\",\n         \"Adaboost\",\n         \"AUC\",\n         round(tree_perf$measures$auc, 3),\n         round(forest_perf$measures$auc, 3),\n         round(adaboost_perf$measures$auc, 3)),\n       ncol = 2)\n\n     [,1]            [,2]   \n[1,] \"Model\"         \"AUC\"  \n[2,] \"Decision Tree\" \"0.994\"\n[3,] \"Random Forest\" \"0.997\"\n[4,] \"Adaboost\"      \"0.998\""
  },
  {
    "objectID": "posts/Problem Set 6/index.html#part-7-importance-of-partitioning-data",
    "href": "posts/Problem Set 6/index.html#part-7-importance-of-partitioning-data",
    "title": "Problem Set #6",
    "section": "Part 7: Importance of Partitioning Data",
    "text": "Part 7: Importance of Partitioning Data\nPartitioning data allows us to avoid overfitting our data. As we split up our data into training and testing datasets, we have the opportunity to tune our data to fit the model. Whereas our testing data uses the model we have developed to make predictions on data that has not been touch and tuned. We also have the opportunity to develop a more efficient model with smaller data to use as a reference. Our training data will always be more accuate to the dataset but seeing that is common for our training data is the data that is not being tuned and used to develop a model."
  },
  {
    "objectID": "posts/Problem Set 6/index.html#part-8-how-baggingensemble-models-can-improve-model-accuracyperformance.",
    "href": "posts/Problem Set 6/index.html#part-8-how-baggingensemble-models-can-improve-model-accuracyperformance.",
    "title": "Problem Set #6",
    "section": "Part 8: How bagging/ensemble models can improve model accuracy/performance.",
    "text": "Part 8: How bagging/ensemble models can improve model accuracy/performance.\nBagging aims to decrease variance, which can then lead to increased accuracy. Boosting aims to reduce the bias present in a model. This can also lead to increased accuracy. These are both a type of an ensemble model. These models combine the predictions that are being developed by multiple individual models to develop an accurate prediction. They can reduce variance, bias, and can learn from different models. We can possibly capture patterns being seen in the data and can rely on more than one model."
  },
  {
    "objectID": "posts/Problem set 8/ps8.html",
    "href": "posts/Problem set 8/ps8.html",
    "title": "Problem Set #8",
    "section": "",
    "text": "Actionable -&gt; Many patients go in for check-ups due to pain and are then typically prescribed medicine. As a result, the hospital could place their pharmacy on the first floor for easier access for patients who have to grab/pick-up their prescriptions, thus avoiding possible instances for more pain to arise.\nTrivial -&gt; Individuals that come to the hospital with broken bones typically get surgery in order to heal the bone properly. This is common and is dealt with by an orthopedic doctor.\nInexplicable -&gt; A big inexplicable event that occurs is cancer. This disorder can develop and affect almost anyone is our society. There is also a variety of forms of cancer, thus making the complete knowledge on all cancer being incomplete. There is no official cure for cancer.\n\n\n\n\nI grew up in the community of artistic gymnastics. This involvement of mine with this sport began when I was six and now I am 20 and currently an assistant gymnastics coach at my previous high school. An application of association rule that might be useful in gymnastics would be if a gymnast practices the uneven bars then they would need to purchase and own a pair of grips and rolls of tape. Grips are common to use on uneven bars to help gymnast gain better grip on the bar whereas tape is used to help cover any rips of skin that may occur on their hands as they continue to practice.\n\n\n\n\ngroceries = read.transactions(\"groceries.csv\", sep = \",\")\n\n\ngroceries\n\ntransactions in sparse format with\n 9835 transactions (rows) and\n 169 items (columns)\n\n\n\nsummary(groceries)\n\ntransactions as itemMatrix in sparse format with\n 9835 rows (elements/itemsets/transactions) and\n 169 columns (items) and a density of 0.02609146 \n\nmost frequent items:\n      whole milk other vegetables       rolls/buns             soda \n            2513             1903             1809             1715 \n          yogurt          (Other) \n            1372            34055 \n\nelement (itemset/transaction) length distribution:\nsizes\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n2159 1643 1299 1005  855  645  545  438  350  246  182  117   78   77   55   46 \n  17   18   19   20   21   22   23   24   26   27   28   29   32 \n  29   14   14    9   11    4    6    1    1    1    1    3    1 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   3.000   4.409   6.000  32.000 \n\nincludes extended item information - examples:\n            labels\n1 abrasive cleaner\n2 artif. sweetener\n3   baby cosmetics\n\n\n\ns = summary(groceries)\n\n\nsizes = s@lengths\n\nprint(sizes)\n\nsizes\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n2159 1643 1299 1005  855  645  545  438  350  246  182  117   78   77   55   46 \n  17   18   19   20   21   22   23   24   26   27   28   29   32 \n  29   14   14    9   11    4    6    1    1    1    1    3    1 \n\n\n\ngroceries_frequency = \n  tibble(\n    Items = names(itemFrequency(groceries)),\n    Frequency = itemFrequency(groceries)\n  )\n\n\nThe 10 least frequently purchased items are…\n\n\n\nbaby food\nsound storage medium\npreservation products\nbags\nkitchen utensil\nbaby cosmetics\nfrozen chicken\ntoilet cleaner\nmake up remover\nsalad dressing\n\n\ngroceries_frequency %&gt;%\n  arrange(Frequency) %&gt;%\n  slice(1:10)\n\n# A tibble: 10 × 2\n   Items                 Frequency\n   &lt;chr&gt;                     &lt;dbl&gt;\n 1 baby food              0.000102\n 2 sound storage medium   0.000102\n 3 preservation products  0.000203\n 4 bags                   0.000407\n 5 kitchen utensil        0.000407\n 6 baby cosmetics         0.000610\n 7 frozen chicken         0.000610\n 8 toilet cleaner         0.000712\n 9 make up remover        0.000813\n10 salad dressing         0.000813\n\n\n\nChanging the minimum rule length to 3 generates 16 rules. Changing it to 4 generates 0 rules.\n\n\ngroceryrules = \n  apriori(groceries,\n          parameter = list(\n            support = 0.015,\n            confidence = 0.25,\n            minlen = 3\n          ) )\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n       0.25    0.1    1 none FALSE            TRUE       5   0.015      3\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 147 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[169 item(s), 9835 transaction(s)] done [0.01s].\nsorting and recoding items ... [73 item(s)] done [0.00s].\ncreating transaction tree ... done [0.01s].\nchecking subsets of size 1 2 3 done [0.00s].\nwriting ... [16 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\n\n\nsummary(groceryrules)\n\nset of 16 rules\n\nrule length distribution (lhs + rhs):sizes\n 3 \n16 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      3       3       3       3       3       3 \n\nsummary of quality measures:\n    support          confidence        coverage            lift      \n Min.   :0.01515   Min.   :0.2704   Min.   :0.02928   Min.   :1.510  \n 1st Qu.:0.01556   1st Qu.:0.3067   1st Qu.:0.04230   1st Qu.:1.840  \n Median :0.01749   Median :0.4007   Median :0.04814   Median :2.016  \n Mean   :0.01865   Mean   :0.3905   Mean   :0.04984   Mean   :2.065  \n 3rd Qu.:0.02227   3rd Qu.:0.4745   3rd Qu.:0.05618   3rd Qu.:2.212  \n Max.   :0.02318   Max.   :0.5174   Max.   :0.07483   Max.   :2.842  \n     count      \n Min.   :149.0  \n 1st Qu.:153.0  \n Median :172.0  \n Mean   :183.4  \n 3rd Qu.:219.0  \n Max.   :228.0  \n\nmining info:\n      data ntransactions support confidence\n groceries          9835   0.015       0.25\n                                                                                        call\n apriori(data = groceries, parameter = list(support = 0.015, confidence = 0.25, minlen = 3))\n\n\n\ngroceryrules %&gt;%\n  sort(by = \"confidence\") %&gt;%\n  inspect()\n\n     lhs                                    rhs                support   \n[1]  {tropical fruit, yogurt}            =&gt; {whole milk}       0.01514997\n[2]  {other vegetables, yogurt}          =&gt; {whole milk}       0.02226741\n[3]  {other vegetables, root vegetables} =&gt; {whole milk}       0.02318251\n[4]  {other vegetables, tropical fruit}  =&gt; {whole milk}       0.01708185\n[5]  {root vegetables, whole milk}       =&gt; {other vegetables} 0.02318251\n[6]  {rolls/buns, yogurt}                =&gt; {whole milk}       0.01555669\n[7]  {other vegetables, rolls/buns}      =&gt; {whole milk}       0.01789527\n[8]  {tropical fruit, whole milk}        =&gt; {other vegetables} 0.01708185\n[9]  {whole milk, yogurt}                =&gt; {other vegetables} 0.02226741\n[10] {tropical fruit, whole milk}        =&gt; {yogurt}           0.01514997\n[11] {rolls/buns, whole milk}            =&gt; {other vegetables} 0.01789527\n[12] {other vegetables, whole milk}      =&gt; {root vegetables}  0.02318251\n[13] {other vegetables, whole milk}      =&gt; {yogurt}           0.02226741\n[14] {whole milk, yogurt}                =&gt; {rolls/buns}       0.01555669\n[15] {rolls/buns, whole milk}            =&gt; {yogurt}           0.01555669\n[16] {whole milk, yogurt}                =&gt; {tropical fruit}   0.01514997\n     confidence coverage   lift     count\n[1]  0.5173611  0.02928317 2.024770 149  \n[2]  0.5128806  0.04341637 2.007235 219  \n[3]  0.4892704  0.04738180 1.914833 228  \n[4]  0.4759207  0.03589222 1.862587 168  \n[5]  0.4740125  0.04890696 2.449770 228  \n[6]  0.4526627  0.03436706 1.771563 153  \n[7]  0.4200477  0.04260295 1.643919 176  \n[8]  0.4038462  0.04229792 2.087140 168  \n[9]  0.3974592  0.05602440 2.054131 219  \n[10] 0.3581731  0.04229792 2.567516 149  \n[11] 0.3159785  0.05663447 1.633026 176  \n[12] 0.3097826  0.07483477 2.842082 228  \n[13] 0.2975543  0.07483477 2.132979 219  \n[14] 0.2776770  0.05602440 1.509648 153  \n[15] 0.2746858  0.05663447 1.969049 153  \n[16] 0.2704174  0.05602440 2.577089 149  \n\n\n\nChange the minimum rule length back to 2, produce a list of rules involving either soda or whipped/sour cream.\n\n\ngroceryrules2 = \n  apriori(groceries,\n          parameter = list(\n            support = 0.015,\n            confidence = 0.25,\n            minlen = 2\n          ) )\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n       0.25    0.1    1 none FALSE            TRUE       5   0.015      2\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 147 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[169 item(s), 9835 transaction(s)] done [0.00s].\nsorting and recoding items ... [73 item(s)] done [0.00s].\ncreating transaction tree ... done [0.01s].\nchecking subsets of size 1 2 3 done [0.00s].\nwriting ... [78 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\n\n\ngroceryrules2 %&gt;%\n  sort(by = \"confidence\") %&gt;%\n  inspect()\n\n     lhs                                    rhs                support   \n[1]  {tropical fruit, yogurt}            =&gt; {whole milk}       0.01514997\n[2]  {other vegetables, yogurt}          =&gt; {whole milk}       0.02226741\n[3]  {butter}                            =&gt; {whole milk}       0.02755465\n[4]  {curd}                              =&gt; {whole milk}       0.02613116\n[5]  {other vegetables, root vegetables} =&gt; {whole milk}       0.02318251\n[6]  {other vegetables, tropical fruit}  =&gt; {whole milk}       0.01708185\n[7]  {root vegetables, whole milk}       =&gt; {other vegetables} 0.02318251\n[8]  {domestic eggs}                     =&gt; {whole milk}       0.02999492\n[9]  {rolls/buns, yogurt}                =&gt; {whole milk}       0.01555669\n[10] {whipped/sour cream}                =&gt; {whole milk}       0.03223183\n[11] {root vegetables}                   =&gt; {whole milk}       0.04890696\n[12] {sugar}                             =&gt; {whole milk}       0.01504830\n[13] {root vegetables}                   =&gt; {other vegetables} 0.04738180\n[14] {frozen vegetables}                 =&gt; {whole milk}       0.02043721\n[15] {other vegetables, rolls/buns}      =&gt; {whole milk}       0.01789527\n[16] {chicken}                           =&gt; {other vegetables} 0.01789527\n[17] {cream cheese}                      =&gt; {whole milk}       0.01647178\n[18] {margarine}                         =&gt; {whole milk}       0.02419929\n[19] {chicken}                           =&gt; {whole milk}       0.01759024\n[20] {white bread}                       =&gt; {whole milk}       0.01708185\n[21] {beef}                              =&gt; {whole milk}       0.02125064\n[22] {tropical fruit, whole milk}        =&gt; {other vegetables} 0.01708185\n[23] {tropical fruit}                    =&gt; {whole milk}       0.04229792\n[24] {whipped/sour cream}                =&gt; {other vegetables} 0.02887646\n[25] {yogurt}                            =&gt; {whole milk}       0.05602440\n[26] {pip fruit}                         =&gt; {whole milk}       0.03009659\n[27] {whole milk, yogurt}                =&gt; {other vegetables} 0.02226741\n[28] {brown bread}                       =&gt; {whole milk}       0.02521607\n[29] {other vegetables}                  =&gt; {whole milk}       0.07483477\n[30] {pork}                              =&gt; {whole milk}       0.02216573\n[31] {napkins}                           =&gt; {whole milk}       0.01972547\n[32] {beef}                              =&gt; {other vegetables} 0.01972547\n[33] {pork}                              =&gt; {other vegetables} 0.02165735\n[34] {pastry}                            =&gt; {whole milk}       0.03324860\n[35] {frozen vegetables}                 =&gt; {other vegetables} 0.01779359\n[36] {citrus fruit}                      =&gt; {whole milk}       0.03050330\n[37] {fruit/vegetable juice}             =&gt; {whole milk}       0.02663955\n[38] {butter}                            =&gt; {other vegetables} 0.02003050\n[39] {tropical fruit, whole milk}        =&gt; {yogurt}           0.01514997\n[40] {domestic eggs}                     =&gt; {other vegetables} 0.02226741\n[41] {citrus fruit}                      =&gt; {other vegetables} 0.02887646\n[42] {frankfurter}                       =&gt; {whole milk}       0.02053889\n[43] {pip fruit}                         =&gt; {other vegetables} 0.02613116\n[44] {newspapers}                        =&gt; {whole milk}       0.02735130\n[45] {tropical fruit}                    =&gt; {other vegetables} 0.03589222\n[46] {margarine}                         =&gt; {other vegetables} 0.01972547\n[47] {chocolate}                         =&gt; {whole milk}       0.01667514\n[48] {beef}                              =&gt; {root vegetables}  0.01738688\n[49] {frankfurter}                       =&gt; {rolls/buns}       0.01921708\n[50] {sausage}                           =&gt; {rolls/buns}       0.03060498\n[51] {curd}                              =&gt; {yogurt}           0.01728521\n[52] {curd}                              =&gt; {other vegetables} 0.01718353\n[53] {coffee}                            =&gt; {whole milk}       0.01870869\n[54] {sausage}                           =&gt; {whole milk}       0.02989324\n[55] {rolls/buns, whole milk}            =&gt; {other vegetables} 0.01789527\n[56] {yogurt}                            =&gt; {other vegetables} 0.04341637\n[57] {bottled water}                     =&gt; {whole milk}       0.03436706\n[58] {other vegetables, whole milk}      =&gt; {root vegetables}  0.02318251\n[59] {rolls/buns}                        =&gt; {whole milk}       0.05663447\n[60] {other vegetables, whole milk}      =&gt; {yogurt}           0.02226741\n[61] {whole milk}                        =&gt; {other vegetables} 0.07483477\n[62] {fruit/vegetable juice}             =&gt; {other vegetables} 0.02104728\n[63] {whipped/sour cream}                =&gt; {yogurt}           0.02074225\n[64] {brown bread}                       =&gt; {other vegetables} 0.01870869\n[65] {sausage}                           =&gt; {other vegetables} 0.02694459\n[66] {frankfurter}                       =&gt; {other vegetables} 0.01647178\n[67] {tropical fruit}                    =&gt; {yogurt}           0.02928317\n[68] {whole milk, yogurt}                =&gt; {rolls/buns}       0.01555669\n[69] {rolls/buns, whole milk}            =&gt; {yogurt}           0.01555669\n[70] {whole milk, yogurt}                =&gt; {tropical fruit}   0.01514997\n[71] {pip fruit}                         =&gt; {tropical fruit}   0.02043721\n[72] {bottled water}                     =&gt; {soda}             0.02897814\n[73] {citrus fruit}                      =&gt; {yogurt}           0.02165735\n[74] {fruit/vegetable juice}             =&gt; {yogurt}           0.01870869\n[75] {sausage}                           =&gt; {soda}             0.02430097\n[76] {fruit/vegetable juice}             =&gt; {soda}             0.01840366\n[77] {bottled beer}                      =&gt; {whole milk}       0.02043721\n[78] {pastry}                            =&gt; {other vegetables} 0.02257245\n     confidence coverage   lift      count\n[1]  0.5173611  0.02928317 2.0247698 149  \n[2]  0.5128806  0.04341637 2.0072345 219  \n[3]  0.4972477  0.05541434 1.9460530 271  \n[4]  0.4904580  0.05327911 1.9194805 257  \n[5]  0.4892704  0.04738180 1.9148326 228  \n[6]  0.4759207  0.03589222 1.8625865 168  \n[7]  0.4740125  0.04890696 2.4497702 228  \n[8]  0.4727564  0.06344687 1.8502027 295  \n[9]  0.4526627  0.03436706 1.7715630 153  \n[10] 0.4496454  0.07168277 1.7597542 317  \n[11] 0.4486940  0.10899847 1.7560310 481  \n[12] 0.4444444  0.03385867 1.7393996 148  \n[13] 0.4347015  0.10899847 2.2466049 466  \n[14] 0.4249471  0.04809354 1.6630940 201  \n[15] 0.4200477  0.04260295 1.6439194 176  \n[16] 0.4170616  0.04290798 2.1554393 176  \n[17] 0.4153846  0.03965430 1.6256696 162  \n[18] 0.4131944  0.05856634 1.6170980 238  \n[19] 0.4099526  0.04290798 1.6044106 173  \n[20] 0.4057971  0.04209456 1.5881474 168  \n[21] 0.4050388  0.05246568 1.5851795 209  \n[22] 0.4038462  0.04229792 2.0871397 168  \n[23] 0.4031008  0.10493137 1.5775950 416  \n[24] 0.4028369  0.07168277 2.0819237 284  \n[25] 0.4016035  0.13950178 1.5717351 551  \n[26] 0.3978495  0.07564820 1.5570432 296  \n[27] 0.3974592  0.05602440 2.0541308 219  \n[28] 0.3887147  0.06487036 1.5212930 248  \n[29] 0.3867578  0.19349263 1.5136341 736  \n[30] 0.3844797  0.05765125 1.5047187 218  \n[31] 0.3766990  0.05236401 1.4742678 194  \n[32] 0.3759690  0.05246568 1.9430662 194  \n[33] 0.3756614  0.05765125 1.9414764 213  \n[34] 0.3737143  0.08896797 1.4625865 327  \n[35] 0.3699789  0.04809354 1.9121083 175  \n[36] 0.3685504  0.08276563 1.4423768 300  \n[37] 0.3684951  0.07229283 1.4421604 262  \n[38] 0.3614679  0.05541434 1.8681223 197  \n[39] 0.3581731  0.04229792 2.5675162 149  \n[40] 0.3509615  0.06344687 1.8138238 219  \n[41] 0.3488943  0.08276563 1.8031403 284  \n[42] 0.3482759  0.05897306 1.3630295 202  \n[43] 0.3454301  0.07564820 1.7852365 257  \n[44] 0.3426752  0.07981698 1.3411103 269  \n[45] 0.3420543  0.10493137 1.7677896 353  \n[46] 0.3368056  0.05856634 1.7406635 194  \n[47] 0.3360656  0.04961871 1.3152427 164  \n[48] 0.3313953  0.05246568 3.0403668 171  \n[49] 0.3258621  0.05897306 1.7716161 189  \n[50] 0.3257576  0.09395018 1.7710480 301  \n[51] 0.3244275  0.05327911 2.3256154 170  \n[52] 0.3225191  0.05327911 1.6668288 169  \n[53] 0.3222417  0.05805796 1.2611408 184  \n[54] 0.3181818  0.09395018 1.2452520 294  \n[55] 0.3159785  0.05663447 1.6330258 176  \n[56] 0.3112245  0.13950178 1.6084566 427  \n[57] 0.3109476  0.11052364 1.2169396 338  \n[58] 0.3097826  0.07483477 2.8420820 228  \n[59] 0.3079049  0.18393493 1.2050318 557  \n[60] 0.2975543  0.07483477 2.1329789 219  \n[61] 0.2928770  0.25551601 1.5136341 736  \n[62] 0.2911392  0.07229283 1.5046529 207  \n[63] 0.2893617  0.07168277 2.0742510 204  \n[64] 0.2884013  0.06487036 1.4905025 184  \n[65] 0.2867965  0.09395018 1.4822091 265  \n[66] 0.2793103  0.05897306 1.4435193 162  \n[67] 0.2790698  0.10493137 2.0004746 288  \n[68] 0.2776770  0.05602440 1.5096478 153  \n[69] 0.2746858  0.05663447 1.9690488 153  \n[70] 0.2704174  0.05602440 2.5770885 149  \n[71] 0.2701613  0.07564820 2.5746476 201  \n[72] 0.2621895  0.11052364 1.5035766 285  \n[73] 0.2616708  0.08276563 1.8757521 213  \n[74] 0.2587904  0.07229283 1.8551049 184  \n[75] 0.2586580  0.09395018 1.4833245 239  \n[76] 0.2545710  0.07229283 1.4598869 181  \n[77] 0.2537879  0.08052872 0.9932367 201  \n[78] 0.2537143  0.08896797 1.3112349 222  \n\n\n\ngroceryrules2 %&gt;%\n  subset(items %in% c(\"soda\", \"whipped/sour cream\")) %&gt;%\n  inspect()\n\n    lhs                        rhs                support    confidence\n[1] {fruit/vegetable juice} =&gt; {soda}             0.01840366 0.2545710 \n[2] {whipped/sour cream}    =&gt; {yogurt}           0.02074225 0.2893617 \n[3] {whipped/sour cream}    =&gt; {other vegetables} 0.02887646 0.4028369 \n[4] {whipped/sour cream}    =&gt; {whole milk}       0.03223183 0.4496454 \n[5] {sausage}               =&gt; {soda}             0.02430097 0.2586580 \n[6] {bottled water}         =&gt; {soda}             0.02897814 0.2621895 \n    coverage   lift     count\n[1] 0.07229283 1.459887 181  \n[2] 0.07168277 2.074251 204  \n[3] 0.07168277 2.081924 284  \n[4] 0.07168277 1.759754 317  \n[5] 0.09395018 1.483324 239  \n[6] 0.11052364 1.503577 285"
  },
  {
    "objectID": "posts/Problem set 8/ps8.html#question-1",
    "href": "posts/Problem set 8/ps8.html#question-1",
    "title": "Problem Set #8",
    "section": "",
    "text": "Actionable -&gt; Many patients go in for check-ups due to pain and are then typically prescribed medicine. As a result, the hospital could place their pharmacy on the first floor for easier access for patients who have to grab/pick-up their prescriptions, thus avoiding possible instances for more pain to arise.\nTrivial -&gt; Individuals that come to the hospital with broken bones typically get surgery in order to heal the bone properly. This is common and is dealt with by an orthopedic doctor.\nInexplicable -&gt; A big inexplicable event that occurs is cancer. This disorder can develop and affect almost anyone is our society. There is also a variety of forms of cancer, thus making the complete knowledge on all cancer being incomplete. There is no official cure for cancer."
  },
  {
    "objectID": "posts/Problem set 8/ps8.html#question-2",
    "href": "posts/Problem set 8/ps8.html#question-2",
    "title": "Problem Set #8",
    "section": "",
    "text": "I grew up in the community of artistic gymnastics. This involvement of mine with this sport began when I was six and now I am 20 and currently an assistant gymnastics coach at my previous high school. An application of association rule that might be useful in gymnastics would be if a gymnast practices the uneven bars then they would need to purchase and own a pair of grips and rolls of tape. Grips are common to use on uneven bars to help gymnast gain better grip on the bar whereas tape is used to help cover any rips of skin that may occur on their hands as they continue to practice."
  },
  {
    "objectID": "posts/Problem set 8/ps8.html#question-3",
    "href": "posts/Problem set 8/ps8.html#question-3",
    "title": "Problem Set #8",
    "section": "",
    "text": "groceries = read.transactions(\"groceries.csv\", sep = \",\")\n\n\ngroceries\n\ntransactions in sparse format with\n 9835 transactions (rows) and\n 169 items (columns)\n\n\n\nsummary(groceries)\n\ntransactions as itemMatrix in sparse format with\n 9835 rows (elements/itemsets/transactions) and\n 169 columns (items) and a density of 0.02609146 \n\nmost frequent items:\n      whole milk other vegetables       rolls/buns             soda \n            2513             1903             1809             1715 \n          yogurt          (Other) \n            1372            34055 \n\nelement (itemset/transaction) length distribution:\nsizes\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n2159 1643 1299 1005  855  645  545  438  350  246  182  117   78   77   55   46 \n  17   18   19   20   21   22   23   24   26   27   28   29   32 \n  29   14   14    9   11    4    6    1    1    1    1    3    1 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   3.000   4.409   6.000  32.000 \n\nincludes extended item information - examples:\n            labels\n1 abrasive cleaner\n2 artif. sweetener\n3   baby cosmetics\n\n\n\ns = summary(groceries)\n\n\nsizes = s@lengths\n\nprint(sizes)\n\nsizes\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n2159 1643 1299 1005  855  645  545  438  350  246  182  117   78   77   55   46 \n  17   18   19   20   21   22   23   24   26   27   28   29   32 \n  29   14   14    9   11    4    6    1    1    1    1    3    1 \n\n\n\ngroceries_frequency = \n  tibble(\n    Items = names(itemFrequency(groceries)),\n    Frequency = itemFrequency(groceries)\n  )\n\n\nThe 10 least frequently purchased items are…\n\n\n\nbaby food\nsound storage medium\npreservation products\nbags\nkitchen utensil\nbaby cosmetics\nfrozen chicken\ntoilet cleaner\nmake up remover\nsalad dressing\n\n\ngroceries_frequency %&gt;%\n  arrange(Frequency) %&gt;%\n  slice(1:10)\n\n# A tibble: 10 × 2\n   Items                 Frequency\n   &lt;chr&gt;                     &lt;dbl&gt;\n 1 baby food              0.000102\n 2 sound storage medium   0.000102\n 3 preservation products  0.000203\n 4 bags                   0.000407\n 5 kitchen utensil        0.000407\n 6 baby cosmetics         0.000610\n 7 frozen chicken         0.000610\n 8 toilet cleaner         0.000712\n 9 make up remover        0.000813\n10 salad dressing         0.000813\n\n\n\nChanging the minimum rule length to 3 generates 16 rules. Changing it to 4 generates 0 rules.\n\n\ngroceryrules = \n  apriori(groceries,\n          parameter = list(\n            support = 0.015,\n            confidence = 0.25,\n            minlen = 3\n          ) )\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n       0.25    0.1    1 none FALSE            TRUE       5   0.015      3\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 147 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[169 item(s), 9835 transaction(s)] done [0.01s].\nsorting and recoding items ... [73 item(s)] done [0.00s].\ncreating transaction tree ... done [0.01s].\nchecking subsets of size 1 2 3 done [0.00s].\nwriting ... [16 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\n\n\nsummary(groceryrules)\n\nset of 16 rules\n\nrule length distribution (lhs + rhs):sizes\n 3 \n16 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      3       3       3       3       3       3 \n\nsummary of quality measures:\n    support          confidence        coverage            lift      \n Min.   :0.01515   Min.   :0.2704   Min.   :0.02928   Min.   :1.510  \n 1st Qu.:0.01556   1st Qu.:0.3067   1st Qu.:0.04230   1st Qu.:1.840  \n Median :0.01749   Median :0.4007   Median :0.04814   Median :2.016  \n Mean   :0.01865   Mean   :0.3905   Mean   :0.04984   Mean   :2.065  \n 3rd Qu.:0.02227   3rd Qu.:0.4745   3rd Qu.:0.05618   3rd Qu.:2.212  \n Max.   :0.02318   Max.   :0.5174   Max.   :0.07483   Max.   :2.842  \n     count      \n Min.   :149.0  \n 1st Qu.:153.0  \n Median :172.0  \n Mean   :183.4  \n 3rd Qu.:219.0  \n Max.   :228.0  \n\nmining info:\n      data ntransactions support confidence\n groceries          9835   0.015       0.25\n                                                                                        call\n apriori(data = groceries, parameter = list(support = 0.015, confidence = 0.25, minlen = 3))\n\n\n\ngroceryrules %&gt;%\n  sort(by = \"confidence\") %&gt;%\n  inspect()\n\n     lhs                                    rhs                support   \n[1]  {tropical fruit, yogurt}            =&gt; {whole milk}       0.01514997\n[2]  {other vegetables, yogurt}          =&gt; {whole milk}       0.02226741\n[3]  {other vegetables, root vegetables} =&gt; {whole milk}       0.02318251\n[4]  {other vegetables, tropical fruit}  =&gt; {whole milk}       0.01708185\n[5]  {root vegetables, whole milk}       =&gt; {other vegetables} 0.02318251\n[6]  {rolls/buns, yogurt}                =&gt; {whole milk}       0.01555669\n[7]  {other vegetables, rolls/buns}      =&gt; {whole milk}       0.01789527\n[8]  {tropical fruit, whole milk}        =&gt; {other vegetables} 0.01708185\n[9]  {whole milk, yogurt}                =&gt; {other vegetables} 0.02226741\n[10] {tropical fruit, whole milk}        =&gt; {yogurt}           0.01514997\n[11] {rolls/buns, whole milk}            =&gt; {other vegetables} 0.01789527\n[12] {other vegetables, whole milk}      =&gt; {root vegetables}  0.02318251\n[13] {other vegetables, whole milk}      =&gt; {yogurt}           0.02226741\n[14] {whole milk, yogurt}                =&gt; {rolls/buns}       0.01555669\n[15] {rolls/buns, whole milk}            =&gt; {yogurt}           0.01555669\n[16] {whole milk, yogurt}                =&gt; {tropical fruit}   0.01514997\n     confidence coverage   lift     count\n[1]  0.5173611  0.02928317 2.024770 149  \n[2]  0.5128806  0.04341637 2.007235 219  \n[3]  0.4892704  0.04738180 1.914833 228  \n[4]  0.4759207  0.03589222 1.862587 168  \n[5]  0.4740125  0.04890696 2.449770 228  \n[6]  0.4526627  0.03436706 1.771563 153  \n[7]  0.4200477  0.04260295 1.643919 176  \n[8]  0.4038462  0.04229792 2.087140 168  \n[9]  0.3974592  0.05602440 2.054131 219  \n[10] 0.3581731  0.04229792 2.567516 149  \n[11] 0.3159785  0.05663447 1.633026 176  \n[12] 0.3097826  0.07483477 2.842082 228  \n[13] 0.2975543  0.07483477 2.132979 219  \n[14] 0.2776770  0.05602440 1.509648 153  \n[15] 0.2746858  0.05663447 1.969049 153  \n[16] 0.2704174  0.05602440 2.577089 149  \n\n\n\nChange the minimum rule length back to 2, produce a list of rules involving either soda or whipped/sour cream.\n\n\ngroceryrules2 = \n  apriori(groceries,\n          parameter = list(\n            support = 0.015,\n            confidence = 0.25,\n            minlen = 2\n          ) )\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n       0.25    0.1    1 none FALSE            TRUE       5   0.015      2\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 147 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[169 item(s), 9835 transaction(s)] done [0.00s].\nsorting and recoding items ... [73 item(s)] done [0.00s].\ncreating transaction tree ... done [0.01s].\nchecking subsets of size 1 2 3 done [0.00s].\nwriting ... [78 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\n\n\ngroceryrules2 %&gt;%\n  sort(by = \"confidence\") %&gt;%\n  inspect()\n\n     lhs                                    rhs                support   \n[1]  {tropical fruit, yogurt}            =&gt; {whole milk}       0.01514997\n[2]  {other vegetables, yogurt}          =&gt; {whole milk}       0.02226741\n[3]  {butter}                            =&gt; {whole milk}       0.02755465\n[4]  {curd}                              =&gt; {whole milk}       0.02613116\n[5]  {other vegetables, root vegetables} =&gt; {whole milk}       0.02318251\n[6]  {other vegetables, tropical fruit}  =&gt; {whole milk}       0.01708185\n[7]  {root vegetables, whole milk}       =&gt; {other vegetables} 0.02318251\n[8]  {domestic eggs}                     =&gt; {whole milk}       0.02999492\n[9]  {rolls/buns, yogurt}                =&gt; {whole milk}       0.01555669\n[10] {whipped/sour cream}                =&gt; {whole milk}       0.03223183\n[11] {root vegetables}                   =&gt; {whole milk}       0.04890696\n[12] {sugar}                             =&gt; {whole milk}       0.01504830\n[13] {root vegetables}                   =&gt; {other vegetables} 0.04738180\n[14] {frozen vegetables}                 =&gt; {whole milk}       0.02043721\n[15] {other vegetables, rolls/buns}      =&gt; {whole milk}       0.01789527\n[16] {chicken}                           =&gt; {other vegetables} 0.01789527\n[17] {cream cheese}                      =&gt; {whole milk}       0.01647178\n[18] {margarine}                         =&gt; {whole milk}       0.02419929\n[19] {chicken}                           =&gt; {whole milk}       0.01759024\n[20] {white bread}                       =&gt; {whole milk}       0.01708185\n[21] {beef}                              =&gt; {whole milk}       0.02125064\n[22] {tropical fruit, whole milk}        =&gt; {other vegetables} 0.01708185\n[23] {tropical fruit}                    =&gt; {whole milk}       0.04229792\n[24] {whipped/sour cream}                =&gt; {other vegetables} 0.02887646\n[25] {yogurt}                            =&gt; {whole milk}       0.05602440\n[26] {pip fruit}                         =&gt; {whole milk}       0.03009659\n[27] {whole milk, yogurt}                =&gt; {other vegetables} 0.02226741\n[28] {brown bread}                       =&gt; {whole milk}       0.02521607\n[29] {other vegetables}                  =&gt; {whole milk}       0.07483477\n[30] {pork}                              =&gt; {whole milk}       0.02216573\n[31] {napkins}                           =&gt; {whole milk}       0.01972547\n[32] {beef}                              =&gt; {other vegetables} 0.01972547\n[33] {pork}                              =&gt; {other vegetables} 0.02165735\n[34] {pastry}                            =&gt; {whole milk}       0.03324860\n[35] {frozen vegetables}                 =&gt; {other vegetables} 0.01779359\n[36] {citrus fruit}                      =&gt; {whole milk}       0.03050330\n[37] {fruit/vegetable juice}             =&gt; {whole milk}       0.02663955\n[38] {butter}                            =&gt; {other vegetables} 0.02003050\n[39] {tropical fruit, whole milk}        =&gt; {yogurt}           0.01514997\n[40] {domestic eggs}                     =&gt; {other vegetables} 0.02226741\n[41] {citrus fruit}                      =&gt; {other vegetables} 0.02887646\n[42] {frankfurter}                       =&gt; {whole milk}       0.02053889\n[43] {pip fruit}                         =&gt; {other vegetables} 0.02613116\n[44] {newspapers}                        =&gt; {whole milk}       0.02735130\n[45] {tropical fruit}                    =&gt; {other vegetables} 0.03589222\n[46] {margarine}                         =&gt; {other vegetables} 0.01972547\n[47] {chocolate}                         =&gt; {whole milk}       0.01667514\n[48] {beef}                              =&gt; {root vegetables}  0.01738688\n[49] {frankfurter}                       =&gt; {rolls/buns}       0.01921708\n[50] {sausage}                           =&gt; {rolls/buns}       0.03060498\n[51] {curd}                              =&gt; {yogurt}           0.01728521\n[52] {curd}                              =&gt; {other vegetables} 0.01718353\n[53] {coffee}                            =&gt; {whole milk}       0.01870869\n[54] {sausage}                           =&gt; {whole milk}       0.02989324\n[55] {rolls/buns, whole milk}            =&gt; {other vegetables} 0.01789527\n[56] {yogurt}                            =&gt; {other vegetables} 0.04341637\n[57] {bottled water}                     =&gt; {whole milk}       0.03436706\n[58] {other vegetables, whole milk}      =&gt; {root vegetables}  0.02318251\n[59] {rolls/buns}                        =&gt; {whole milk}       0.05663447\n[60] {other vegetables, whole milk}      =&gt; {yogurt}           0.02226741\n[61] {whole milk}                        =&gt; {other vegetables} 0.07483477\n[62] {fruit/vegetable juice}             =&gt; {other vegetables} 0.02104728\n[63] {whipped/sour cream}                =&gt; {yogurt}           0.02074225\n[64] {brown bread}                       =&gt; {other vegetables} 0.01870869\n[65] {sausage}                           =&gt; {other vegetables} 0.02694459\n[66] {frankfurter}                       =&gt; {other vegetables} 0.01647178\n[67] {tropical fruit}                    =&gt; {yogurt}           0.02928317\n[68] {whole milk, yogurt}                =&gt; {rolls/buns}       0.01555669\n[69] {rolls/buns, whole milk}            =&gt; {yogurt}           0.01555669\n[70] {whole milk, yogurt}                =&gt; {tropical fruit}   0.01514997\n[71] {pip fruit}                         =&gt; {tropical fruit}   0.02043721\n[72] {bottled water}                     =&gt; {soda}             0.02897814\n[73] {citrus fruit}                      =&gt; {yogurt}           0.02165735\n[74] {fruit/vegetable juice}             =&gt; {yogurt}           0.01870869\n[75] {sausage}                           =&gt; {soda}             0.02430097\n[76] {fruit/vegetable juice}             =&gt; {soda}             0.01840366\n[77] {bottled beer}                      =&gt; {whole milk}       0.02043721\n[78] {pastry}                            =&gt; {other vegetables} 0.02257245\n     confidence coverage   lift      count\n[1]  0.5173611  0.02928317 2.0247698 149  \n[2]  0.5128806  0.04341637 2.0072345 219  \n[3]  0.4972477  0.05541434 1.9460530 271  \n[4]  0.4904580  0.05327911 1.9194805 257  \n[5]  0.4892704  0.04738180 1.9148326 228  \n[6]  0.4759207  0.03589222 1.8625865 168  \n[7]  0.4740125  0.04890696 2.4497702 228  \n[8]  0.4727564  0.06344687 1.8502027 295  \n[9]  0.4526627  0.03436706 1.7715630 153  \n[10] 0.4496454  0.07168277 1.7597542 317  \n[11] 0.4486940  0.10899847 1.7560310 481  \n[12] 0.4444444  0.03385867 1.7393996 148  \n[13] 0.4347015  0.10899847 2.2466049 466  \n[14] 0.4249471  0.04809354 1.6630940 201  \n[15] 0.4200477  0.04260295 1.6439194 176  \n[16] 0.4170616  0.04290798 2.1554393 176  \n[17] 0.4153846  0.03965430 1.6256696 162  \n[18] 0.4131944  0.05856634 1.6170980 238  \n[19] 0.4099526  0.04290798 1.6044106 173  \n[20] 0.4057971  0.04209456 1.5881474 168  \n[21] 0.4050388  0.05246568 1.5851795 209  \n[22] 0.4038462  0.04229792 2.0871397 168  \n[23] 0.4031008  0.10493137 1.5775950 416  \n[24] 0.4028369  0.07168277 2.0819237 284  \n[25] 0.4016035  0.13950178 1.5717351 551  \n[26] 0.3978495  0.07564820 1.5570432 296  \n[27] 0.3974592  0.05602440 2.0541308 219  \n[28] 0.3887147  0.06487036 1.5212930 248  \n[29] 0.3867578  0.19349263 1.5136341 736  \n[30] 0.3844797  0.05765125 1.5047187 218  \n[31] 0.3766990  0.05236401 1.4742678 194  \n[32] 0.3759690  0.05246568 1.9430662 194  \n[33] 0.3756614  0.05765125 1.9414764 213  \n[34] 0.3737143  0.08896797 1.4625865 327  \n[35] 0.3699789  0.04809354 1.9121083 175  \n[36] 0.3685504  0.08276563 1.4423768 300  \n[37] 0.3684951  0.07229283 1.4421604 262  \n[38] 0.3614679  0.05541434 1.8681223 197  \n[39] 0.3581731  0.04229792 2.5675162 149  \n[40] 0.3509615  0.06344687 1.8138238 219  \n[41] 0.3488943  0.08276563 1.8031403 284  \n[42] 0.3482759  0.05897306 1.3630295 202  \n[43] 0.3454301  0.07564820 1.7852365 257  \n[44] 0.3426752  0.07981698 1.3411103 269  \n[45] 0.3420543  0.10493137 1.7677896 353  \n[46] 0.3368056  0.05856634 1.7406635 194  \n[47] 0.3360656  0.04961871 1.3152427 164  \n[48] 0.3313953  0.05246568 3.0403668 171  \n[49] 0.3258621  0.05897306 1.7716161 189  \n[50] 0.3257576  0.09395018 1.7710480 301  \n[51] 0.3244275  0.05327911 2.3256154 170  \n[52] 0.3225191  0.05327911 1.6668288 169  \n[53] 0.3222417  0.05805796 1.2611408 184  \n[54] 0.3181818  0.09395018 1.2452520 294  \n[55] 0.3159785  0.05663447 1.6330258 176  \n[56] 0.3112245  0.13950178 1.6084566 427  \n[57] 0.3109476  0.11052364 1.2169396 338  \n[58] 0.3097826  0.07483477 2.8420820 228  \n[59] 0.3079049  0.18393493 1.2050318 557  \n[60] 0.2975543  0.07483477 2.1329789 219  \n[61] 0.2928770  0.25551601 1.5136341 736  \n[62] 0.2911392  0.07229283 1.5046529 207  \n[63] 0.2893617  0.07168277 2.0742510 204  \n[64] 0.2884013  0.06487036 1.4905025 184  \n[65] 0.2867965  0.09395018 1.4822091 265  \n[66] 0.2793103  0.05897306 1.4435193 162  \n[67] 0.2790698  0.10493137 2.0004746 288  \n[68] 0.2776770  0.05602440 1.5096478 153  \n[69] 0.2746858  0.05663447 1.9690488 153  \n[70] 0.2704174  0.05602440 2.5770885 149  \n[71] 0.2701613  0.07564820 2.5746476 201  \n[72] 0.2621895  0.11052364 1.5035766 285  \n[73] 0.2616708  0.08276563 1.8757521 213  \n[74] 0.2587904  0.07229283 1.8551049 184  \n[75] 0.2586580  0.09395018 1.4833245 239  \n[76] 0.2545710  0.07229283 1.4598869 181  \n[77] 0.2537879  0.08052872 0.9932367 201  \n[78] 0.2537143  0.08896797 1.3112349 222  \n\n\n\ngroceryrules2 %&gt;%\n  subset(items %in% c(\"soda\", \"whipped/sour cream\")) %&gt;%\n  inspect()\n\n    lhs                        rhs                support    confidence\n[1] {fruit/vegetable juice} =&gt; {soda}             0.01840366 0.2545710 \n[2] {whipped/sour cream}    =&gt; {yogurt}           0.02074225 0.2893617 \n[3] {whipped/sour cream}    =&gt; {other vegetables} 0.02887646 0.4028369 \n[4] {whipped/sour cream}    =&gt; {whole milk}       0.03223183 0.4496454 \n[5] {sausage}               =&gt; {soda}             0.02430097 0.2586580 \n[6] {bottled water}         =&gt; {soda}             0.02897814 0.2621895 \n    coverage   lift     count\n[1] 0.07229283 1.459887 181  \n[2] 0.07168277 2.074251 204  \n[3] 0.07168277 2.081924 284  \n[4] 0.07168277 1.759754 317  \n[5] 0.09395018 1.483324 239  \n[6] 0.11052364 1.503577 285"
  },
  {
    "objectID": "posts/Problem set 8/ps8.html#section",
    "href": "posts/Problem set 8/ps8.html#section",
    "title": "Problem Set #8",
    "section": "1.",
    "text": "1.\n\ndf = read.transactions('Market_Basket_Optimisation.csv', sep = \",\")\n\nWarning in asMethod(object): removing duplicated items in transactions"
  },
  {
    "objectID": "posts/Problem set 8/ps8.html#section-1",
    "href": "posts/Problem set 8/ps8.html#section-1",
    "title": "Problem Set #8",
    "section": "2.",
    "text": "2.\nThere are a total of 7501 transactions in the data. There are 119 distinct items in the data.\n\ndf\n\ntransactions in sparse format with\n 7501 transactions (rows) and\n 119 items (columns)"
  },
  {
    "objectID": "posts/Problem set 8/ps8.html#section-2",
    "href": "posts/Problem set 8/ps8.html#section-2",
    "title": "Problem Set #8",
    "section": "3.",
    "text": "3.\n\ns = summary(df)\n\n\nsizes = s@lengths\nprint(sizes)\n\nsizes\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n1754 1358 1044  816  667  493  391  324  259  139  102   67   40   22   17    4 \n  18   19   20 \n   1    2    1 \n\n\n\nplot(sizes, \n     lw = 6, col = \"aquamarine\",\n     main=\"Transaction Sizes\",\n     xlab=\"# Items in Transaction\",\n     ylab=\"Size Frequency\")"
  },
  {
    "objectID": "posts/Problem set 8/ps8.html#section-3",
    "href": "posts/Problem set 8/ps8.html#section-3",
    "title": "Problem Set #8",
    "section": "4.",
    "text": "4.\n\nlibrary(tidyverse)\ndf_frequency = \n  tibble(\n    Items = names(itemFrequency(df)),\n    Frequency = itemFrequency(df)\n  )\n\nThe 10 most frequent items are…\n\n\nmineral water\neggs\nspaghetti\nfrench fries\nchocolate\ngreen tea\nmilk\nground beef\nfrozen vegetables\npancakes\n\n\ndf_frequency %&gt;%\n  arrange(desc(Frequency)) %&gt;%\n  slice(1:10)\n\n# A tibble: 10 × 2\n   Items             Frequency\n   &lt;chr&gt;                 &lt;dbl&gt;\n 1 mineral water        0.238 \n 2 eggs                 0.180 \n 3 spaghetti            0.174 \n 4 french fries         0.171 \n 5 chocolate            0.164 \n 6 green tea            0.132 \n 7 milk                 0.130 \n 8 ground beef          0.0983\n 9 frozen vegetables    0.0953\n10 pancakes             0.0951\n\n\nThe 10 least frequent items are…\n\n\nwater spray\nnapkins\ncream\nbramble\ntea\nchutney\nmashed potato\nchocolate bread\ndessert wine\nketchup\n\n\ndf_frequency %&gt;%\n  arrange(Frequency) %&gt;%\n  slice(1:10)\n\n# A tibble: 10 × 2\n   Items           Frequency\n   &lt;chr&gt;               &lt;dbl&gt;\n 1 water spray      0.000400\n 2 napkins          0.000667\n 3 cream            0.000933\n 4 bramble          0.00187 \n 5 tea              0.00387 \n 6 chutney          0.00413 \n 7 mashed potato    0.00413 \n 8 chocolate bread  0.00427 \n 9 dessert wine     0.00440 \n10 ketchup          0.00440"
  },
  {
    "objectID": "posts/Problem set 8/ps8.html#section-4",
    "href": "posts/Problem set 8/ps8.html#section-4",
    "title": "Problem Set #8",
    "section": "5.",
    "text": "5.\nThere is a total of 52 association rules that were generated.\n\ndf.rules = \n  apriori(df,\n          parameter = list(\n            support = 0.015,\n            confidence = 0.25,\n            minlen = 2\n          ) )\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n       0.25    0.1    1 none FALSE            TRUE       5   0.015      2\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 112 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[119 item(s), 7501 transaction(s)] done [0.00s].\nsorting and recoding items ... [62 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 done [0.00s].\nwriting ... [52 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s]."
  },
  {
    "objectID": "posts/Problem set 8/ps8.html#section-5",
    "href": "posts/Problem set 8/ps8.html#section-5",
    "title": "Problem Set #8",
    "section": "6.",
    "text": "6.\nThere are 43 rules with a length of 2 items & 9 rules with a length of 3 items, resulting in our total of 52 rules.\n\nsummary(df.rules)\n\nset of 52 rules\n\nrule length distribution (lhs + rhs):sizes\n 2  3 \n43  9 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.000   2.000   2.000   2.173   2.000   3.000 \n\nsummary of quality measures:\n    support          confidence        coverage            lift      \n Min.   :0.01506   Min.   :0.2506   Min.   :0.03546   Min.   :1.174  \n 1st Qu.:0.01693   1st Qu.:0.2909   1st Qu.:0.05106   1st Qu.:1.446  \n Median :0.02020   Median :0.3224   Median :0.06292   Median :1.650  \n Mean   :0.02467   Mean   :0.3310   Mean   :0.07636   Mean   :1.707  \n 3rd Qu.:0.02750   3rd Qu.:0.3601   3rd Qu.:0.08719   3rd Qu.:1.829  \n Max.   :0.05973   Max.   :0.4565   Max.   :0.23837   Max.   :3.292  \n     count      \n Min.   :113.0  \n 1st Qu.:127.0  \n Median :151.5  \n Mean   :185.0  \n 3rd Qu.:206.2  \n Max.   :448.0  \n\nmining info:\n data ntransactions support confidence\n   df          7501   0.015       0.25\n                                                                                 call\n apriori(data = df, parameter = list(support = 0.015, confidence = 0.25, minlen = 2))\n\n\nHere are the top 12 association rules by confidence.\n\ndf.rules %&gt;%\n  sort(by = \"confidence\") %&gt;%\n  head(n = 12) %&gt;%\n  inspect()\n\n     lhs                             rhs             support    confidence\n[1]  {soup}                       =&gt; {mineral water} 0.02306359 0.4564644 \n[2]  {milk, spaghetti}            =&gt; {mineral water} 0.01573124 0.4436090 \n[3]  {ground beef, spaghetti}     =&gt; {mineral water} 0.01706439 0.4353741 \n[4]  {olive oil}                  =&gt; {mineral water} 0.02759632 0.4190283 \n[5]  {ground beef, mineral water} =&gt; {spaghetti}     0.01706439 0.4169381 \n[6]  {ground beef}                =&gt; {mineral water} 0.04092788 0.4165536 \n[7]  {chocolate, spaghetti}       =&gt; {mineral water} 0.01586455 0.4047619 \n[8]  {salmon}                     =&gt; {mineral water} 0.01706439 0.4012539 \n[9]  {ground beef}                =&gt; {spaghetti}     0.03919477 0.3989145 \n[10] {cooking oil}                =&gt; {mineral water} 0.02013065 0.3942559 \n[11] {chicken}                    =&gt; {mineral water} 0.02279696 0.3800000 \n[12] {frozen vegetables}          =&gt; {mineral water} 0.03572857 0.3748252 \n     coverage   lift     count\n[1]  0.05052660 1.914955 173  \n[2]  0.03546194 1.861024 118  \n[3]  0.03919477 1.826477 128  \n[4]  0.06585789 1.757904 207  \n[5]  0.04092788 2.394681 128  \n[6]  0.09825357 1.747522 307  \n[7]  0.03919477 1.698053 119  \n[8]  0.04252766 1.683336 128  \n[9]  0.09825357 2.291162 294  \n[10] 0.05105986 1.653978 151  \n[11] 0.05999200 1.594172 171  \n[12] 0.09532062 1.572463 268  \n\n\nHere are the top 12 association rules by lift.\n\ndf.rules %&gt;%\n  sort(by = \"lift\") %&gt;%\n  head(n = 12) %&gt;%\n  inspect()\n\n     lhs                             rhs             support    confidence\n[1]  {herb & pepper}              =&gt; {ground beef}   0.01599787 0.3234501 \n[2]  {mineral water, spaghetti}   =&gt; {ground beef}   0.01706439 0.2857143 \n[3]  {ground beef, mineral water} =&gt; {spaghetti}     0.01706439 0.4169381 \n[4]  {soup}                       =&gt; {milk}          0.01519797 0.3007916 \n[5]  {ground beef}                =&gt; {spaghetti}     0.03919477 0.3989145 \n[6]  {mineral water, spaghetti}   =&gt; {milk}          0.01573124 0.2633929 \n[7]  {olive oil}                  =&gt; {spaghetti}     0.02293028 0.3481781 \n[8]  {olive oil}                  =&gt; {milk}          0.01706439 0.2591093 \n[9]  {soup}                       =&gt; {mineral water} 0.02306359 0.4564644 \n[10] {herb & pepper}              =&gt; {spaghetti}     0.01626450 0.3288410 \n[11] {milk, mineral water}        =&gt; {spaghetti}     0.01573124 0.3277778 \n[12] {milk, spaghetti}            =&gt; {mineral water} 0.01573124 0.4436090 \n     coverage   lift     count\n[1]  0.04946007 3.291994 120  \n[2]  0.05972537 2.907928 128  \n[3]  0.04092788 2.394681 128  \n[4]  0.05052660 2.321232 114  \n[5]  0.09825357 2.291162 294  \n[6]  0.05972537 2.032623 118  \n[7]  0.06585789 1.999758 172  \n[8]  0.06585789 1.999567 128  \n[9]  0.05052660 1.914955 173  \n[10] 0.04946007 1.888695 122  \n[11] 0.04799360 1.882589 118  \n[12] 0.03546194 1.861024 118"
  },
  {
    "objectID": "posts/Problem set 8/ps8.html#section-6",
    "href": "posts/Problem set 8/ps8.html#section-6",
    "title": "Problem Set #8",
    "section": "7.",
    "text": "7.\nHere are the top 10 association rules by lift, that do not include the 6 most frequent items. We can see that there are only 3 available.\n\ndf.rules %&gt;%\n  subset(!items %in% c(\"mineral water\", \"eggs\", \"spaghetti\", \"french fries\", \"chocolate\", \"green tea\")) %&gt;%\n  sort(by = \"lift\") %&gt;%\n  inspect()\n\n    lhs                rhs           support    confidence coverage   lift    \n[1] {herb & pepper} =&gt; {ground beef} 0.01599787 0.3234501  0.04946007 3.291994\n[2] {soup}          =&gt; {milk}        0.01519797 0.3007916  0.05052660 2.321232\n[3] {olive oil}     =&gt; {milk}        0.01706439 0.2591093  0.06585789 1.999567\n    count\n[1] 120  \n[2] 114  \n[3] 128"
  },
  {
    "objectID": "posts/Problem set 8/ps8.html#section-7",
    "href": "posts/Problem set 8/ps8.html#section-7",
    "title": "Problem Set #8",
    "section": "8.",
    "text": "8.\nA couple of rules I found the most interesting were the purchases of shrimp and chocolate, burgers and eggs, and pancakes and spaghetti. All of these combinations are quite uncommon to me for each pair has another purchased item from such a different food group (such as sweet and savory & breakfast and dinner). When it comes to retail context, I think that these items being purchased together were due to the commonness that each item has. Possibly putting common items such as all of these mentioned closer together could possibly result in more convenient grocery store run for customers. If buying shrimp for dinner, purchasing chocolate for dessert is a pretty common thought process. When buying burgers, purchasing eggs to eat the next morning for breakfast could easily be a reason for purchase. Lastly, when buying pancake mix for breakfast, buying spaghetti for dinner later i the day would save additional shopping and prepping later in time. All of these possibly scenarios were just a. few of the possible ones that could occur when relating them back to the retail context.\n\ndf.rules %&gt;%\n  inspect()\n\n     lhs                             rhs             support    confidence\n[1]  {salmon}                     =&gt; {mineral water} 0.01706439 0.4012539 \n[2]  {honey}                      =&gt; {mineral water} 0.01506466 0.3174157 \n[3]  {herb & pepper}              =&gt; {ground beef}   0.01599787 0.3234501 \n[4]  {herb & pepper}              =&gt; {spaghetti}     0.01626450 0.3288410 \n[5]  {herb & pepper}              =&gt; {mineral water} 0.01706439 0.3450135 \n[6]  {grated cheese}              =&gt; {spaghetti}     0.01653113 0.3155216 \n[7]  {grated cheese}              =&gt; {mineral water} 0.01746434 0.3333333 \n[8]  {soup}                       =&gt; {milk}          0.01519797 0.3007916 \n[9]  {soup}                       =&gt; {mineral water} 0.02306359 0.4564644 \n[10] {cooking oil}                =&gt; {spaghetti}     0.01586455 0.3107050 \n[11] {cooking oil}                =&gt; {mineral water} 0.02013065 0.3942559 \n[12] {whole wheat rice}           =&gt; {mineral water} 0.02013065 0.3439636 \n[13] {turkey}                     =&gt; {eggs}          0.01946407 0.3113006 \n[14] {turkey}                     =&gt; {spaghetti}     0.01653113 0.2643923 \n[15] {turkey}                     =&gt; {mineral water} 0.01919744 0.3070362 \n[16] {chicken}                    =&gt; {spaghetti}     0.01719771 0.2866667 \n[17] {chicken}                    =&gt; {mineral water} 0.02279696 0.3800000 \n[18] {frozen smoothie}            =&gt; {mineral water} 0.02026396 0.3200000 \n[19] {low fat yogurt}             =&gt; {mineral water} 0.02399680 0.3135889 \n[20] {tomatoes}                   =&gt; {spaghetti}     0.02093054 0.3060429 \n[21] {tomatoes}                   =&gt; {mineral water} 0.02439675 0.3567251 \n[22] {olive oil}                  =&gt; {milk}          0.01706439 0.2591093 \n[23] {olive oil}                  =&gt; {spaghetti}     0.02293028 0.3481781 \n[24] {olive oil}                  =&gt; {mineral water} 0.02759632 0.4190283 \n[25] {shrimp}                     =&gt; {chocolate}     0.01799760 0.2518657 \n[26] {shrimp}                     =&gt; {spaghetti}     0.02119717 0.2966418 \n[27] {shrimp}                     =&gt; {mineral water} 0.02359685 0.3302239 \n[28] {cake}                       =&gt; {mineral water} 0.02746300 0.3388158 \n[29] {burgers}                    =&gt; {french fries}  0.02199707 0.2522936 \n[30] {burgers}                    =&gt; {eggs}          0.02879616 0.3302752 \n[31] {burgers}                    =&gt; {mineral water} 0.02439675 0.2798165 \n[32] {pancakes}                   =&gt; {spaghetti}     0.02519664 0.2650771 \n[33] {pancakes}                   =&gt; {mineral water} 0.03372884 0.3548387 \n[34] {frozen vegetables}          =&gt; {spaghetti}     0.02786295 0.2923077 \n[35] {frozen vegetables}          =&gt; {mineral water} 0.03572857 0.3748252 \n[36] {ground beef}                =&gt; {spaghetti}     0.03919477 0.3989145 \n[37] {ground beef}                =&gt; {mineral water} 0.04092788 0.4165536 \n[38] {milk}                       =&gt; {spaghetti}     0.03546194 0.2736626 \n[39] {milk}                       =&gt; {mineral water} 0.04799360 0.3703704 \n[40] {chocolate}                  =&gt; {mineral water} 0.05265965 0.3213995 \n[41] {eggs}                       =&gt; {mineral water} 0.05092654 0.2833828 \n[42] {spaghetti}                  =&gt; {mineral water} 0.05972537 0.3430322 \n[43] {mineral water}              =&gt; {spaghetti}     0.05972537 0.2505593 \n[44] {ground beef, spaghetti}     =&gt; {mineral water} 0.01706439 0.4353741 \n[45] {ground beef, mineral water} =&gt; {spaghetti}     0.01706439 0.4169381 \n[46] {mineral water, spaghetti}   =&gt; {ground beef}   0.01706439 0.2857143 \n[47] {milk, spaghetti}            =&gt; {mineral water} 0.01573124 0.4436090 \n[48] {milk, mineral water}        =&gt; {spaghetti}     0.01573124 0.3277778 \n[49] {mineral water, spaghetti}   =&gt; {milk}          0.01573124 0.2633929 \n[50] {chocolate, spaghetti}       =&gt; {mineral water} 0.01586455 0.4047619 \n[51] {chocolate, mineral water}   =&gt; {spaghetti}     0.01586455 0.3012658 \n[52] {mineral water, spaghetti}   =&gt; {chocolate}     0.01586455 0.2656250 \n     coverage   lift     count\n[1]  0.04252766 1.683336 128  \n[2]  0.04746034 1.331619 113  \n[3]  0.04946007 3.291994 120  \n[4]  0.04946007 1.888695 122  \n[5]  0.04946007 1.447397 128  \n[6]  0.05239301 1.812196 124  \n[7]  0.05239301 1.398397 131  \n[8]  0.05052660 2.321232 114  \n[9]  0.05052660 1.914955 173  \n[10] 0.05105986 1.784531 119  \n[11] 0.05105986 1.653978 151  \n[12] 0.05852553 1.442993 151  \n[13] 0.06252500 1.732245 146  \n[14] 0.06252500 1.518535 124  \n[15] 0.06252500 1.288075 144  \n[16] 0.05999200 1.646468 129  \n[17] 0.05999200 1.594172 171  \n[18] 0.06332489 1.342461 152  \n[19] 0.07652313 1.315565 180  \n[20] 0.06839088 1.757755 157  \n[21] 0.06839088 1.496530 183  \n[22] 0.06585789 1.999567 128  \n[23] 0.06585789 1.999758 172  \n[24] 0.06585789 1.757904 207  \n[25] 0.07145714 1.537221 135  \n[26] 0.07145714 1.703760 159  \n[27] 0.07145714 1.385352 177  \n[28] 0.08105586 1.421397 206  \n[29] 0.08718837 1.476173 165  \n[30] 0.08718837 1.837830 216  \n[31] 0.08718837 1.173883 183  \n[32] 0.09505399 1.522468 189  \n[33] 0.09505399 1.488616 253  \n[34] 0.09532062 1.678867 209  \n[35] 0.09532062 1.572463 268  \n[36] 0.09825357 2.291162 294  \n[37] 0.09825357 1.747522 307  \n[38] 0.12958272 1.571779 266  \n[39] 0.12958272 1.553774 360  \n[40] 0.16384482 1.348332 395  \n[41] 0.17970937 1.188845 382  \n[42] 0.17411012 1.439085 448  \n[43] 0.23836822 1.439085 448  \n[44] 0.03919477 1.826477 128  \n[45] 0.04092788 2.394681 128  \n[46] 0.05972537 2.907928 128  \n[47] 0.03546194 1.861024 118  \n[48] 0.04799360 1.882589 118  \n[49] 0.05972537 2.032623 118  \n[50] 0.03919477 1.698053 119  \n[51] 0.05265965 1.730318 119  \n[52] 0.05972537 1.621199 119"
  }
]