---
title: "Problem Set 3"
author: "Annika G. Lee"
date: "2023-10-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1

```{r}
#| message: false
rm(list = ls())
library(tidyverse)
library(glmnet)
library(lubridate)
library(caret)
library(dummy)
library(gamlr)
library(rmarkdown)
library(GGally)
library(rpart)
library(rpart.plot)
```

## Part 2

Within this dataset there are a total of 1,436 observations and 39 columns of different features. 36 our of the 39 features are *numeric* date types, whereas 3 are *categorical* data types.

```{r}
cars = read_csv("ToyotaCorolla.csv")
glimpse(cars)
```

When looking at our data using glimpse(), we can assume that the features with little importance are `Id`, `Model`, `Mfg_Month`, and `Cylinders`. These features do not have much of an affect on generating predictions of prices for used Toyota Corollas within this dataset. Therefore removing these features will allow us a more simplified dataset to use to develop predictions. I have also renamed the `Age_08_04` feature into `Age`.

```{r}
cars = cars %>%
  select(-Id, -Model, -Mfg_Month, -Cylinders) %>%
  rename(Age = Age_08_04)
```

Some of our features are better represented as nominal data types. As we change those features into categorical and nominal data, we will change them into factor data. With now numeric and factor data types present, combining all the data back into the the `cars` dataset will allow us to look for missing values and help us determine where to impute the feature's median into spots for missing values.

```{r}
cars_fct = cars %>%
  select(-Price, -Age, -KM, -HP, -CC, -Weight, -Quarterly_Tax) %>%
  mutate_all(.funs = factor)

cars_num = cars %>%
  select(Price, Age, KM, HP, CC, Weight, Quarterly_Tax)

cars = bind_cols(cars_num, cars_fct)
```

Our dataset shows that each numeric feature has no missing values, therefore not needing to impute any median values into spots of missing values.

```{r}
summary(cars_num)
```

## Part 3

After looking at the variable `Price`, we can look at its distribution and can determine that `Price` is appropriate for a linear regression model for it does not have any missing values and does not have low variability within its data The distribution does have a slight right skew and a tiny amount of outliers. When lookong at our *Linear Regression* model that has been developed below, the **Min** and **Max** of the data set are not too different from one another and the same goes for **1Q** and **3Q** of the dataset as well. These are good signs of normal distribution that we want. The only slight concern is that the model's **Median** is quite a far distance from 0.

```{r}
#| warning: false
#| message: false
lm_Price = train(Price ~ .,
            data = cars,
            method = "lm")

lm_Price
```

```{r}
summary(lm_Price)
```

```{r}
histogram(cars$Price)
```

## Part 4

```{r}
#| warning: false
caret::featurePlot(keep(cars, is.numeric), cars$Price, plot = "scatter")
```

## Part 5

```{r}
#| message: false
cars %>%
  keep(is.numeric) %>%
  ggpairs()
```

## Part 6

```{r}
cars_dum = dummy(cars, int = TRUE)
cars_num = cars %>%
  keep(is.numeric)
cars = bind_cols(cars_num, cars_dum)
rm(cars_dum, cars_num)
```

```{r}
#DATA PARTITION 
set.seed(4532)
samp = createDataPartition(cars$Price, p=0.7, list = FALSE)
training = cars[samp, ]
testing = cars[-samp, ]
rm(samp)
```

## Part 7

Pre-pruning sets limitations and boundaries to the tree and limits the overall complexity of it. Whereas post-pruning allows the tree to continue growing nwith justifications being made to it. As we justify which data to use, we get to tune and retrain the tree.

```{r}
#TRAIN/CONTROL DATA
train_ctrl = trainControl(method = "repeatedcv", number = 20, repeats = 10)
tree = train(Price ~ .,
             data = training,
             method = "rpart",
             trControl = train_ctrl,
             tuneGrid = expand.grid(cp = seq(0.0, 0.1, 0.01)),
             control = rpart.control(method = "anova", minsplit = 1, minbucket = 1)
             )
tree 
```

```{r}
ctrl = rpart::rpart.control(cp= 0.01, 
                            minbucket = 7,
                            maxdepth = 30)

training_regression_tree = rpart(Price ~ .,
           data = training,
           method = "anova",
           control = ctrl)

pred = predict(training_regression_tree, training)

pred.rmse = caret::RMSE(pred = pred, obs = training$Price)

paste("Sample Mean:", mean(training$Price))
paste("Sample Stdev:", sd(training$Price))
paste("Model RMSE:", pred.rmse)
```

```{r}
rpart.plot(training_regression_tree)
```

## Part 8

When looking at our feature importance, we can determine that `Age`, `KM`, and \`Weight' are the only three variables that are difference in importance when compared to all other variables. We could possibly remove all other variables in the dataset.

```{r}
#| warning: false
library(iml)
library(patchwork)
tree_predictor = iml::Predictor$new(tree, data = training)
tree_imp = iml::FeatureImp$new(tree_predictor, loss = "rmse", compare = "ratio", n.repetitions = 10)
plot(tree_imp)
```

```{r}
tree_imp$results
```

## Part 9

```{r}
ctrl = rpart::rpart.control(cp= 0.01, 
                            minbucket = 7,
                            maxdepth = 30)
regression_tree2 = rpart(Price ~ KM + Age + Weight,
           data = training,
           method = "anova",
           control = ctrl)

pred = predict(regression_tree2, training)

pred.rmse = caret::RMSE(pred = pred, obs = training$Price)

paste("Sample Mean:", mean(training$Price))
paste("Sample Stdev:", sd(training$Price))
paste("Model RMSE:", pred.rmse)
```

```{r}
rpart.plot(regression_tree2)
```

## Part 10

```{r}
test_predictions = predict(regression_tree2, newdata = testing)
test_rmse = postResample(pred = test_predictions,
                         obs = testing$Price)
test_rmse
```

We can see from the training and testing of our dataset that the RMSE from the cross validation error with the most optimal model with the smallest complexity model is 1372.104. The RMSE for the testing data is 1495.690. We can see this due to our usage of less variables in when developing predictions with the testing data. Less variables means less accuracy to the model, which makes sense in this instance.
