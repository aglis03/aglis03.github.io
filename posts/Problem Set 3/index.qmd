---
title: "Problem Set 3"
author: "Annika G. Lee"
date: "2023-10-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1

```{r}
#| message: false
rm(list = ls())
library(tidyverse)
library(glmnet)
library(lubridate)
library(caret)
library(dummy)
library(gamlr)
library(rmarkdown)
library(GGally)
library(rpart)
library(rpart.plot)
library(corrplot)
```

## Part 2

Within this dataset there are a total of 1,436 observations and 39 columns of different features. 36 our of the 39 features are *numeric* date types, whereas 3 are *categorical* data types.

```{r}
cars = read_csv("ToyotaCorolla.csv")
glimpse(cars)
```

When looking at our data using glimpse(), we can assume that the features with little importance are `Id`, `Model`, `Mfg_Month`, and `Cylinders`. These features do not have much of an affect on generating predictions of prices for used Toyota Corollas within this dataset. Therefore removing these features will allow us a more simplified dataset to use to develop predictions. I have also renamed the `Age_08_04` feature into `Age`.

```{r}
cars = cars %>%
  select(-Id, -Model, -Mfg_Month, -Cylinders) %>%
  rename(Age = Age_08_04)
```

Some of our features are better represented as nominal data types. As we change those features into categorical and nominal data, we will change them into factor data. With now numeric and factor data types present, combining all the data back into the the `cars` dataset will allow us to look for missing values and help us determine where to impute the feature's median into spots for missing values.

```{r}
cars_fct = cars %>%
  select(-Price, -Age, -KM, -HP, -CC, -Weight, -Quarterly_Tax) %>%
  mutate_all(.funs = factor)

cars_num = cars %>%
  select(Price, Age, KM, HP, CC, Weight, Quarterly_Tax)

cars = bind_cols(cars_num, cars_fct)
```

Our dataset shows that each feature has no missing values. This mean we do not need to impute any values into spots of missing values.

```{r}
summary(cars)
```

## Part 3

After looking at the variable `Price`, we can look at its distribution and can determine that `Price` is appropriate for a linear regression model for it does not have any missing values and does not have low variability within its data. The distribution is right-skewed and has a small amount of outliers. When looking at our *Linear Regression* model that has been developed below, the **Min** and **Max** of the data set are not too different from one another and the same goes for **1Q** and **3Q** of the dataset as well. These are good signs of normal distribution that we want. The only slight concern is that the model's **Median** is quite a far distance from 0.

```{r}
#| warning: false
#| message: false
lm_Price = train(Price ~ .,
            data = cars,
            method = "lm")

lm_Price
```

```{r}
summary(lm_Price)
```

When creating a Histogram, we can take a look at the `Price` feature.

```{r}
cars %>%
  ggplot(aes(Price)) +
  geom_histogram(color = "black", bg = "skyblue") +
  labs(title = "Distribution of Selling Prices",
       x = "Selling price",
       y = "Count of cars") +
  theme_classic()
```

## Part 4

We can see that `Age` and `KM` have quite a strong negative relationship with `Price`.
```{r}
caret::featurePlot(keep(cars, is.numeric), cars$Price, plot = "scatter")
```

## Part 5

```{r}
#| message: false
cars %>%
  keep(is.numeric) %>%
  ggpairs()
```

## Part 6
 
We can convert our categorical variables into dummy variables.
```{r}
cars_dum = dummy(cars, int = TRUE)
cars_num = cars %>%
  keep(is.numeric)
cars = bind_cols(cars_num, cars_dum)
rm(cars_dum, cars_num)
```

Now we will partition our data.
```{r}
#DATA PARTITION 
set.seed(4532)
samp = createDataPartition(cars$Price, p=0.7, list = FALSE)
training = cars[samp, ]
testing = cars[-samp, ]
rm(samp)
```

## Part 7

Pre-pruning sets limitations and boundaries to the tree and limits the overall complexity of it. Whereas post-pruning allows the tree to continue growing nwith justifications being made to it. As we justify which data to use, we get to tune and retrain the tree.

```{r}
train_model = train(Price ~ .,
                   data = training,
                   method = "rpart",
                   trControl = trainControl(method = "cv", number = 10),
                   tuneGrid = expand.grid(cp = seq(0.0, 0.01, 0.0001)),
                   control = rpart.control(minbucket = 1)
                   )
plot(train_model)
```

```{r}
rpart.plot(train_model$finalModel)
```

## Part 8

When looking at our feature importance, we can determine that `Age`, `KM`, and `Weight` are the only three variables that have a difference in importance when compared to all other variables. We could possibly remove all other variables in the dataset.

```{r}
#| warning: false
library(iml)
library(patchwork)

tree_predictor = iml::Predictor$new(train_model,
                                    data = testing,
                                    y = testing$Price)

tree_imp = iml::FeatureImp$new(tree_predictor, loss = "rmse", compare = "ratio")
plot(tree_imp)
```

We can see that `Age` has the highest importance when compared to other feautures in the dataset. 
```{r}
tree_imp$results %>%
  filter(importance > 1)
```

## Part 9

```{r}
train_new = dplyr::select(training, Age, Weight, KM, Metallic_Rim_0, Powered_Windows_0, Quarterly_Tax, Automatic_airco_0, HP, Powered_Windows_0, Price)

new_tree = caret::train(Price ~ .,
                        data = train_new,
                        method = "rpart",
                        trControl = trainControl(method = "cv", number = 10),
                        tuneGrid = expand.grid(cp = seq(0.0, 0.01, 0.0001)),
                        control = rpart.control(minbucket = 1)
                        )
plot(new_tree)
```

```{r}
rpart.plot(new_tree$finalModel)
```

## Part 10

```{r}
train_error = postResample(predict(new_tree, training), training$Price)[["RMSE"]]

cv_error = min(new_tree$results$RMSE)

test_error = postResample(predict(new_tree, testing), testing$Price)[["RMSE"]]

data.frame(
  "Error Source" = c("Training", "Cross-Validation", "Testing"),
  "RMSE" = c(train_error, cv_error, test_error)
)
```

From our results, we can see that the model with the highest RMSE is for our Testing data. This makes sense for our Testing data is not being trained and fitted as well like our Training data. Both the Cross-Validation and Testing RMSE values are quite close to one another. 